{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN AND VAT COMPARISON IN SVHN DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Zheyi Qin,Zihui Li December 12, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PROBLEM FORMULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For nowadays, image recognition has become one of the most hit fields in machine learning. As we depend more and more on smart electrical devices, vehicles, and family facilities, the accuracy and reliability of these machines become the most significant issue. Recognizing numbers in images can extract the information contents from pictures, thus it will be used in future to realize smart city and IoT world. For example, it can be applied to automatic mail distribution in communities; Handwritten digit recognition can be applied to bank remittance number identification to greatly reduce labor costs. Print identification can be applied to postal code recognition. Natural scene digital recognition are applied to license plate number identification.   \n",
    "Among these image recognition applications, multi-diigit photographic numbers in captured at street level such as Google Street View consists of millions of geo-located panoramic images. The ability of convert street view digital number to computer recognizable numbers will helps us pinpoint the building locations, increase the accuracy of finding real world store, etc. <sup>[4]</sup>   \n",
    "More broadly, recognizing photographic street view numbers is related to the optical character recognition technique. The difficulties lies in such problem invloves the wide variability in the visual appearance of numbers, they might have different fonts, colors, styles, intra-digit spaces. The recognition problem is further complicated by environmental factors such as lighting, shadows, it is also affected by photographic issues such as resolution, motion and focus blurs.   \n",
    "In this project, we mainly focus on single-digit(affected by close neighbor digits) recognition from Street View photos. Due to the above complexities, traditional approaches to solve this problem typically separate out the segmentation, and recognition steps.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already learned in class, convolutional neural network is a deep learning algorithm which can take in an input image, adjust learnable weights and bias to different aspects in the image, thus differentiate one from other, and use softmax technique to classify different categories of images. Also, CNN is able to successfully capture the spatial and temporal dependencies in an image. But CNN can not solve all kinds of recognition and classification problem.    \n",
    "In fast developing digital world, we human generate terabytes of images every day, most of the images are not labeled, manually classify categories of images cost a lot of energy. 99% of the images are not classified doesn't mean they are meaningless, unclassified images also contain features and information. Thus we need a new machine learning technique to utilize these chaos information and extract useful features from them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAT<sup>[2]</sup> is a regularization method, which introduces additional information in order to manage the inevitable gap between the training error and the test error, it has given good results in supervised learning, semi-supervised learning, and unsupervised clustering. Even without label information, virtual adversarial directions can be defined on unlabeled data points. Then 'virtual' training can be performed on these 'new' datasets. Base on these concepts, the method can be used on semi-supervised learning. That is why we interested in VAT. We only need to teach a model a few things, then it can learn more and more.  \n",
    "The core in VAT is a method called local distributional smoothness (LDS). The LDS bases on KL-divergence of outputs of original inputs and perturbation outputs of perturbation inputs. The KL-divergence is a kind of loss, and its reduction would make the model smooth at each data point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key steps for virtual adversarial training are:\n",
    "1. Give an input date point, $x$.\n",
    "2. Add a small perturbation $r$ to $x$. Thus the transformed data will be $T(x) = x + r$. Note that the perturbation $r$ should be in the adversarial direction.\n",
    "3. The KL divergence between the model output of perturbed input $T(x)$ and the non-perturbed output should be maximum. From all the perturbations $r$, $r_{v-adv}$ would be the perturbation in the adversarial direction.\n",
    "\n",
    "$$\\Delta_{KL}(r,x^{(n)},\\theta) \\equiv KL[p(y|x^{(x)}),\\theta\\parallel p(y|x^{(x)} + r, \\theta)]$$\n",
    "\n",
    "$$r_{v-adv}^{(n)} \\equiv \\operatorname*{arg\\,max}_r\\{\\Delta_{KL}(r, x^{(n)}, \\theta;\\|r\\|_2) \\leq \\epsilon\\}$$\n",
    "\n",
    "There are two types of hyperparameters<sup>[2]</sup> in VAT:  \n",
    "1. The norm constraint $\\epsilon$ > 0 for the adversarial direction  \n",
    "2. The regularization coefficient $\\alpha$ > 0 that controls the relative balance between the negative log-likelihood and the regularizer $r_{v-adv}$  \n",
    "By properly choosing these two hyperparameters, VAT-based model can perform very well. \n",
    "\n",
    "In this project, we will apply CNN and VAT-Virtual Adversarial Training network to predict MNIST hand-writing digits and number street views, the MNIST dataset is used to testing keras, sklearn and other python machine learning tools, then make further exploration on SVHN dataset to compare the advantages and disadvantages of these two techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. METHODOLOGY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. We tried basic CNN model we learned in class. Due to lacking of normalizatition, padding, maxpooling, dropout techniques, the training and prediction results are not ideal, then we choose to use keras tools to build machine learning models both in CNN and VAT.\n",
    "2. Delete 54800 labels of MNIST and 70257/71257/72257 labels of SVHN dataset, preparing for semi-supervised VAT. \n",
    "* Explore keras tool using MNIST dataset, adjust parameters such as kernal size, strides, maxpooling parameters, then apply CNN and VAT techniques based on those to verify if VAT performs better in inadequate-classifed dataset.\n",
    "3. Using the above training model on SVHN dataset and compare precidtion accuray of CNN and VAT respectively, we tried 4 different set of training data: 1000 labeled, 72257 unlabeled images; 2000 labeled, 71257 unlabeled images; 3000 labeled, 70257 unlabeled images. Note that unlike MNIST dataset, the SVHN dataset comes from the real world images, which has 3 color channels. The color doesn't matter because we only want to recognize digits. Before we put data as input for VAT model, they are converted to greyscale, then substract the mean of each image and divide by its standard deviation. In CNN, we simply add normalization layer to realize the same functionality after we input the data to CNN model.  \n",
    "4. Discuss about the situations that VAT performs better than CNN and why it is better. Explore the hyperparameters in different models and why these parameters affect our reulsts.\n",
    "\n",
    "There are mutiple layers in both CNN and VAT network. Two/Three of them are convolutional neural networks with different kernal sieze and strides, and we mainly use relu algorithm as activation because it has lower computational simplicity, it is unlike the tanh and sigmoid activation function that require the use of an exponential calculation and is capable of outputting a true zero value.  \n",
    "\n",
    "Then we apply local response normalization with different parameters according to the dataset. In neurobiology, there is a concept called “lateral inhibition”. This refers to the capacity of an excited neuron to subdue its neighbors. We basically want a significant peak so that we have a form of local maxima. Local Response Normalization (LRN) layer implements the lateral inhibition in the CNN layers.\n",
    "\n",
    "$$b_{x,y}^i = \\frac{a_{x,y}^i}{(k+\\alpha {\\sum_{j=max(0,j-n/2)}^{min(N-1,i+n/2)}}(a_{x,y}^j)^2)^\\beta}$$\n",
    "$a_{x,y}^i$ is the activity of a neuron computed by applying kernel $i$ at position $(x,y)$ and then applying the ReLU nonlinearity, the response-normalized activity $b{x,y}^i$ is given by the above expression. The sum runs over $n$ “adjacent” kernel maps at the same spatial position, and N is the total number of kernels in the layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After convolutional and activation layer we also employ the dropout method. Dropout layer will set the output of each hidden neuron to 0 with the probability we set (usually less than 0.5). The 0 output of neurons do not contribute to the forward pass nor the back-propagation part. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons since a neuron cannot rely on the presence of particular other neurons. In this way, the model will be forced to learn different features that are useful in conjunction with different random subsets of the other neurons. The dropout layer also has the function of reducing substantial overfitting. It boost the MNIST and SVHN images prediction 5-7% accuracy.     \n",
    "\n",
    "We use sgd optimizer on MNIST dataset and Adam on SVHN dataset. Adam is very efficient stochastic optimization method, it only requires first-order gradients with little memory requirement. This method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. Adam optimizer has advantages such as the magnitudes of paramete updates doesn't change with rescaling of the gradient, the stationary objectives are not the necessary requirements. Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example x and lable y.$$\\theta = \\theta - \\eta\\cdot\\nabla_{\\theta} J(\\theta;x^{(i)};y^{(i)})$$  \n",
    "Batch gradient descent performs redundant computations for large datasets because it will recompute gradients for similar data entries before each parameter update. SGD has an efficient way to deal with this issue by performing one update at a time. Therefore, as we learned in class, SGD is usually much faster than Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DATASET & TOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we use the dataset provided by the Google Street View and processed by Stanford University researchers. The dataset is approximately 1 GB in size and formatted in .png and .mat style. It covers a broad range of dataset about various kinds of street house numbers, make it possible and suitable to implement convolutional neural network method to recognize and extract numbers from image<sup>[1]</sup>. The second dataset is MNIST, this dataset is suitable for testing and comparing the performance of CNN and other machine learning algorithms. \n",
    "\n",
    "* The street view house numbers dataset (SVHN) (Download Link:http://ufldl.stanford.edu/housenumbers/)   \n",
    "* MNIST (Download Link: http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. The images in this dataset are 28×28 pixels. The images size is fixed, as a result, this dataset is easy for us to start to try implementing new models based on new concepts. After we try our concepts on this dataset, we can implement more complex models in other datasets which is more difficult to do classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main dataset we use is called the Street View House Numbers (SVHN) Dataset. This dataset has 73257 digits for training, and 26032 digits for testing. This dataset has two types of format: original images with character level bounding boxes, and MNIST-like 32×32 images centered around a single character. The images in the first type format do not have fixed size, it is difficult for training the model, there are 2 ways to resize them to the same dimensions, padding or cutting. Cutting does not guarantee that the digit will be kept because by cutting the center part of images, the marginal digits will be dropped, and label would change either. If we choose to pad the images, the padded part doesn't contain any real world imformation, such as shades, texture, colors. As a result, we choose the 32*32 images as SVHN dataset. \n",
    "\n",
    "The sample images are in the fourth part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 TOOLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 3.7  \n",
    "Keras 2.2.5/2.3.1 : Keras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible.   \n",
    "Tensorflow 1.14.0/2.0.0: TensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. CODE and DISCUSSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the library we used in the program. In our project, we use keras to build our model. We also try another way to implement our model which will be shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import struct\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import * \n",
    "from keras.layers import *\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the versions of tools we use for this program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:1.14.0\n",
      "keras:2.2.5\n",
      "numpy:1.17.4\n"
     ]
    }
   ],
   "source": [
    "print(f'tensorflow:{tf.__version__}')\n",
    "print(f'keras:{keras.__version__}')\n",
    "print(f'numpy:{np.version.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can read training dataset, return 32×32 pixels images and labels. Originally, the labels are represented in 1-9, we will transform the labels to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile_train():\n",
    "    with open(r'MNIST_data/train-images-idx3-ubyte','rb') as f:\n",
    "        train_image = f.read()\n",
    "    with open(r'MNIST_data/train-labels-idx1-ubyte', 'rb') as f:\n",
    "        train_labels = f.read()\n",
    "    return train_image,train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function prepare the testing dataset, which is similar to training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile_test():\n",
    "    with open(r'MNIST_data/t10k-images-idx3-ubyte','rb') as f:\n",
    "        train_image = f.read()\n",
    "    with open(r'MNIST_data/t10k-labels-idx1-ubyte', 'rb') as f:\n",
    "        train_labels = f.read()\n",
    "    return train_image,train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read n images in the training/testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(buf,n):\n",
    "    im=[]\n",
    "    index = struct.calcsize('>IIII')\n",
    "    for i in range(n):\n",
    "        temp = struct.unpack_from('>784B', buf, index)\n",
    "        im.append(np.reshape(temp, (28, 28)))\n",
    "        index += struct.calcsize('>784B')\n",
    "    return np.array(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read n labels in the training/testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(buf,n):\n",
    "    l=[]\n",
    "    index = struct.calcsize('>II')\n",
    "    for i in range(n):\n",
    "        temp = struct.unpack_from('>1B', buf, index)\n",
    "        l.append(temp[0])\n",
    "        index += struct.calcsize('>1B')\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use all 55000 images in the training dataset as training data, all 10000 images in the testing dataset as testing data. We also normalize the pixel representation from 0-255 to 0-1. The format for model input is (image_index, row, column, channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image,label=readfile_train()\n",
    "train_img=get_images(image,60000)/255.0\n",
    "train_label=get_labels(label,60000)\n",
    "image_t,label_t=readfile_train()\n",
    "test_img=get_images(image_t,10000)/255.0\n",
    "test_label=get_labels(label_t,10000)\n",
    "Y_train_cat = keras.utils.to_categorical(train_label)\n",
    "Y_test_cat = keras.utils.to_categorical(test_label)\n",
    "\n",
    "train_img784=train_img.reshape(-1,28,28,1)\n",
    "test_img784=test_img.reshape(-1,28,28,1)\n",
    "del image,label,image_t,label_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our Convolutional Neural Network (CNN) model for MNIST dataset. It is implemented by two convolutional layers and two full connection layers. The activation functions of all layers except the last layer are ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_cnn(input_shape):\n",
    "    model_full=keras.Sequential()\n",
    "    #conv 1\n",
    "    model_full.add(keras.layers.Conv2D(filters=32,kernel_size = 5,strides = (1,1),\n",
    "                                  padding = 'same',activation = tf.nn.relu,input_shape = input_shape))\n",
    "    model_full.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (2,2), padding = 'valid'))\n",
    "    #conv 2\n",
    "    model_full.add(keras.layers.Conv2D(filters=64,kernel_size = 3,strides = (1,1),\n",
    "                                       padding = 'same',activation = tf.nn.relu))\n",
    "    model_full.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (2,2), padding = 'valid'))\n",
    "    model_full.add(keras.layers.Dropout(0.25))\n",
    "    model_full.add(keras.layers.Flatten())\n",
    "    #fc\n",
    "    model_full.add(keras.layers.Dense(units=128,activation = tf.nn.relu))\n",
    "    model_full.add(keras.layers.Dropout(0.5))\n",
    "    #softmax\n",
    "    model_full.add(keras.layers.Dense(units=10,activation = tf.nn.softmax))\n",
    "    return model_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create our model and use adam optimizer to compile our model.   \n",
    "First, we test our CNN model on MNIST dataset where all training data are labeled.  \n",
    "The accuracy on testing dataset and loss during the training are recorded for plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 573us/step - loss: 0.1932 - acc: 0.9401\n",
      "epoch:0, accruracy in testing dataset(full labeled): 0.9862\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 565us/step - loss: 0.0766 - acc: 0.9773\n",
      "epoch:1, accruracy in testing dataset(full labeled): 0.9907\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 33s 555us/step - loss: 0.0590 - acc: 0.9826\n",
      "epoch:2, accruracy in testing dataset(full labeled): 0.993\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 563us/step - loss: 0.0494 - acc: 0.9849\n",
      "epoch:3, accruracy in testing dataset(full labeled): 0.9945\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 562us/step - loss: 0.0417 - acc: 0.9871\n",
      "epoch:4, accruracy in testing dataset(full labeled): 0.9942\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 565us/step - loss: 0.0379 - acc: 0.9885\n",
      "epoch:5, accruracy in testing dataset(full labeled): 0.9957\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 562us/step - loss: 0.0341 - acc: 0.9894\n",
      "epoch:6, accruracy in testing dataset(full labeled): 0.9947\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 560us/step - loss: 0.0324 - acc: 0.9906\n",
      "epoch:7, accruracy in testing dataset(full labeled): 0.9972\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 562us/step - loss: 0.0297 - acc: 0.9910\n",
      "epoch:8, accruracy in testing dataset(full labeled): 0.9971\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 560us/step - loss: 0.0265 - acc: 0.9914\n",
      "epoch:9, accruracy in testing dataset(full labeled): 0.9966\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 562us/step - loss: 0.0253 - acc: 0.9923\n",
      "epoch:10, accruracy in testing dataset(full labeled): 0.9986\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 561us/step - loss: 0.0233 - acc: 0.9924\n",
      "epoch:11, accruracy in testing dataset(full labeled): 0.9978\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 562us/step - loss: 0.0210 - acc: 0.9932\n",
      "epoch:12, accruracy in testing dataset(full labeled): 0.9985\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 566us/step - loss: 0.0211 - acc: 0.9933\n",
      "epoch:13, accruracy in testing dataset(full labeled): 0.9983\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 34s 565us/step - loss: 0.0203 - acc: 0.9935\n",
      "epoch:14, accruracy in testing dataset(full labeled): 0.9984\n"
     ]
    }
   ],
   "source": [
    "model_cnn_mnist_full_acc_trace=[]\n",
    "model_cnn_mnist_full_loss_trace=[]\n",
    "model_cnn_mnist_full=mnist_cnn((28,28,1))\n",
    "model_cnn_mnist_full.compile('adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "for i in range(15):\n",
    "    model_cnn_mnist_full.fit(train_img784 ,Y_train_cat,epochs=1)\n",
    "    y_pred  = model_cnn_mnist_full.predict( test_img784 ).argmax(-1)\n",
    "    acc=accuracy_score(test_label , y_pred)\n",
    "    print(f\"epoch:{i}, accruracy in testing dataset(full labeled): {acc}\")\n",
    "    model_cnn_mnist_full_acc_trace.append(acc)\n",
    "    model_cnn_mnist_full_loss_trace.append(model_cnn_mnist_full.history.history['loss'][-1])\n",
    "#del model_cnn_mnist_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we try training the same CNN model but only use 200 labeled MNIST data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train our model, we have to shuffle the dataset and select the first 200 images as labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(train_img784.shape[0])\n",
    "train_img784 = train_img784[permutation, :, :, :]\n",
    "Y_train_cat = Y_train_cat[permutation,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 2.2765 - acc: 0.1550\n",
      "epoch:0, accruracy in testing dataset(200 labeled): 0.2263\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 565us/step - loss: 2.1064 - acc: 0.3050\n",
      "epoch:1, accruracy in testing dataset(200 labeled): 0.3567\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 639us/step - loss: 1.8173 - acc: 0.4250\n",
      "epoch:2, accruracy in testing dataset(200 labeled): 0.6024\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 606us/step - loss: 1.4769 - acc: 0.5850\n",
      "epoch:3, accruracy in testing dataset(200 labeled): 0.6632\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 600us/step - loss: 1.2496 - acc: 0.6050\n",
      "epoch:4, accruracy in testing dataset(200 labeled): 0.6984\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 605us/step - loss: 1.0465 - acc: 0.6450\n",
      "epoch:5, accruracy in testing dataset(200 labeled): 0.6857\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 727us/step - loss: 0.8842 - acc: 0.7150\n",
      "epoch:6, accruracy in testing dataset(200 labeled): 0.7903\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 636us/step - loss: 0.6992 - acc: 0.7500\n",
      "epoch:7, accruracy in testing dataset(200 labeled): 0.809\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 597us/step - loss: 0.7113 - acc: 0.7800\n",
      "epoch:8, accruracy in testing dataset(200 labeled): 0.8175\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 573us/step - loss: 0.5207 - acc: 0.8350\n",
      "epoch:9, accruracy in testing dataset(200 labeled): 0.8543\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 578us/step - loss: 0.5635 - acc: 0.8200\n",
      "epoch:10, accruracy in testing dataset(200 labeled): 0.858\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 605us/step - loss: 0.4195 - acc: 0.8650\n",
      "epoch:11, accruracy in testing dataset(200 labeled): 0.8483\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 629us/step - loss: 0.3546 - acc: 0.9000\n",
      "epoch:12, accruracy in testing dataset(200 labeled): 0.8383\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 542us/step - loss: 0.3586 - acc: 0.8800\n",
      "epoch:13, accruracy in testing dataset(200 labeled): 0.8582\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 588us/step - loss: 0.3092 - acc: 0.8900\n",
      "epoch:14, accruracy in testing dataset(200 labeled): 0.8425\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 540us/step - loss: 0.2989 - acc: 0.8950\n",
      "epoch:15, accruracy in testing dataset(200 labeled): 0.8641\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 565us/step - loss: 0.2651 - acc: 0.9050\n",
      "epoch:16, accruracy in testing dataset(200 labeled): 0.8694\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 659us/step - loss: 0.2237 - acc: 0.9400\n",
      "epoch:17, accruracy in testing dataset(200 labeled): 0.8773\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 592us/step - loss: 0.1941 - acc: 0.9500\n",
      "epoch:18, accruracy in testing dataset(200 labeled): 0.8612\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 661us/step - loss: 0.1878 - acc: 0.9550\n",
      "epoch:19, accruracy in testing dataset(200 labeled): 0.8817\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 589us/step - loss: 0.1978 - acc: 0.9350\n",
      "epoch:20, accruracy in testing dataset(200 labeled): 0.8776\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 595us/step - loss: 0.1190 - acc: 0.9650\n",
      "epoch:21, accruracy in testing dataset(200 labeled): 0.8588\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 596us/step - loss: 0.1534 - acc: 0.9550\n",
      "epoch:22, accruracy in testing dataset(200 labeled): 0.8763\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 577us/step - loss: 0.1327 - acc: 0.9550\n",
      "epoch:23, accruracy in testing dataset(200 labeled): 0.8788\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 589us/step - loss: 0.0857 - acc: 0.9800\n",
      "epoch:24, accruracy in testing dataset(200 labeled): 0.8653\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 567us/step - loss: 0.1389 - acc: 0.9650\n",
      "epoch:25, accruracy in testing dataset(200 labeled): 0.8566\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 621us/step - loss: 0.1029 - acc: 0.9750\n",
      "epoch:26, accruracy in testing dataset(200 labeled): 0.8861\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 553us/step - loss: 0.1270 - acc: 0.9550\n",
      "epoch:27, accruracy in testing dataset(200 labeled): 0.8819\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 630us/step - loss: 0.0873 - acc: 0.9600\n",
      "epoch:28, accruracy in testing dataset(200 labeled): 0.8828\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 583us/step - loss: 0.1104 - acc: 0.9700\n",
      "epoch:29, accruracy in testing dataset(200 labeled): 0.8791\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 585us/step - loss: 0.0653 - acc: 0.9850\n",
      "epoch:30, accruracy in testing dataset(200 labeled): 0.8807\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 625us/step - loss: 0.0721 - acc: 0.9800\n",
      "epoch:31, accruracy in testing dataset(200 labeled): 0.8605\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 595us/step - loss: 0.0784 - acc: 0.9750\n",
      "epoch:32, accruracy in testing dataset(200 labeled): 0.874\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 609us/step - loss: 0.0742 - acc: 0.9750\n",
      "epoch:33, accruracy in testing dataset(200 labeled): 0.8774\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 584us/step - loss: 0.0393 - acc: 0.9950\n",
      "epoch:34, accruracy in testing dataset(200 labeled): 0.8849\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 657us/step - loss: 0.0792 - acc: 0.9800\n",
      "epoch:35, accruracy in testing dataset(200 labeled): 0.8799\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 681us/step - loss: 0.1031 - acc: 0.9650\n",
      "epoch:36, accruracy in testing dataset(200 labeled): 0.8755\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 580us/step - loss: 0.0873 - acc: 0.9800\n",
      "epoch:37, accruracy in testing dataset(200 labeled): 0.885\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 586us/step - loss: 0.0557 - acc: 0.9750\n",
      "epoch:38, accruracy in testing dataset(200 labeled): 0.8858\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 614us/step - loss: 0.0516 - acc: 0.9850\n",
      "epoch:39, accruracy in testing dataset(200 labeled): 0.8914\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 584us/step - loss: 0.0417 - acc: 0.9950\n",
      "epoch:40, accruracy in testing dataset(200 labeled): 0.8864\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 612us/step - loss: 0.0537 - acc: 0.9850\n",
      "epoch:41, accruracy in testing dataset(200 labeled): 0.8862\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 598us/step - loss: 0.0221 - acc: 1.0000\n",
      "epoch:42, accruracy in testing dataset(200 labeled): 0.8835\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 617us/step - loss: 0.0575 - acc: 0.9800\n",
      "epoch:43, accruracy in testing dataset(200 labeled): 0.883\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 577us/step - loss: 0.0591 - acc: 0.9800\n",
      "epoch:44, accruracy in testing dataset(200 labeled): 0.8806\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 608us/step - loss: 0.0338 - acc: 0.9900\n",
      "epoch:45, accruracy in testing dataset(200 labeled): 0.8792\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 634us/step - loss: 0.0781 - acc: 0.9850\n",
      "epoch:46, accruracy in testing dataset(200 labeled): 0.881\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 565us/step - loss: 0.0459 - acc: 0.9900\n",
      "epoch:47, accruracy in testing dataset(200 labeled): 0.8873\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 574us/step - loss: 0.0500 - acc: 0.9850\n",
      "epoch:48, accruracy in testing dataset(200 labeled): 0.8895\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 593us/step - loss: 0.0195 - acc: 1.0000\n",
      "epoch:49, accruracy in testing dataset(200 labeled): 0.8858\n"
     ]
    }
   ],
   "source": [
    "model_cnn_mnist_200_acc_trace=[]\n",
    "model_cnn_mnist_200_loss_trace=[]\n",
    "model_cnn_mnist_200=mnist_cnn((28,28,1))\n",
    "model_cnn_mnist_200.compile('adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "for i in range(50):\n",
    "    model_cnn_mnist_200.fit(train_img784[0:200,:],Y_train_cat[0:200,:] ,epochs=1)\n",
    "    y_pred  =model_cnn_mnist_200.predict(test_img784).argmax(-1)\n",
    "    acc=accuracy_score(test_label , y_pred)\n",
    "    print(f\"epoch:{i}, accruracy in testing dataset(200 labeled): {acc}\")\n",
    "    model_cnn_mnist_200_acc_trace.append(acc)\n",
    "    model_cnn_mnist_200_loss_trace.append(model_cnn_mnist_200.history.history['loss'][-1])\n",
    "#del model_cnn_mnist_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAT on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy shows that using CNN on semi-supervised learning is not good. We implement a model using VAT which is talked in [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easily calculating the adversarial perturbation, the model input shape is changed to (28×28,), the CNN model used in this VAT model is the same as the model used in M3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img7784=train_img.reshape(-1,784)\n",
    "test_img7784=test_img.reshape(-1,784)\n",
    "train_img7784 = train_img7784[permutation, :]\n",
    "del train_img,test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function used to calculate Kullback-Leibler divergence (relative entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p_logit, q_logit):#KL divergence\n",
    "    p = tf.nn.softmax(p_logit)\n",
    "    q = tf.nn.softmax(q_logit)\n",
    "    return tf.reduce_sum(p*(tf.log(p + 1e-16) - tf.log(q + 1e-16)), axis=1)\n",
    "def unit_norm(x):\n",
    "    return x/(tf.reshape(tf.sqrt(tf.reduce_sum(tf.pow(x, 2.0), axis=1)), [-1, 1]) + 1e-16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build our VAT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vat_mnist_full(epsilon=10,alpha=0.3):\n",
    "    network = Sequential()\n",
    "    input_s=(784,)\n",
    "    network.add(keras.layers.Reshape((28,28,1),input_shape = (784,)))\n",
    "    network.add(keras.layers.Conv2D(filters=32,kernel_size = 5,strides = (1,1),padding = 'same',activation = tf.nn.relu))\n",
    "    network.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (2,2), padding = 'valid'))\n",
    "    network.add(keras.layers.Conv2D(filters=64,kernel_size = 3,strides = (1,1),padding = 'same',activation = tf.nn.relu))\n",
    "    network.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (2,2), padding = 'valid'))\n",
    "    network.add(keras.layers.Dropout(0.25))\n",
    "    network.add(keras.layers.Flatten())\n",
    "    network.add(keras.layers.Dense(units=128,activation = tf.nn.relu))\n",
    "    network.add(keras.layers.Dropout(0.5))\n",
    "    network.add(keras.layers.Dense(units=10))\n",
    "\n",
    "    model_input = Input(input_s)\n",
    "    p_logit = network( model_input )\n",
    "    p = Activation('softmax')(p_logit)\n",
    "    r_adv = tf.random_normal(shape=tf.shape(model_input))\n",
    "    r_adv = unit_norm(r_adv)\n",
    "    p_logit_r = network(model_input+epsilon*r_adv)\n",
    "    kld = tf.reduce_mean(kl_divergence(p_logit,p_logit_r))\n",
    "    grad_kld = tf.gradients(kld,[r_adv])[0]\n",
    "    r_vadv = tf.stop_gradient(grad_kld)\n",
    "    r_vadv = unit_norm(r_vadv)*alpha\n",
    "    p_logit_no_gradient = tf.stop_gradient(p_logit)\n",
    "    p_logit_r_adv = network(model_input+r_vadv)\n",
    "    vat_loss = tf.reduce_mean(kl_divergence(p_logit_no_gradient,p_logit_r_adv))\n",
    "    model_vat = Model(model_input,p)\n",
    "    model_vat.add_loss(vat_loss)\n",
    "    model_vat.compile('sgd','categorical_crossentropy',metrics=['accuracy'])\n",
    "    model_vat.metrics_names.append('VAT_loss')\n",
    "    model_vat.metrics_tensors.append(vat_loss)\n",
    "    return model_vat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use SGD optimizer to compile our model.  \n",
    "First, we test our VAT model on MNIST dataset where all training data are labeled.  \n",
    "The accuracy on testing dataset and loss (include total loss and VAT loss) during the training are recorded for plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 1.0318 - acc: 0.7763 - VAT_loss: 0.2450\n",
      "epoch:0, accruracy in testing dataset(full labeled): 0.9488\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 0.4429 - acc: 0.9394 - VAT_loss: 0.1944\n",
      "epoch:1, accruracy in testing dataset(full labeled): 0.9663\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.3390 - acc: 0.9561 - VAT_loss: 0.1576\n",
      "epoch:2, accruracy in testing dataset(full labeled): 0.9755\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.2844 - acc: 0.9645 - VAT_loss: 0.1382\n",
      "epoch:3, accruracy in testing dataset(full labeled): 0.9782\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.2530 - acc: 0.9696 - VAT_loss: 0.1254\n",
      "epoch:4, accruracy in testing dataset(full labeled): 0.9808\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.2312 - acc: 0.9720 - VAT_loss: 0.1154\n",
      "epoch:5, accruracy in testing dataset(full labeled): 0.9824\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.2129 - acc: 0.9740 - VAT_loss: 0.1075\n",
      "epoch:6, accruracy in testing dataset(full labeled): 0.9836\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.1986 - acc: 0.9759 - VAT_loss: 0.1019\n",
      "epoch:7, accruracy in testing dataset(full labeled): 0.9832\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1881 - acc: 0.9776 - VAT_loss: 0.0974\n",
      "epoch:8, accruracy in testing dataset(full labeled): 0.9843\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1799 - acc: 0.9788 - VAT_loss: 0.0936\n",
      "epoch:9, accruracy in testing dataset(full labeled): 0.9852\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1711 - acc: 0.9807 - VAT_loss: 0.0901\n",
      "epoch:10, accruracy in testing dataset(full labeled): 0.9857\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1663 - acc: 0.9809 - VAT_loss: 0.0878\n",
      "epoch:11, accruracy in testing dataset(full labeled): 0.9869\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.1591 - acc: 0.9814 - VAT_loss: 0.0844\n",
      "epoch:12, accruracy in testing dataset(full labeled): 0.988\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.1538 - acc: 0.9831 - VAT_loss: 0.0819\n",
      "epoch:13, accruracy in testing dataset(full labeled): 0.988\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.1493 - acc: 0.9835 - VAT_loss: 0.0800\n",
      "epoch:14, accruracy in testing dataset(full labeled): 0.9885\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1423 - acc: 0.9844 - VAT_loss: 0.0770\n",
      "epoch:15, accruracy in testing dataset(full labeled): 0.989\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1384 - acc: 0.9841 - VAT_loss: 0.0750\n",
      "epoch:16, accruracy in testing dataset(full labeled): 0.9898\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.1356 - acc: 0.9845 - VAT_loss: 0.0734\n",
      "epoch:17, accruracy in testing dataset(full labeled): 0.9899\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.1310 - acc: 0.9855 - VAT_loss: 0.0716\n",
      "epoch:18, accruracy in testing dataset(full labeled): 0.9899\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1260 - acc: 0.9861 - VAT_loss: 0.0691\n",
      "epoch:19, accruracy in testing dataset(full labeled): 0.9904\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.1284 - acc: 0.9862 - VAT_loss: 0.0703\n",
      "epoch:20, accruracy in testing dataset(full labeled): 0.9906\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1220 - acc: 0.9862 - VAT_loss: 0.0668\n",
      "epoch:21, accruracy in testing dataset(full labeled): 0.9905\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1215 - acc: 0.9866 - VAT_loss: 0.0673\n",
      "epoch:22, accruracy in testing dataset(full labeled): 0.9912\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.1171 - acc: 0.9871 - VAT_loss: 0.0647\n",
      "epoch:23, accruracy in testing dataset(full labeled): 0.9907\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.1152 - acc: 0.9876 - VAT_loss: 0.0638\n",
      "epoch:24, accruracy in testing dataset(full labeled): 0.9913\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.1146 - acc: 0.9874 - VAT_loss: 0.0639\n",
      "epoch:25, accruracy in testing dataset(full labeled): 0.9918\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1107 - acc: 0.9881 - VAT_loss: 0.0617\n",
      "epoch:26, accruracy in testing dataset(full labeled): 0.9918\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.1081 - acc: 0.9883 - VAT_loss: 0.0602\n",
      "epoch:27, accruracy in testing dataset(full labeled): 0.9924\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.1063 - acc: 0.9889 - VAT_loss: 0.0598\n",
      "epoch:28, accruracy in testing dataset(full labeled): 0.9923\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.1053 - acc: 0.9893 - VAT_loss: 0.0596\n",
      "epoch:29, accruracy in testing dataset(full labeled): 0.9926\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.1045 - acc: 0.9892 - VAT_loss: 0.0586\n",
      "epoch:30, accruracy in testing dataset(full labeled): 0.9927\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0996 - acc: 0.9896 - VAT_loss: 0.0566\n",
      "epoch:31, accruracy in testing dataset(full labeled): 0.9931\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0995 - acc: 0.9892 - VAT_loss: 0.0565\n",
      "epoch:32, accruracy in testing dataset(full labeled): 0.9931\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0968 - acc: 0.9901 - VAT_loss: 0.0554\n",
      "epoch:33, accruracy in testing dataset(full labeled): 0.9932\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0973 - acc: 0.9892 - VAT_loss: 0.0550\n",
      "epoch:34, accruracy in testing dataset(full labeled): 0.9938\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0977 - acc: 0.9901 - VAT_loss: 0.0554\n",
      "epoch:35, accruracy in testing dataset(full labeled): 0.9932\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0976 - acc: 0.9903 - VAT_loss: 0.0561\n",
      "epoch:36, accruracy in testing dataset(full labeled): 0.9941\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0941 - acc: 0.9899 - VAT_loss: 0.0539\n",
      "epoch:37, accruracy in testing dataset(full labeled): 0.9935\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0917 - acc: 0.9905 - VAT_loss: 0.0520\n",
      "epoch:38, accruracy in testing dataset(full labeled): 0.9936\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0923 - acc: 0.9904 - VAT_loss: 0.0530\n",
      "epoch:39, accruracy in testing dataset(full labeled): 0.994\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0931 - acc: 0.9905 - VAT_loss: 0.0537\n",
      "epoch:40, accruracy in testing dataset(full labeled): 0.9943\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0908 - acc: 0.9907 - VAT_loss: 0.0526\n",
      "epoch:41, accruracy in testing dataset(full labeled): 0.9938\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0862 - acc: 0.9916 - VAT_loss: 0.0498\n",
      "epoch:42, accruracy in testing dataset(full labeled): 0.9946\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0864 - acc: 0.9912 - VAT_loss: 0.0498\n",
      "epoch:43, accruracy in testing dataset(full labeled): 0.9948\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0852 - acc: 0.9914 - VAT_loss: 0.0497\n",
      "epoch:44, accruracy in testing dataset(full labeled): 0.9945\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0871 - acc: 0.9908 - VAT_loss: 0.0500\n",
      "epoch:45, accruracy in testing dataset(full labeled): 0.9953\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0830 - acc: 0.9919 - VAT_loss: 0.0479\n",
      "epoch:46, accruracy in testing dataset(full labeled): 0.9944\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0829 - acc: 0.9914 - VAT_loss: 0.0483\n",
      "epoch:47, accruracy in testing dataset(full labeled): 0.995\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0808 - acc: 0.9917 - VAT_loss: 0.0470\n",
      "epoch:48, accruracy in testing dataset(full labeled): 0.9944\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0825 - acc: 0.9921 - VAT_loss: 0.0485\n",
      "epoch:49, accruracy in testing dataset(full labeled): 0.9953\n"
     ]
    }
   ],
   "source": [
    "model_vat_mnist_full_acc_trace=[]\n",
    "model_vat_mnist_full_loss_trace=[]\n",
    "model_vat_mnist_full_vatloss_trace=[]\n",
    "model_vat_mnist_full=vat_mnist_full(epsilon=10,alpha=0.3)\n",
    "for i in range(50):\n",
    "    model_vat_mnist_full.fit(train_img7784,Y_train_cat,epochs=1)\n",
    "    y_pred  =model_vat_mnist_full.predict(test_img7784).argmax(-1)\n",
    "    acc=accuracy_score(test_label,y_pred)\n",
    "    print(f\"epoch:{i}, accruracy in testing dataset(full labeled): {acc}\")\n",
    "    model_vat_mnist_full_acc_trace.append(acc)\n",
    "    model_vat_mnist_full_loss_trace.append(model_vat_mnist_full.history.history['loss'][-1])\n",
    "    model_vat_mnist_full_vatloss_trace.append(model_vat_mnist_full.history.history['VAT_loss'][-1])\n",
    "#del model_vat_mnist_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to know the VAT based model performance when only using 200 labeled MNIST data.  \n",
    "For this purpose, we modify our model. The model inputs are changed to \n",
    "(200 unlabeled images, 200 labeled images, 200 labels of the labeled images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing our model, we have to define some function to calculate cross entropy and entropy which are talked about in [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(logit, y):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y))\n",
    "def logsoftmax(x):\n",
    "    xdev = x - tf.reduce_max(x, 1, keepdims=True)\n",
    "    lsm = xdev - tf.log(tf.reduce_sum(tf.exp(xdev), 1, keepdims=True))\n",
    "    return lsm\n",
    "def entropy_y_x(logit):\n",
    "    p = tf.nn.softmax(logit)\n",
    "    return -tf.reduce_mean(tf.reduce_sum(p * logsoftmax(logit), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vat_mnist_200(epsilon=1,alpha=0.3):\n",
    "    network_200 = Sequential()\n",
    "    input_shape_200=(784,)\n",
    "    network_200.add(keras.layers.Reshape((28,28,1),input_shape = input_shape_200))\n",
    "    network_200.add(keras.layers.Conv2D(filters=32,kernel_size = 5,strides = (1,1),\n",
    "                                  padding = 'same',activation = tf.nn.relu))\n",
    "    network_200.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (2,2), padding = 'valid'))\n",
    "    network_200.add(keras.layers.Conv2D(filters=64,kernel_size = 3,strides = (1,1),padding = 'same',activation = tf.nn.relu))\n",
    "    network_200.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (2,2), padding = 'valid'))\n",
    "    network_200.add(keras.layers.Dropout(0.25))\n",
    "    network_200.add(keras.layers.Flatten())\n",
    "    network_200.add(keras.layers.Dense(units=128,activation = tf.nn.relu))\n",
    "    network_200.add(keras.layers.Dropout(0.5))\n",
    "    network_200.add(keras.layers.Dense(units=10))\n",
    "\n",
    "    model_input_200_unlabeled = Input((784,))\n",
    "    model_input_200labels=Input((10,))\n",
    "    model_input_200_labeled=Input((784,))\n",
    "\n",
    "    p_logit_200 = network_200(model_input_200_unlabeled)\n",
    "    p_200 = Activation('softmax')(p_logit_200)\n",
    "    logit = network_200(model_input_200_labeled)\n",
    "    nll_loss = ce_loss(logit, model_input_200labels)\n",
    "    ul_logit = network_200(model_input_200_unlabeled)\n",
    "    r_200 = tf.random_normal(shape=tf.shape(model_input_200_unlabeled))\n",
    "    r_200 = unit_norm(r_200)\n",
    "    p_logit_r_200 = network_200(model_input_200_unlabeled+epsilon*r_200)\n",
    "    kld_200 = tf.reduce_mean(kl_divergence(p_logit_200,p_logit_r_200))\n",
    "    grad_kld_200 = tf.gradients(kld_200,[r_200])[0]\n",
    "    r_vadv_200 = tf.stop_gradient(grad_kld_200)\n",
    "    r_vadv_200 = unit_norm(r_vadv_200)*alpha\n",
    "    p_logit_no_gradient_200 = tf.stop_gradient(p_logit_200)\n",
    "    p_logit_r_adv_200 = network_200(model_input_200_unlabeled+r_vadv_200)\n",
    "    vat_loss =  tf.reduce_mean(kl_divergence(p_logit_no_gradient_200,p_logit_r_adv_200))\n",
    "    ent_loss = entropy_y_x(ul_logit)\n",
    "    additional_loss = vat_loss + ent_loss\n",
    "    loss_f = nll_loss + additional_loss\n",
    "    model_vat_200 = Model([model_input_200_unlabeled,model_input_200_labeled,model_input_200labels],p_200)\n",
    "    model_vat_200.add_loss(loss_f)\n",
    "    model_vat_200.compile( 'sgd',None,metrics=['accuracy'])\n",
    "    model_vat_200.metrics_names.append('total_loss')\n",
    "    model_vat_200.metrics_tensors.append(loss_f)\n",
    "    return model_vat_200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use SGD optimizer to compile our model.  \n",
    "We test our VAT model on MNIST dataset where only 200 training data are labeled.  \n",
    "The accuracy on testing dataset and total loss during the training are recorded for plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model has three inputs (n unlabeled images,n labeled images, n labels of the labeled images), when we use our model for prediction, we have to give it an useless label as the third input. This label does not influence the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_useless=np.array([9]*len(test_label))\n",
    "test_label_useless = keras.utils.to_categorical(test_label_useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-e3c5ec9d56a9>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 4.6147 - total_loss: 4.6147\n",
      "epoch:1, accruracy in testing dataset(200 labeled): 0.119\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.6045 - total_loss: 4.6045\n",
      "epoch:2, accruracy in testing dataset(200 labeled): 0.1329\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.5977 - total_loss: 4.5977\n",
      "epoch:3, accruracy in testing dataset(200 labeled): 0.137\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.5684 - total_loss: 4.5684\n",
      "epoch:4, accruracy in testing dataset(200 labeled): 0.1571\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.5569 - total_loss: 4.5569\n",
      "epoch:5, accruracy in testing dataset(200 labeled): 0.1585\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.5385 - total_loss: 4.5385\n",
      "epoch:6, accruracy in testing dataset(200 labeled): 0.1454\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.5190 - total_loss: 4.5190\n",
      "epoch:7, accruracy in testing dataset(200 labeled): 0.1575\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.5250 - total_loss: 4.5250\n",
      "epoch:8, accruracy in testing dataset(200 labeled): 0.1494\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.4927 - total_loss: 4.4927\n",
      "epoch:9, accruracy in testing dataset(200 labeled): 0.1655\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.4955 - total_loss: 4.4955\n",
      "epoch:10, accruracy in testing dataset(200 labeled): 0.191\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.4660 - total_loss: 4.4660\n",
      "epoch:11, accruracy in testing dataset(200 labeled): 0.2183\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.4827 - total_loss: 4.4827\n",
      "epoch:12, accruracy in testing dataset(200 labeled): 0.2078\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.4321 - total_loss: 4.4321\n",
      "epoch:13, accruracy in testing dataset(200 labeled): 0.1969\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.4070 - total_loss: 4.4070\n",
      "epoch:14, accruracy in testing dataset(200 labeled): 0.1821\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.3761 - total_loss: 4.3761\n",
      "epoch:15, accruracy in testing dataset(200 labeled): 0.1678\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.3674 - total_loss: 4.3674\n",
      "epoch:16, accruracy in testing dataset(200 labeled): 0.1576\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.2832 - total_loss: 4.2832\n",
      "epoch:17, accruracy in testing dataset(200 labeled): 0.1038\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.2031 - total_loss: 4.2031\n",
      "epoch:18, accruracy in testing dataset(200 labeled): 0.1125\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.1274 - total_loss: 4.1274\n",
      "epoch:19, accruracy in testing dataset(200 labeled): 0.0945\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.2497 - total_loss: 4.2497\n",
      "epoch:20, accruracy in testing dataset(200 labeled): 0.1283\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.0776 - total_loss: 4.0776\n",
      "epoch:21, accruracy in testing dataset(200 labeled): 0.1255\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 4.0334 - total_loss: 4.0334\n",
      "epoch:22, accruracy in testing dataset(200 labeled): 0.1471\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.9591 - total_loss: 3.9591\n",
      "epoch:23, accruracy in testing dataset(200 labeled): 0.2473\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.8545 - total_loss: 3.8545\n",
      "epoch:24, accruracy in testing dataset(200 labeled): 0.1989\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.6881 - total_loss: 3.6881\n",
      "epoch:25, accruracy in testing dataset(200 labeled): 0.2258\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.6894 - total_loss: 3.6894\n",
      "epoch:26, accruracy in testing dataset(200 labeled): 0.2432\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.5825 - total_loss: 3.5825\n",
      "epoch:27, accruracy in testing dataset(200 labeled): 0.2715\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.5486 - total_loss: 3.5486\n",
      "epoch:28, accruracy in testing dataset(200 labeled): 0.2996\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.4642 - total_loss: 3.4642\n",
      "epoch:29, accruracy in testing dataset(200 labeled): 0.2997\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.3698 - total_loss: 3.3698\n",
      "epoch:30, accruracy in testing dataset(200 labeled): 0.4185\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.4268 - total_loss: 3.4268\n",
      "epoch:31, accruracy in testing dataset(200 labeled): 0.4228\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.2572 - total_loss: 3.2572\n",
      "epoch:32, accruracy in testing dataset(200 labeled): 0.4778\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.4092 - total_loss: 3.4092\n",
      "epoch:33, accruracy in testing dataset(200 labeled): 0.4854\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.1826 - total_loss: 3.1826\n",
      "epoch:34, accruracy in testing dataset(200 labeled): 0.6136\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.0559 - total_loss: 3.0559\n",
      "epoch:35, accruracy in testing dataset(200 labeled): 0.5373\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.1402 - total_loss: 3.1402\n",
      "epoch:36, accruracy in testing dataset(200 labeled): 0.4791\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 3.1512 - total_loss: 3.1512\n",
      "epoch:37, accruracy in testing dataset(200 labeled): 0.6741\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.8923 - total_loss: 2.8923\n",
      "epoch:38, accruracy in testing dataset(200 labeled): 0.5515\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.8231 - total_loss: 2.8231\n",
      "epoch:39, accruracy in testing dataset(200 labeled): 0.5136\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.9292 - total_loss: 2.9292\n",
      "epoch:40, accruracy in testing dataset(200 labeled): 0.6448\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.8632 - total_loss: 2.8632\n",
      "epoch:41, accruracy in testing dataset(200 labeled): 0.6488\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.7782 - total_loss: 2.7782\n",
      "epoch:42, accruracy in testing dataset(200 labeled): 0.6256\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.7777 - total_loss: 2.7777\n",
      "epoch:43, accruracy in testing dataset(200 labeled): 0.6696\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.4796 - total_loss: 2.4796\n",
      "epoch:44, accruracy in testing dataset(200 labeled): 0.6967\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.5339 - total_loss: 2.5339\n",
      "epoch:45, accruracy in testing dataset(200 labeled): 0.6833\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.5377 - total_loss: 2.5377\n",
      "epoch:46, accruracy in testing dataset(200 labeled): 0.6689\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.2737 - total_loss: 2.2737\n",
      "epoch:47, accruracy in testing dataset(200 labeled): 0.6663\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.3316 - total_loss: 2.3316\n",
      "epoch:48, accruracy in testing dataset(200 labeled): 0.7098\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.3649 - total_loss: 2.3649\n",
      "epoch:49, accruracy in testing dataset(200 labeled): 0.7282\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.3610 - total_loss: 2.3610\n",
      "epoch:50, accruracy in testing dataset(200 labeled): 0.765\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.3167 - total_loss: 2.3167\n",
      "epoch:51, accruracy in testing dataset(200 labeled): 0.6861\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.3528 - total_loss: 2.3528\n",
      "epoch:52, accruracy in testing dataset(200 labeled): 0.7013\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.0809 - total_loss: 2.0809\n",
      "epoch:53, accruracy in testing dataset(200 labeled): 0.7578\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.2984 - total_loss: 2.2984\n",
      "epoch:54, accruracy in testing dataset(200 labeled): 0.7368\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.1915 - total_loss: 2.1915\n",
      "epoch:55, accruracy in testing dataset(200 labeled): 0.7076\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.1987 - total_loss: 2.1987\n",
      "epoch:56, accruracy in testing dataset(200 labeled): 0.772\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.0051 - total_loss: 2.0051\n",
      "epoch:57, accruracy in testing dataset(200 labeled): 0.8016\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.1410 - total_loss: 2.1410\n",
      "epoch:58, accruracy in testing dataset(200 labeled): 0.7181\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.1243 - total_loss: 2.1243\n",
      "epoch:59, accruracy in testing dataset(200 labeled): 0.7851\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.9271 - total_loss: 1.9271\n",
      "epoch:60, accruracy in testing dataset(200 labeled): 0.7597\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.8416 - total_loss: 1.8416\n",
      "epoch:61, accruracy in testing dataset(200 labeled): 0.7733\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.9966 - total_loss: 1.9966\n",
      "epoch:62, accruracy in testing dataset(200 labeled): 0.7902\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.9578 - total_loss: 1.9578\n",
      "epoch:63, accruracy in testing dataset(200 labeled): 0.7998\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.7227 - total_loss: 1.7227\n",
      "epoch:64, accruracy in testing dataset(200 labeled): 0.7901\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.8801 - total_loss: 1.8801\n",
      "epoch:65, accruracy in testing dataset(200 labeled): 0.7973\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.7682 - total_loss: 1.7682\n",
      "epoch:66, accruracy in testing dataset(200 labeled): 0.8152\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.7593 - total_loss: 1.7593\n",
      "epoch:67, accruracy in testing dataset(200 labeled): 0.7675\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.6596 - total_loss: 1.6596\n",
      "epoch:68, accruracy in testing dataset(200 labeled): 0.7729\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.8471 - total_loss: 1.8471\n",
      "epoch:69, accruracy in testing dataset(200 labeled): 0.798\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.8154 - total_loss: 1.8154\n",
      "epoch:70, accruracy in testing dataset(200 labeled): 0.779\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.6551 - total_loss: 1.6551\n",
      "epoch:71, accruracy in testing dataset(200 labeled): 0.7696\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.9426 - total_loss: 1.9426\n",
      "epoch:72, accruracy in testing dataset(200 labeled): 0.7627\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.6183 - total_loss: 1.6183\n",
      "epoch:73, accruracy in testing dataset(200 labeled): 0.7439\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.7682 - total_loss: 1.7682\n",
      "epoch:74, accruracy in testing dataset(200 labeled): 0.7619\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.6191 - total_loss: 1.6191\n",
      "epoch:75, accruracy in testing dataset(200 labeled): 0.8068\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.7481 - total_loss: 1.7481\n",
      "epoch:76, accruracy in testing dataset(200 labeled): 0.8056\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.6393 - total_loss: 1.6393\n",
      "epoch:77, accruracy in testing dataset(200 labeled): 0.7655\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.5878 - total_loss: 1.5878\n",
      "epoch:78, accruracy in testing dataset(200 labeled): 0.8083\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.5114 - total_loss: 1.5114\n",
      "epoch:79, accruracy in testing dataset(200 labeled): 0.8013\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4877 - total_loss: 1.4877\n",
      "epoch:80, accruracy in testing dataset(200 labeled): 0.8059\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.5672 - total_loss: 1.5672\n",
      "epoch:81, accruracy in testing dataset(200 labeled): 0.8254\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3107 - total_loss: 1.3107\n",
      "epoch:82, accruracy in testing dataset(200 labeled): 0.8293\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.6043 - total_loss: 1.6043\n",
      "epoch:83, accruracy in testing dataset(200 labeled): 0.8076\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.5062 - total_loss: 1.5062\n",
      "epoch:84, accruracy in testing dataset(200 labeled): 0.8176\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4843 - total_loss: 1.4843\n",
      "epoch:85, accruracy in testing dataset(200 labeled): 0.83\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.4887 - total_loss: 1.4887\n",
      "epoch:86, accruracy in testing dataset(200 labeled): 0.8307\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4898 - total_loss: 1.4898\n",
      "epoch:87, accruracy in testing dataset(200 labeled): 0.8095\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4165 - total_loss: 1.4165\n",
      "epoch:88, accruracy in testing dataset(200 labeled): 0.8194\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4352 - total_loss: 1.4352\n",
      "epoch:89, accruracy in testing dataset(200 labeled): 0.818\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3756 - total_loss: 1.3756\n",
      "epoch:90, accruracy in testing dataset(200 labeled): 0.8047\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4496 - total_loss: 1.4496\n",
      "epoch:91, accruracy in testing dataset(200 labeled): 0.8229\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4771 - total_loss: 1.4771\n",
      "epoch:92, accruracy in testing dataset(200 labeled): 0.8372\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3782 - total_loss: 1.3782\n",
      "epoch:93, accruracy in testing dataset(200 labeled): 0.8388\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4261 - total_loss: 1.4261\n",
      "epoch:94, accruracy in testing dataset(200 labeled): 0.7708\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.5808 - total_loss: 1.5808\n",
      "epoch:95, accruracy in testing dataset(200 labeled): 0.8179\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.4607 - total_loss: 1.4607\n",
      "epoch:96, accruracy in testing dataset(200 labeled): 0.8302\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.3905 - total_loss: 1.3905\n",
      "epoch:97, accruracy in testing dataset(200 labeled): 0.8278\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3212 - total_loss: 1.3212\n",
      "epoch:98, accruracy in testing dataset(200 labeled): 0.7826\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2788 - total_loss: 1.2788\n",
      "epoch:99, accruracy in testing dataset(200 labeled): 0.8208\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2471 - total_loss: 1.2471\n",
      "epoch:100, accruracy in testing dataset(200 labeled): 0.8513\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3159 - total_loss: 1.3159\n",
      "epoch:101, accruracy in testing dataset(200 labeled): 0.8273\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2536 - total_loss: 1.2536\n",
      "epoch:102, accruracy in testing dataset(200 labeled): 0.8428\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2921 - total_loss: 1.2921\n",
      "epoch:103, accruracy in testing dataset(200 labeled): 0.8458\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2875 - total_loss: 1.2875\n",
      "epoch:104, accruracy in testing dataset(200 labeled): 0.7915\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2831 - total_loss: 1.2831\n",
      "epoch:105, accruracy in testing dataset(200 labeled): 0.8417\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.3383 - total_loss: 1.3383\n",
      "epoch:106, accruracy in testing dataset(200 labeled): 0.8429\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.1973 - total_loss: 1.1973\n",
      "epoch:107, accruracy in testing dataset(200 labeled): 0.8541\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2598 - total_loss: 1.2598\n",
      "epoch:108, accruracy in testing dataset(200 labeled): 0.8355\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2090 - total_loss: 1.2090\n",
      "epoch:109, accruracy in testing dataset(200 labeled): 0.8547\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2267 - total_loss: 1.2267\n",
      "epoch:110, accruracy in testing dataset(200 labeled): 0.8202\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2469 - total_loss: 1.2469\n",
      "epoch:111, accruracy in testing dataset(200 labeled): 0.8436\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.1441 - total_loss: 1.1441\n",
      "epoch:112, accruracy in testing dataset(200 labeled): 0.8614\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9657 - total_loss: 0.9657\n",
      "epoch:113, accruracy in testing dataset(200 labeled): 0.8587\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.1838 - total_loss: 1.1838\n",
      "epoch:114, accruracy in testing dataset(200 labeled): 0.8453\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.1796 - total_loss: 1.1796\n",
      "epoch:115, accruracy in testing dataset(200 labeled): 0.8633\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0409 - total_loss: 1.0409\n",
      "epoch:116, accruracy in testing dataset(200 labeled): 0.8536\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.1259 - total_loss: 1.1259\n",
      "epoch:117, accruracy in testing dataset(200 labeled): 0.8611\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2989 - total_loss: 1.2989\n",
      "epoch:118, accruracy in testing dataset(200 labeled): 0.8462\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0232 - total_loss: 1.0232\n",
      "epoch:119, accruracy in testing dataset(200 labeled): 0.8596\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.1458 - total_loss: 1.1458\n",
      "epoch:120, accruracy in testing dataset(200 labeled): 0.8558\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2316 - total_loss: 1.2316\n",
      "epoch:121, accruracy in testing dataset(200 labeled): 0.8629\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.1057 - total_loss: 1.1057\n",
      "epoch:122, accruracy in testing dataset(200 labeled): 0.8652\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0992 - total_loss: 1.0992\n",
      "epoch:123, accruracy in testing dataset(200 labeled): 0.8653\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0680 - total_loss: 1.0680\n",
      "epoch:124, accruracy in testing dataset(200 labeled): 0.8591\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.1015 - total_loss: 1.1015\n",
      "epoch:125, accruracy in testing dataset(200 labeled): 0.842\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.1328 - total_loss: 1.1328\n",
      "epoch:126, accruracy in testing dataset(200 labeled): 0.8629\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9740 - total_loss: 0.9740\n",
      "epoch:127, accruracy in testing dataset(200 labeled): 0.842\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9335 - total_loss: 0.9335\n",
      "epoch:128, accruracy in testing dataset(200 labeled): 0.8377\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0185 - total_loss: 1.0185\n",
      "epoch:129, accruracy in testing dataset(200 labeled): 0.8557\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9999 - total_loss: 0.9999\n",
      "epoch:130, accruracy in testing dataset(200 labeled): 0.8596\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0397 - total_loss: 1.0397\n",
      "epoch:131, accruracy in testing dataset(200 labeled): 0.8633\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9728 - total_loss: 0.9728\n",
      "epoch:132, accruracy in testing dataset(200 labeled): 0.8304\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9376 - total_loss: 0.9376\n",
      "epoch:133, accruracy in testing dataset(200 labeled): 0.8424\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.1358 - total_loss: 1.1358\n",
      "epoch:134, accruracy in testing dataset(200 labeled): 0.8393\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.2484 - total_loss: 1.2484\n",
      "epoch:135, accruracy in testing dataset(200 labeled): 0.867\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0871 - total_loss: 1.0871\n",
      "epoch:136, accruracy in testing dataset(200 labeled): 0.8604\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0330 - total_loss: 1.0330\n",
      "epoch:137, accruracy in testing dataset(200 labeled): 0.8565\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.0133 - total_loss: 1.0133\n",
      "epoch:138, accruracy in testing dataset(200 labeled): 0.8485\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9157 - total_loss: 0.9157\n",
      "epoch:139, accruracy in testing dataset(200 labeled): 0.8564\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9686 - total_loss: 0.9686\n",
      "epoch:140, accruracy in testing dataset(200 labeled): 0.8667\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9937 - total_loss: 0.9937\n",
      "epoch:141, accruracy in testing dataset(200 labeled): 0.8631\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0154 - total_loss: 1.0154\n",
      "epoch:142, accruracy in testing dataset(200 labeled): 0.8643\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0271 - total_loss: 1.0271\n",
      "epoch:143, accruracy in testing dataset(200 labeled): 0.8465\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9642 - total_loss: 0.9642\n",
      "epoch:144, accruracy in testing dataset(200 labeled): 0.8804\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9736 - total_loss: 0.9736\n",
      "epoch:145, accruracy in testing dataset(200 labeled): 0.8663\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7797 - total_loss: 0.7797\n",
      "epoch:146, accruracy in testing dataset(200 labeled): 0.8738\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0419 - total_loss: 1.0419\n",
      "epoch:147, accruracy in testing dataset(200 labeled): 0.8696\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9677 - total_loss: 0.9677\n",
      "epoch:148, accruracy in testing dataset(200 labeled): 0.849\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0082 - total_loss: 1.0082\n",
      "epoch:149, accruracy in testing dataset(200 labeled): 0.8702\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8767 - total_loss: 0.8767\n",
      "epoch:150, accruracy in testing dataset(200 labeled): 0.8501\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9405 - total_loss: 0.9405\n",
      "epoch:151, accruracy in testing dataset(200 labeled): 0.8598\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9451 - total_loss: 0.9451\n",
      "epoch:152, accruracy in testing dataset(200 labeled): 0.8627\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0156 - total_loss: 1.0156\n",
      "epoch:153, accruracy in testing dataset(200 labeled): 0.8678\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9671 - total_loss: 0.9671\n",
      "epoch:154, accruracy in testing dataset(200 labeled): 0.8693\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9830 - total_loss: 0.9830\n",
      "epoch:155, accruracy in testing dataset(200 labeled): 0.8559\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9591 - total_loss: 0.9591\n",
      "epoch:156, accruracy in testing dataset(200 labeled): 0.8747\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9914 - total_loss: 0.9914\n",
      "epoch:157, accruracy in testing dataset(200 labeled): 0.874\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8687 - total_loss: 0.8687\n",
      "epoch:158, accruracy in testing dataset(200 labeled): 0.8761\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8608 - total_loss: 0.8608\n",
      "epoch:159, accruracy in testing dataset(200 labeled): 0.8708\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8612 - total_loss: 0.8612\n",
      "epoch:160, accruracy in testing dataset(200 labeled): 0.8782\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8363 - total_loss: 0.8363\n",
      "epoch:161, accruracy in testing dataset(200 labeled): 0.8774\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8475 - total_loss: 0.8475\n",
      "epoch:162, accruracy in testing dataset(200 labeled): 0.8573\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9188 - total_loss: 0.9188\n",
      "epoch:163, accruracy in testing dataset(200 labeled): 0.8621\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9633 - total_loss: 0.9633\n",
      "epoch:164, accruracy in testing dataset(200 labeled): 0.866\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9200 - total_loss: 0.9200\n",
      "epoch:165, accruracy in testing dataset(200 labeled): 0.8652\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7784 - total_loss: 0.7784\n",
      "epoch:166, accruracy in testing dataset(200 labeled): 0.8681\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7942 - total_loss: 0.7942\n",
      "epoch:167, accruracy in testing dataset(200 labeled): 0.878\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8495 - total_loss: 0.8495\n",
      "epoch:168, accruracy in testing dataset(200 labeled): 0.8711\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8762 - total_loss: 0.8762\n",
      "epoch:169, accruracy in testing dataset(200 labeled): 0.8641\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8112 - total_loss: 0.8112\n",
      "epoch:170, accruracy in testing dataset(200 labeled): 0.8739\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8596 - total_loss: 0.8596\n",
      "epoch:171, accruracy in testing dataset(200 labeled): 0.8679\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.8597 - total_loss: 0.8597\n",
      "epoch:172, accruracy in testing dataset(200 labeled): 0.867\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9472 - total_loss: 0.9472\n",
      "epoch:173, accruracy in testing dataset(200 labeled): 0.8628\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7935 - total_loss: 0.7935\n",
      "epoch:174, accruracy in testing dataset(200 labeled): 0.8637\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9712 - total_loss: 0.9712\n",
      "epoch:175, accruracy in testing dataset(200 labeled): 0.8716\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8889 - total_loss: 0.8889\n",
      "epoch:176, accruracy in testing dataset(200 labeled): 0.8716\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8350 - total_loss: 0.8350\n",
      "epoch:177, accruracy in testing dataset(200 labeled): 0.8747\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8484 - total_loss: 0.8484\n",
      "epoch:178, accruracy in testing dataset(200 labeled): 0.8811\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7481 - total_loss: 0.7481\n",
      "epoch:179, accruracy in testing dataset(200 labeled): 0.8758\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7611 - total_loss: 0.7611\n",
      "epoch:180, accruracy in testing dataset(200 labeled): 0.8814\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9760 - total_loss: 0.9760\n",
      "epoch:181, accruracy in testing dataset(200 labeled): 0.8656\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.7609 - total_loss: 0.7609\n",
      "epoch:182, accruracy in testing dataset(200 labeled): 0.8532\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8371 - total_loss: 0.8371\n",
      "epoch:183, accruracy in testing dataset(200 labeled): 0.8744\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8357 - total_loss: 0.8357\n",
      "epoch:184, accruracy in testing dataset(200 labeled): 0.8698\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8722 - total_loss: 0.8722\n",
      "epoch:185, accruracy in testing dataset(200 labeled): 0.873\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6464 - total_loss: 0.6464\n",
      "epoch:186, accruracy in testing dataset(200 labeled): 0.8837\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8653 - total_loss: 0.8653\n",
      "epoch:187, accruracy in testing dataset(200 labeled): 0.8853\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8834 - total_loss: 0.8834\n",
      "epoch:188, accruracy in testing dataset(200 labeled): 0.8841\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7571 - total_loss: 0.7571\n",
      "epoch:189, accruracy in testing dataset(200 labeled): 0.8816\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.7913 - total_loss: 0.7913\n",
      "epoch:190, accruracy in testing dataset(200 labeled): 0.8735\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9225 - total_loss: 0.9225\n",
      "epoch:191, accruracy in testing dataset(200 labeled): 0.8761\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8937 - total_loss: 0.8937\n",
      "epoch:192, accruracy in testing dataset(200 labeled): 0.8754\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7617 - total_loss: 0.7617\n",
      "epoch:193, accruracy in testing dataset(200 labeled): 0.8761\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8919 - total_loss: 0.8919\n",
      "epoch:194, accruracy in testing dataset(200 labeled): 0.8789\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 1.0191 - total_loss: 1.0191\n",
      "epoch:195, accruracy in testing dataset(200 labeled): 0.889\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8554 - total_loss: 0.8554\n",
      "epoch:196, accruracy in testing dataset(200 labeled): 0.8827\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7593 - total_loss: 0.7593\n",
      "epoch:197, accruracy in testing dataset(200 labeled): 0.8786\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8626 - total_loss: 0.8626\n",
      "epoch:198, accruracy in testing dataset(200 labeled): 0.8852\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8712 - total_loss: 0.8712\n",
      "epoch:199, accruracy in testing dataset(200 labeled): 0.8775\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7780 - total_loss: 0.7780\n",
      "epoch:200, accruracy in testing dataset(200 labeled): 0.883\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7867 - total_loss: 0.7867\n",
      "epoch:201, accruracy in testing dataset(200 labeled): 0.8812\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7232 - total_loss: 0.7232\n",
      "epoch:202, accruracy in testing dataset(200 labeled): 0.8791\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7192 - total_loss: 0.7192\n",
      "epoch:203, accruracy in testing dataset(200 labeled): 0.8751\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7364 - total_loss: 0.7364\n",
      "epoch:204, accruracy in testing dataset(200 labeled): 0.8903\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7648 - total_loss: 0.7648\n",
      "epoch:205, accruracy in testing dataset(200 labeled): 0.8908\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8220 - total_loss: 0.8220\n",
      "epoch:206, accruracy in testing dataset(200 labeled): 0.8873\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7229 - total_loss: 0.7229\n",
      "epoch:207, accruracy in testing dataset(200 labeled): 0.8853\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6707 - total_loss: 0.6707\n",
      "epoch:208, accruracy in testing dataset(200 labeled): 0.8836\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6989 - total_loss: 0.6989\n",
      "epoch:209, accruracy in testing dataset(200 labeled): 0.8867\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7736 - total_loss: 0.7736\n",
      "epoch:210, accruracy in testing dataset(200 labeled): 0.8892\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8917 - total_loss: 0.8917\n",
      "epoch:211, accruracy in testing dataset(200 labeled): 0.8872\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7258 - total_loss: 0.7258\n",
      "epoch:212, accruracy in testing dataset(200 labeled): 0.8942\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.9080 - total_loss: 0.9080\n",
      "epoch:213, accruracy in testing dataset(200 labeled): 0.8924\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6519 - total_loss: 0.6519\n",
      "epoch:214, accruracy in testing dataset(200 labeled): 0.8893\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6848 - total_loss: 0.6848\n",
      "epoch:215, accruracy in testing dataset(200 labeled): 0.87\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7904 - total_loss: 0.7904\n",
      "epoch:216, accruracy in testing dataset(200 labeled): 0.8903\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7123 - total_loss: 0.7123\n",
      "epoch:217, accruracy in testing dataset(200 labeled): 0.8794\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6815 - total_loss: 0.6815\n",
      "epoch:218, accruracy in testing dataset(200 labeled): 0.8843\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7145 - total_loss: 0.7145\n",
      "epoch:219, accruracy in testing dataset(200 labeled): 0.8754\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7002 - total_loss: 0.7002\n",
      "epoch:220, accruracy in testing dataset(200 labeled): 0.8766\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7248 - total_loss: 0.7248\n",
      "epoch:221, accruracy in testing dataset(200 labeled): 0.8863\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7472 - total_loss: 0.7472\n",
      "epoch:222, accruracy in testing dataset(200 labeled): 0.8861\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6174 - total_loss: 0.6174\n",
      "epoch:223, accruracy in testing dataset(200 labeled): 0.8919\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7718 - total_loss: 0.7718\n",
      "epoch:224, accruracy in testing dataset(200 labeled): 0.8887\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7533 - total_loss: 0.7533\n",
      "epoch:225, accruracy in testing dataset(200 labeled): 0.8906\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5855 - total_loss: 0.5855\n",
      "epoch:226, accruracy in testing dataset(200 labeled): 0.8964\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7100 - total_loss: 0.7100\n",
      "epoch:227, accruracy in testing dataset(200 labeled): 0.8975\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6918 - total_loss: 0.6918\n",
      "epoch:228, accruracy in testing dataset(200 labeled): 0.8981\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6424 - total_loss: 0.6424\n",
      "epoch:229, accruracy in testing dataset(200 labeled): 0.8978\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7496 - total_loss: 0.7496\n",
      "epoch:230, accruracy in testing dataset(200 labeled): 0.8972\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6859 - total_loss: 0.6859\n",
      "epoch:231, accruracy in testing dataset(200 labeled): 0.8914\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6471 - total_loss: 0.6471\n",
      "epoch:232, accruracy in testing dataset(200 labeled): 0.8958\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7726 - total_loss: 0.7726\n",
      "epoch:233, accruracy in testing dataset(200 labeled): 0.8941\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7435 - total_loss: 0.7435\n",
      "epoch:234, accruracy in testing dataset(200 labeled): 0.8981\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7266 - total_loss: 0.7266\n",
      "epoch:235, accruracy in testing dataset(200 labeled): 0.8976\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6888 - total_loss: 0.6888\n",
      "epoch:236, accruracy in testing dataset(200 labeled): 0.8976\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8053 - total_loss: 0.8053\n",
      "epoch:237, accruracy in testing dataset(200 labeled): 0.8883\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7879 - total_loss: 0.7879\n",
      "epoch:238, accruracy in testing dataset(200 labeled): 0.8935\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5867 - total_loss: 0.5867\n",
      "epoch:239, accruracy in testing dataset(200 labeled): 0.8869\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5746 - total_loss: 0.5746\n",
      "epoch:240, accruracy in testing dataset(200 labeled): 0.8837\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5759 - total_loss: 0.5759\n",
      "epoch:241, accruracy in testing dataset(200 labeled): 0.8993\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6466 - total_loss: 0.6466\n",
      "epoch:242, accruracy in testing dataset(200 labeled): 0.8926\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.7143 - total_loss: 0.7143\n",
      "epoch:243, accruracy in testing dataset(200 labeled): 0.8897\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6899 - total_loss: 0.6899\n",
      "epoch:244, accruracy in testing dataset(200 labeled): 0.8858\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7516 - total_loss: 0.7516\n",
      "epoch:245, accruracy in testing dataset(200 labeled): 0.887\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.8427 - total_loss: 0.8427\n",
      "epoch:246, accruracy in testing dataset(200 labeled): 0.8965\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6702 - total_loss: 0.6702\n",
      "epoch:247, accruracy in testing dataset(200 labeled): 0.8993\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6529 - total_loss: 0.6529\n",
      "epoch:248, accruracy in testing dataset(200 labeled): 0.9027\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5972 - total_loss: 0.5972\n",
      "epoch:249, accruracy in testing dataset(200 labeled): 0.9025\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6596 - total_loss: 0.6596\n",
      "epoch:250, accruracy in testing dataset(200 labeled): 0.8989\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6516 - total_loss: 0.6516\n",
      "epoch:251, accruracy in testing dataset(200 labeled): 0.8953\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7435 - total_loss: 0.7435\n",
      "epoch:252, accruracy in testing dataset(200 labeled): 0.9007\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6300 - total_loss: 0.6300\n",
      "epoch:253, accruracy in testing dataset(200 labeled): 0.9016\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6484 - total_loss: 0.6484\n",
      "epoch:254, accruracy in testing dataset(200 labeled): 0.8924\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6242 - total_loss: 0.6242\n",
      "epoch:255, accruracy in testing dataset(200 labeled): 0.8944\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7036 - total_loss: 0.7036\n",
      "epoch:256, accruracy in testing dataset(200 labeled): 0.8983\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5933 - total_loss: 0.5933\n",
      "epoch:257, accruracy in testing dataset(200 labeled): 0.9016\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6208 - total_loss: 0.6208\n",
      "epoch:258, accruracy in testing dataset(200 labeled): 0.8953\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6920 - total_loss: 0.6920\n",
      "epoch:259, accruracy in testing dataset(200 labeled): 0.898\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.7014 - total_loss: 0.7014\n",
      "epoch:260, accruracy in testing dataset(200 labeled): 0.8982\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6773 - total_loss: 0.6773\n",
      "epoch:261, accruracy in testing dataset(200 labeled): 0.8906\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6904 - total_loss: 0.6904\n",
      "epoch:262, accruracy in testing dataset(200 labeled): 0.8983\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6373 - total_loss: 0.6373\n",
      "epoch:263, accruracy in testing dataset(200 labeled): 0.9042\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6090 - total_loss: 0.6090\n",
      "epoch:264, accruracy in testing dataset(200 labeled): 0.9019\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5365 - total_loss: 0.5365\n",
      "epoch:265, accruracy in testing dataset(200 labeled): 0.8982\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.7348 - total_loss: 0.7348\n",
      "epoch:266, accruracy in testing dataset(200 labeled): 0.9072\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7082 - total_loss: 0.7082\n",
      "epoch:267, accruracy in testing dataset(200 labeled): 0.9029\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5223 - total_loss: 0.5223\n",
      "epoch:268, accruracy in testing dataset(200 labeled): 0.9071\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6820 - total_loss: 0.6820\n",
      "epoch:269, accruracy in testing dataset(200 labeled): 0.9074\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5419 - total_loss: 0.5419\n",
      "epoch:270, accruracy in testing dataset(200 labeled): 0.9033\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6179 - total_loss: 0.6179\n",
      "epoch:271, accruracy in testing dataset(200 labeled): 0.9049\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7217 - total_loss: 0.7217\n",
      "epoch:272, accruracy in testing dataset(200 labeled): 0.9085\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7685 - total_loss: 0.7685\n",
      "epoch:273, accruracy in testing dataset(200 labeled): 0.9007\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6408 - total_loss: 0.6408\n",
      "epoch:274, accruracy in testing dataset(200 labeled): 0.9039\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6419 - total_loss: 0.6419\n",
      "epoch:275, accruracy in testing dataset(200 labeled): 0.9103\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6353 - total_loss: 0.6353\n",
      "epoch:276, accruracy in testing dataset(200 labeled): 0.9101\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6665 - total_loss: 0.6665\n",
      "epoch:277, accruracy in testing dataset(200 labeled): 0.9115\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5840 - total_loss: 0.5840\n",
      "epoch:278, accruracy in testing dataset(200 labeled): 0.9072\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6155 - total_loss: 0.6155\n",
      "epoch:279, accruracy in testing dataset(200 labeled): 0.9066\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6735 - total_loss: 0.6735\n",
      "epoch:280, accruracy in testing dataset(200 labeled): 0.9032\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5679 - total_loss: 0.5679\n",
      "epoch:281, accruracy in testing dataset(200 labeled): 0.9058\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5830 - total_loss: 0.5830\n",
      "epoch:282, accruracy in testing dataset(200 labeled): 0.8946\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6679 - total_loss: 0.6679\n",
      "epoch:283, accruracy in testing dataset(200 labeled): 0.8981\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5942 - total_loss: 0.5942\n",
      "epoch:284, accruracy in testing dataset(200 labeled): 0.8972\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6434 - total_loss: 0.6434\n",
      "epoch:285, accruracy in testing dataset(200 labeled): 0.9002\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5583 - total_loss: 0.5583\n",
      "epoch:286, accruracy in testing dataset(200 labeled): 0.9002\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6387 - total_loss: 0.6387\n",
      "epoch:287, accruracy in testing dataset(200 labeled): 0.9051\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6350 - total_loss: 0.6350\n",
      "epoch:288, accruracy in testing dataset(200 labeled): 0.9073\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6432 - total_loss: 0.6432\n",
      "epoch:289, accruracy in testing dataset(200 labeled): 0.9026\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5821 - total_loss: 0.5821\n",
      "epoch:290, accruracy in testing dataset(200 labeled): 0.8975\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6367 - total_loss: 0.6367\n",
      "epoch:291, accruracy in testing dataset(200 labeled): 0.8979\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6136 - total_loss: 0.6136\n",
      "epoch:292, accruracy in testing dataset(200 labeled): 0.899\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6543 - total_loss: 0.6543\n",
      "epoch:293, accruracy in testing dataset(200 labeled): 0.9064\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6308 - total_loss: 0.6308\n",
      "epoch:294, accruracy in testing dataset(200 labeled): 0.9055\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6128 - total_loss: 0.6128\n",
      "epoch:295, accruracy in testing dataset(200 labeled): 0.8994\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4147 - total_loss: 0.4147\n",
      "epoch:296, accruracy in testing dataset(200 labeled): 0.9089\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4978 - total_loss: 0.4978\n",
      "epoch:297, accruracy in testing dataset(200 labeled): 0.8967\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6542 - total_loss: 0.6542\n",
      "epoch:298, accruracy in testing dataset(200 labeled): 0.8872\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6376 - total_loss: 0.6376\n",
      "epoch:1, accruracy in testing dataset(200 labeled): 0.897\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6481 - total_loss: 0.6481\n",
      "epoch:2, accruracy in testing dataset(200 labeled): 0.9022\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5749 - total_loss: 0.5749\n",
      "epoch:3, accruracy in testing dataset(200 labeled): 0.9087\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6248 - total_loss: 0.6248\n",
      "epoch:4, accruracy in testing dataset(200 labeled): 0.9052\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5435 - total_loss: 0.5435\n",
      "epoch:5, accruracy in testing dataset(200 labeled): 0.9111\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4776 - total_loss: 0.4776\n",
      "epoch:6, accruracy in testing dataset(200 labeled): 0.9124\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6593 - total_loss: 0.6593\n",
      "epoch:7, accruracy in testing dataset(200 labeled): 0.9017\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6151 - total_loss: 0.6151\n",
      "epoch:8, accruracy in testing dataset(200 labeled): 0.9058\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5362 - total_loss: 0.5362\n",
      "epoch:9, accruracy in testing dataset(200 labeled): 0.8973\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5737 - total_loss: 0.5737\n",
      "epoch:10, accruracy in testing dataset(200 labeled): 0.8989\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6341 - total_loss: 0.6341\n",
      "epoch:11, accruracy in testing dataset(200 labeled): 0.8972\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5817 - total_loss: 0.5817\n",
      "epoch:12, accruracy in testing dataset(200 labeled): 0.886\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6356 - total_loss: 0.6356\n",
      "epoch:13, accruracy in testing dataset(200 labeled): 0.9023\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6029 - total_loss: 0.6029\n",
      "epoch:14, accruracy in testing dataset(200 labeled): 0.9044\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6531 - total_loss: 0.6531\n",
      "epoch:15, accruracy in testing dataset(200 labeled): 0.9048\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7992 - total_loss: 0.7992\n",
      "epoch:16, accruracy in testing dataset(200 labeled): 0.9047\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5916 - total_loss: 0.5916\n",
      "epoch:17, accruracy in testing dataset(200 labeled): 0.9018\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5650 - total_loss: 0.5650\n",
      "epoch:18, accruracy in testing dataset(200 labeled): 0.8978\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4984 - total_loss: 0.4984\n",
      "epoch:19, accruracy in testing dataset(200 labeled): 0.8983\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5616 - total_loss: 0.5616\n",
      "epoch:20, accruracy in testing dataset(200 labeled): 0.8996\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5241 - total_loss: 0.5241\n",
      "epoch:21, accruracy in testing dataset(200 labeled): 0.9016\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6750 - total_loss: 0.6750\n",
      "epoch:22, accruracy in testing dataset(200 labeled): 0.8979\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6466 - total_loss: 0.6466\n",
      "epoch:23, accruracy in testing dataset(200 labeled): 0.9027\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6619 - total_loss: 0.6619\n",
      "epoch:24, accruracy in testing dataset(200 labeled): 0.9014\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5840 - total_loss: 0.5840\n",
      "epoch:25, accruracy in testing dataset(200 labeled): 0.9053\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5262 - total_loss: 0.5262\n",
      "epoch:26, accruracy in testing dataset(200 labeled): 0.9104\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6135 - total_loss: 0.6135\n",
      "epoch:27, accruracy in testing dataset(200 labeled): 0.9044\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6999 - total_loss: 0.6999\n",
      "epoch:28, accruracy in testing dataset(200 labeled): 0.8946\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7355 - total_loss: 0.7355\n",
      "epoch:29, accruracy in testing dataset(200 labeled): 0.8857\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5342 - total_loss: 0.5342\n",
      "epoch:30, accruracy in testing dataset(200 labeled): 0.8904\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5485 - total_loss: 0.5485\n",
      "epoch:31, accruracy in testing dataset(200 labeled): 0.8924\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5003 - total_loss: 0.5003\n",
      "epoch:32, accruracy in testing dataset(200 labeled): 0.8954\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4796 - total_loss: 0.4796\n",
      "epoch:33, accruracy in testing dataset(200 labeled): 0.9018\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5700 - total_loss: 0.5700\n",
      "epoch:34, accruracy in testing dataset(200 labeled): 0.9009\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5130 - total_loss: 0.5130\n",
      "epoch:35, accruracy in testing dataset(200 labeled): 0.9031\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5317 - total_loss: 0.5317\n",
      "epoch:36, accruracy in testing dataset(200 labeled): 0.9023\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5259 - total_loss: 0.5259\n",
      "epoch:37, accruracy in testing dataset(200 labeled): 0.9043\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5469 - total_loss: 0.5469\n",
      "epoch:38, accruracy in testing dataset(200 labeled): 0.9038\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6789 - total_loss: 0.6789\n",
      "epoch:39, accruracy in testing dataset(200 labeled): 0.9018\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4621 - total_loss: 0.4621\n",
      "epoch:40, accruracy in testing dataset(200 labeled): 0.905\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5868 - total_loss: 0.5868\n",
      "epoch:41, accruracy in testing dataset(200 labeled): 0.8939\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6419 - total_loss: 0.6419\n",
      "epoch:42, accruracy in testing dataset(200 labeled): 0.9031\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.7607 - total_loss: 0.7607\n",
      "epoch:43, accruracy in testing dataset(200 labeled): 0.9136\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4644 - total_loss: 0.4644\n",
      "epoch:44, accruracy in testing dataset(200 labeled): 0.9116\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5398 - total_loss: 0.5398\n",
      "epoch:45, accruracy in testing dataset(200 labeled): 0.9163\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4906 - total_loss: 0.4906\n",
      "epoch:46, accruracy in testing dataset(200 labeled): 0.9083\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5258 - total_loss: 0.5258\n",
      "epoch:47, accruracy in testing dataset(200 labeled): 0.9072\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4483 - total_loss: 0.4483\n",
      "epoch:48, accruracy in testing dataset(200 labeled): 0.9086\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6082 - total_loss: 0.6082\n",
      "epoch:49, accruracy in testing dataset(200 labeled): 0.9039\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6551 - total_loss: 0.6551\n",
      "epoch:50, accruracy in testing dataset(200 labeled): 0.9089\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5346 - total_loss: 0.5346\n",
      "epoch:51, accruracy in testing dataset(200 labeled): 0.9053\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6086 - total_loss: 0.6086\n",
      "epoch:52, accruracy in testing dataset(200 labeled): 0.9029\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5759 - total_loss: 0.5759\n",
      "epoch:53, accruracy in testing dataset(200 labeled): 0.9052\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5757 - total_loss: 0.5757\n",
      "epoch:54, accruracy in testing dataset(200 labeled): 0.907\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6711 - total_loss: 0.6711\n",
      "epoch:55, accruracy in testing dataset(200 labeled): 0.9078\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6909 - total_loss: 0.6909\n",
      "epoch:56, accruracy in testing dataset(200 labeled): 0.9093\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5861 - total_loss: 0.5861\n",
      "epoch:57, accruracy in testing dataset(200 labeled): 0.9046\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5753 - total_loss: 0.5753\n",
      "epoch:58, accruracy in testing dataset(200 labeled): 0.9094\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5966 - total_loss: 0.5966\n",
      "epoch:59, accruracy in testing dataset(200 labeled): 0.9043\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5265 - total_loss: 0.5265\n",
      "epoch:60, accruracy in testing dataset(200 labeled): 0.9048\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6716 - total_loss: 0.6716\n",
      "epoch:61, accruracy in testing dataset(200 labeled): 0.9063\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6170 - total_loss: 0.6170\n",
      "epoch:62, accruracy in testing dataset(200 labeled): 0.9161\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4433 - total_loss: 0.4433\n",
      "epoch:63, accruracy in testing dataset(200 labeled): 0.9157\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4948 - total_loss: 0.4948\n",
      "epoch:64, accruracy in testing dataset(200 labeled): 0.9163\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6055 - total_loss: 0.6055\n",
      "epoch:65, accruracy in testing dataset(200 labeled): 0.9165\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5570 - total_loss: 0.5570\n",
      "epoch:66, accruracy in testing dataset(200 labeled): 0.9151\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5476 - total_loss: 0.5476\n",
      "epoch:67, accruracy in testing dataset(200 labeled): 0.9073\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4794 - total_loss: 0.4794\n",
      "epoch:68, accruracy in testing dataset(200 labeled): 0.9014\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6440 - total_loss: 0.6440\n",
      "epoch:69, accruracy in testing dataset(200 labeled): 0.9039\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6154 - total_loss: 0.6154\n",
      "epoch:70, accruracy in testing dataset(200 labeled): 0.9041\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5184 - total_loss: 0.5184\n",
      "epoch:71, accruracy in testing dataset(200 labeled): 0.9066\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6141 - total_loss: 0.6141\n",
      "epoch:72, accruracy in testing dataset(200 labeled): 0.9081\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4979 - total_loss: 0.4979\n",
      "epoch:73, accruracy in testing dataset(200 labeled): 0.9044\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6901 - total_loss: 0.6901\n",
      "epoch:74, accruracy in testing dataset(200 labeled): 0.9046\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5828 - total_loss: 0.5828\n",
      "epoch:75, accruracy in testing dataset(200 labeled): 0.9061\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5275 - total_loss: 0.5275\n",
      "epoch:76, accruracy in testing dataset(200 labeled): 0.9101\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6113 - total_loss: 0.6113\n",
      "epoch:77, accruracy in testing dataset(200 labeled): 0.9163\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5429 - total_loss: 0.5429\n",
      "epoch:78, accruracy in testing dataset(200 labeled): 0.9144\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4641 - total_loss: 0.4641\n",
      "epoch:79, accruracy in testing dataset(200 labeled): 0.9142\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5720 - total_loss: 0.5720\n",
      "epoch:80, accruracy in testing dataset(200 labeled): 0.9135\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4427 - total_loss: 0.4427\n",
      "epoch:81, accruracy in testing dataset(200 labeled): 0.9009\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6756 - total_loss: 0.6756\n",
      "epoch:82, accruracy in testing dataset(200 labeled): 0.9146\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4321 - total_loss: 0.4321\n",
      "epoch:83, accruracy in testing dataset(200 labeled): 0.9052\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4691 - total_loss: 0.4691\n",
      "epoch:84, accruracy in testing dataset(200 labeled): 0.9093\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5981 - total_loss: 0.5981\n",
      "epoch:85, accruracy in testing dataset(200 labeled): 0.9056\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5380 - total_loss: 0.5380\n",
      "epoch:86, accruracy in testing dataset(200 labeled): 0.908\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5963 - total_loss: 0.5963\n",
      "epoch:87, accruracy in testing dataset(200 labeled): 0.9029\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6284 - total_loss: 0.6284\n",
      "epoch:88, accruracy in testing dataset(200 labeled): 0.9097\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5126 - total_loss: 0.5126\n",
      "epoch:89, accruracy in testing dataset(200 labeled): 0.907\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5215 - total_loss: 0.5215\n",
      "epoch:90, accruracy in testing dataset(200 labeled): 0.9037\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5877 - total_loss: 0.5877\n",
      "epoch:91, accruracy in testing dataset(200 labeled): 0.9079\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5240 - total_loss: 0.5240\n",
      "epoch:92, accruracy in testing dataset(200 labeled): 0.9102\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4984 - total_loss: 0.4984\n",
      "epoch:93, accruracy in testing dataset(200 labeled): 0.9071\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5361 - total_loss: 0.5361\n",
      "epoch:94, accruracy in testing dataset(200 labeled): 0.9115\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6859 - total_loss: 0.6859\n",
      "epoch:95, accruracy in testing dataset(200 labeled): 0.9101\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5366 - total_loss: 0.5366\n",
      "epoch:96, accruracy in testing dataset(200 labeled): 0.9119\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6010 - total_loss: 0.6010\n",
      "epoch:97, accruracy in testing dataset(200 labeled): 0.9063\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6117 - total_loss: 0.6117\n",
      "epoch:98, accruracy in testing dataset(200 labeled): 0.9049\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3673 - total_loss: 0.3673\n",
      "epoch:99, accruracy in testing dataset(200 labeled): 0.9049\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5168 - total_loss: 0.5168\n",
      "epoch:100, accruracy in testing dataset(200 labeled): 0.9143\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5327 - total_loss: 0.5327\n",
      "epoch:101, accruracy in testing dataset(200 labeled): 0.9171\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4879 - total_loss: 0.4879\n",
      "epoch:102, accruracy in testing dataset(200 labeled): 0.9109\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6214 - total_loss: 0.6214\n",
      "epoch:103, accruracy in testing dataset(200 labeled): 0.9162\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5057 - total_loss: 0.5057\n",
      "epoch:104, accruracy in testing dataset(200 labeled): 0.9178\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6332 - total_loss: 0.6332\n",
      "epoch:105, accruracy in testing dataset(200 labeled): 0.9163\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.6714 - total_loss: 0.6714\n",
      "epoch:106, accruracy in testing dataset(200 labeled): 0.9163\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5547 - total_loss: 0.5547\n",
      "epoch:107, accruracy in testing dataset(200 labeled): 0.9153\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5669 - total_loss: 0.5669\n",
      "epoch:108, accruracy in testing dataset(200 labeled): 0.9149\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4718 - total_loss: 0.4718\n",
      "epoch:109, accruracy in testing dataset(200 labeled): 0.9146\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6031 - total_loss: 0.6031\n",
      "epoch:110, accruracy in testing dataset(200 labeled): 0.909\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5344 - total_loss: 0.5344\n",
      "epoch:111, accruracy in testing dataset(200 labeled): 0.9116\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4489 - total_loss: 0.4489\n",
      "epoch:112, accruracy in testing dataset(200 labeled): 0.9109\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5428 - total_loss: 0.5428\n",
      "epoch:113, accruracy in testing dataset(200 labeled): 0.9139\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5373 - total_loss: 0.5373\n",
      "epoch:114, accruracy in testing dataset(200 labeled): 0.915\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4225 - total_loss: 0.4225\n",
      "epoch:115, accruracy in testing dataset(200 labeled): 0.917\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4465 - total_loss: 0.4465\n",
      "epoch:116, accruracy in testing dataset(200 labeled): 0.9169\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5155 - total_loss: 0.5155\n",
      "epoch:117, accruracy in testing dataset(200 labeled): 0.912\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5616 - total_loss: 0.5616\n",
      "epoch:118, accruracy in testing dataset(200 labeled): 0.9089\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5631 - total_loss: 0.5631\n",
      "epoch:119, accruracy in testing dataset(200 labeled): 0.9077\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4998 - total_loss: 0.4998\n",
      "epoch:120, accruracy in testing dataset(200 labeled): 0.9122\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4883 - total_loss: 0.4883\n",
      "epoch:121, accruracy in testing dataset(200 labeled): 0.9129\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4142 - total_loss: 0.4142\n",
      "epoch:122, accruracy in testing dataset(200 labeled): 0.9147\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3904 - total_loss: 0.3904\n",
      "epoch:123, accruracy in testing dataset(200 labeled): 0.9139\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4947 - total_loss: 0.4947\n",
      "epoch:124, accruracy in testing dataset(200 labeled): 0.9166\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5629 - total_loss: 0.5629\n",
      "epoch:125, accruracy in testing dataset(200 labeled): 0.912\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4824 - total_loss: 0.4824\n",
      "epoch:126, accruracy in testing dataset(200 labeled): 0.9139\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4538 - total_loss: 0.4538\n",
      "epoch:127, accruracy in testing dataset(200 labeled): 0.9122\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4956 - total_loss: 0.4956\n",
      "epoch:128, accruracy in testing dataset(200 labeled): 0.9116\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4949 - total_loss: 0.4949\n",
      "epoch:129, accruracy in testing dataset(200 labeled): 0.9103\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5317 - total_loss: 0.5317\n",
      "epoch:130, accruracy in testing dataset(200 labeled): 0.8991\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4330 - total_loss: 0.4330\n",
      "epoch:131, accruracy in testing dataset(200 labeled): 0.8991\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4101 - total_loss: 0.4101\n",
      "epoch:132, accruracy in testing dataset(200 labeled): 0.9037\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5071 - total_loss: 0.5071\n",
      "epoch:133, accruracy in testing dataset(200 labeled): 0.9022\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6390 - total_loss: 0.6390\n",
      "epoch:134, accruracy in testing dataset(200 labeled): 0.9057\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6056 - total_loss: 0.6056\n",
      "epoch:135, accruracy in testing dataset(200 labeled): 0.9109\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5087 - total_loss: 0.5087\n",
      "epoch:136, accruracy in testing dataset(200 labeled): 0.9073\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5384 - total_loss: 0.5384\n",
      "epoch:137, accruracy in testing dataset(200 labeled): 0.9097\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4882 - total_loss: 0.4882\n",
      "epoch:138, accruracy in testing dataset(200 labeled): 0.8993\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4205 - total_loss: 0.4205\n",
      "epoch:139, accruracy in testing dataset(200 labeled): 0.9033\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5240 - total_loss: 0.5240\n",
      "epoch:140, accruracy in testing dataset(200 labeled): 0.8975\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5708 - total_loss: 0.5708\n",
      "epoch:141, accruracy in testing dataset(200 labeled): 0.9054\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6087 - total_loss: 0.6087\n",
      "epoch:142, accruracy in testing dataset(200 labeled): 0.9062\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5564 - total_loss: 0.5564\n",
      "epoch:143, accruracy in testing dataset(200 labeled): 0.9064\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4373 - total_loss: 0.4373\n",
      "epoch:144, accruracy in testing dataset(200 labeled): 0.9005\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6098 - total_loss: 0.6098\n",
      "epoch:145, accruracy in testing dataset(200 labeled): 0.907\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4148 - total_loss: 0.4148\n",
      "epoch:146, accruracy in testing dataset(200 labeled): 0.9167\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5999 - total_loss: 0.5999\n",
      "epoch:147, accruracy in testing dataset(200 labeled): 0.9186\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4695 - total_loss: 0.4695\n",
      "epoch:148, accruracy in testing dataset(200 labeled): 0.9177\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3858 - total_loss: 0.3858\n",
      "epoch:149, accruracy in testing dataset(200 labeled): 0.9125\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4424 - total_loss: 0.4424\n",
      "epoch:150, accruracy in testing dataset(200 labeled): 0.9065\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4083 - total_loss: 0.4083\n",
      "epoch:151, accruracy in testing dataset(200 labeled): 0.9082\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5275 - total_loss: 0.5275\n",
      "epoch:152, accruracy in testing dataset(200 labeled): 0.9125\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4453 - total_loss: 0.4453\n",
      "epoch:153, accruracy in testing dataset(200 labeled): 0.915\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4254 - total_loss: 0.4254\n",
      "epoch:154, accruracy in testing dataset(200 labeled): 0.9129\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4815 - total_loss: 0.4815\n",
      "epoch:155, accruracy in testing dataset(200 labeled): 0.9107\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5088 - total_loss: 0.5088\n",
      "epoch:156, accruracy in testing dataset(200 labeled): 0.9149\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5916 - total_loss: 0.5916\n",
      "epoch:157, accruracy in testing dataset(200 labeled): 0.9135\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3211 - total_loss: 0.3211\n",
      "epoch:158, accruracy in testing dataset(200 labeled): 0.9098\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5214 - total_loss: 0.5214\n",
      "epoch:159, accruracy in testing dataset(200 labeled): 0.9137\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5697 - total_loss: 0.5697\n",
      "epoch:160, accruracy in testing dataset(200 labeled): 0.9102\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4291 - total_loss: 0.4291\n",
      "epoch:161, accruracy in testing dataset(200 labeled): 0.9128\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4314 - total_loss: 0.4314\n",
      "epoch:162, accruracy in testing dataset(200 labeled): 0.9141\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5295 - total_loss: 0.5295\n",
      "epoch:163, accruracy in testing dataset(200 labeled): 0.9095\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4531 - total_loss: 0.4531\n",
      "epoch:164, accruracy in testing dataset(200 labeled): 0.9171\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4493 - total_loss: 0.4493\n",
      "epoch:165, accruracy in testing dataset(200 labeled): 0.9188\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4966 - total_loss: 0.4966\n",
      "epoch:166, accruracy in testing dataset(200 labeled): 0.9131\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5232 - total_loss: 0.5232\n",
      "epoch:167, accruracy in testing dataset(200 labeled): 0.9172\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4055 - total_loss: 0.4055\n",
      "epoch:168, accruracy in testing dataset(200 labeled): 0.9187\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4979 - total_loss: 0.4979\n",
      "epoch:169, accruracy in testing dataset(200 labeled): 0.9121\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5057 - total_loss: 0.5057\n",
      "epoch:170, accruracy in testing dataset(200 labeled): 0.9201\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4179 - total_loss: 0.4179\n",
      "epoch:171, accruracy in testing dataset(200 labeled): 0.9195\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4394 - total_loss: 0.4394\n",
      "epoch:172, accruracy in testing dataset(200 labeled): 0.9259\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5108 - total_loss: 0.5108\n",
      "epoch:173, accruracy in testing dataset(200 labeled): 0.9249\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5150 - total_loss: 0.5150\n",
      "epoch:174, accruracy in testing dataset(200 labeled): 0.9205\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4617 - total_loss: 0.4617\n",
      "epoch:175, accruracy in testing dataset(200 labeled): 0.9207\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4791 - total_loss: 0.4791\n",
      "epoch:176, accruracy in testing dataset(200 labeled): 0.9199\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5236 - total_loss: 0.5236\n",
      "epoch:177, accruracy in testing dataset(200 labeled): 0.9185\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5309 - total_loss: 0.5309\n",
      "epoch:178, accruracy in testing dataset(200 labeled): 0.924\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3481 - total_loss: 0.3481\n",
      "epoch:179, accruracy in testing dataset(200 labeled): 0.9271\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3756 - total_loss: 0.3756\n",
      "epoch:180, accruracy in testing dataset(200 labeled): 0.9261\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5187 - total_loss: 0.5187\n",
      "epoch:181, accruracy in testing dataset(200 labeled): 0.919\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4466 - total_loss: 0.4466\n",
      "epoch:182, accruracy in testing dataset(200 labeled): 0.9157\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5194 - total_loss: 0.5194\n",
      "epoch:183, accruracy in testing dataset(200 labeled): 0.9184\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3994 - total_loss: 0.3994\n",
      "epoch:184, accruracy in testing dataset(200 labeled): 0.9259\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5943 - total_loss: 0.5943\n",
      "epoch:185, accruracy in testing dataset(200 labeled): 0.9283\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3907 - total_loss: 0.3907\n",
      "epoch:186, accruracy in testing dataset(200 labeled): 0.9271\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5549 - total_loss: 0.5549\n",
      "epoch:187, accruracy in testing dataset(200 labeled): 0.9264\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5333 - total_loss: 0.5333\n",
      "epoch:188, accruracy in testing dataset(200 labeled): 0.9268\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3769 - total_loss: 0.3769\n",
      "epoch:189, accruracy in testing dataset(200 labeled): 0.9281\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2991 - total_loss: 0.2991\n",
      "epoch:190, accruracy in testing dataset(200 labeled): 0.9269\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5225 - total_loss: 0.5225\n",
      "epoch:191, accruracy in testing dataset(200 labeled): 0.9253\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6467 - total_loss: 0.6467\n",
      "epoch:192, accruracy in testing dataset(200 labeled): 0.9299\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3290 - total_loss: 0.3290\n",
      "epoch:193, accruracy in testing dataset(200 labeled): 0.9293\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6165 - total_loss: 0.6165\n",
      "epoch:194, accruracy in testing dataset(200 labeled): 0.9236\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5416 - total_loss: 0.5416\n",
      "epoch:195, accruracy in testing dataset(200 labeled): 0.9224\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4566 - total_loss: 0.4566\n",
      "epoch:196, accruracy in testing dataset(200 labeled): 0.9286\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3964 - total_loss: 0.3964\n",
      "epoch:197, accruracy in testing dataset(200 labeled): 0.929\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4587 - total_loss: 0.4587\n",
      "epoch:198, accruracy in testing dataset(200 labeled): 0.9274\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4462 - total_loss: 0.4462\n",
      "epoch:199, accruracy in testing dataset(200 labeled): 0.9267\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4697 - total_loss: 0.4697\n",
      "epoch:200, accruracy in testing dataset(200 labeled): 0.927\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4160 - total_loss: 0.4160\n",
      "epoch:201, accruracy in testing dataset(200 labeled): 0.9283\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4145 - total_loss: 0.4145\n",
      "epoch:202, accruracy in testing dataset(200 labeled): 0.925\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3549 - total_loss: 0.3549\n",
      "epoch:203, accruracy in testing dataset(200 labeled): 0.9235\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3884 - total_loss: 0.3884\n",
      "epoch:204, accruracy in testing dataset(200 labeled): 0.9236\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4972 - total_loss: 0.4972\n",
      "epoch:205, accruracy in testing dataset(200 labeled): 0.9218\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3794 - total_loss: 0.3794\n",
      "epoch:206, accruracy in testing dataset(200 labeled): 0.9261\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4862 - total_loss: 0.4862\n",
      "epoch:207, accruracy in testing dataset(200 labeled): 0.9257\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3701 - total_loss: 0.3701\n",
      "epoch:208, accruracy in testing dataset(200 labeled): 0.9265\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5176 - total_loss: 0.5176\n",
      "epoch:209, accruracy in testing dataset(200 labeled): 0.9239\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3967 - total_loss: 0.3967\n",
      "epoch:210, accruracy in testing dataset(200 labeled): 0.9248\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4869 - total_loss: 0.4869\n",
      "epoch:211, accruracy in testing dataset(200 labeled): 0.9251\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4399 - total_loss: 0.4399\n",
      "epoch:212, accruracy in testing dataset(200 labeled): 0.9237\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4949 - total_loss: 0.4949\n",
      "epoch:213, accruracy in testing dataset(200 labeled): 0.9263\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3760 - total_loss: 0.3760\n",
      "epoch:214, accruracy in testing dataset(200 labeled): 0.9255\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4342 - total_loss: 0.4342\n",
      "epoch:215, accruracy in testing dataset(200 labeled): 0.9283\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4253 - total_loss: 0.4253\n",
      "epoch:216, accruracy in testing dataset(200 labeled): 0.9312\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4448 - total_loss: 0.4448\n",
      "epoch:217, accruracy in testing dataset(200 labeled): 0.9284\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4149 - total_loss: 0.4149\n",
      "epoch:218, accruracy in testing dataset(200 labeled): 0.9254\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4778 - total_loss: 0.4778\n",
      "epoch:219, accruracy in testing dataset(200 labeled): 0.9252\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3528 - total_loss: 0.3528\n",
      "epoch:220, accruracy in testing dataset(200 labeled): 0.9268\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3519 - total_loss: 0.3519\n",
      "epoch:221, accruracy in testing dataset(200 labeled): 0.9259\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3949 - total_loss: 0.3949\n",
      "epoch:222, accruracy in testing dataset(200 labeled): 0.9279\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4360 - total_loss: 0.4360\n",
      "epoch:223, accruracy in testing dataset(200 labeled): 0.9319\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4622 - total_loss: 0.4622\n",
      "epoch:224, accruracy in testing dataset(200 labeled): 0.9312\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5324 - total_loss: 0.5324\n",
      "epoch:225, accruracy in testing dataset(200 labeled): 0.9317\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3224 - total_loss: 0.3224\n",
      "epoch:226, accruracy in testing dataset(200 labeled): 0.93\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4179 - total_loss: 0.4179\n",
      "epoch:227, accruracy in testing dataset(200 labeled): 0.9283\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3780 - total_loss: 0.3780\n",
      "epoch:228, accruracy in testing dataset(200 labeled): 0.9287\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3578 - total_loss: 0.3578\n",
      "epoch:229, accruracy in testing dataset(200 labeled): 0.9258\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4559 - total_loss: 0.4559\n",
      "epoch:230, accruracy in testing dataset(200 labeled): 0.919\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4759 - total_loss: 0.4759\n",
      "epoch:231, accruracy in testing dataset(200 labeled): 0.9184\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4194 - total_loss: 0.4194\n",
      "epoch:232, accruracy in testing dataset(200 labeled): 0.924\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4490 - total_loss: 0.4490\n",
      "epoch:233, accruracy in testing dataset(200 labeled): 0.9205\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4894 - total_loss: 0.4894\n",
      "epoch:234, accruracy in testing dataset(200 labeled): 0.9248\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4693 - total_loss: 0.4693\n",
      "epoch:235, accruracy in testing dataset(200 labeled): 0.9289\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3514 - total_loss: 0.3514\n",
      "epoch:236, accruracy in testing dataset(200 labeled): 0.923\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4916 - total_loss: 0.4916\n",
      "epoch:237, accruracy in testing dataset(200 labeled): 0.9311\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4531 - total_loss: 0.4531\n",
      "epoch:238, accruracy in testing dataset(200 labeled): 0.932\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3634 - total_loss: 0.3634\n",
      "epoch:239, accruracy in testing dataset(200 labeled): 0.9286\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3667 - total_loss: 0.3667\n",
      "epoch:240, accruracy in testing dataset(200 labeled): 0.9215\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3537 - total_loss: 0.3537\n",
      "epoch:241, accruracy in testing dataset(200 labeled): 0.9194\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3726 - total_loss: 0.3726\n",
      "epoch:242, accruracy in testing dataset(200 labeled): 0.9213\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4591 - total_loss: 0.4591\n",
      "epoch:243, accruracy in testing dataset(200 labeled): 0.924\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4443 - total_loss: 0.4443\n",
      "epoch:244, accruracy in testing dataset(200 labeled): 0.9246\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5242 - total_loss: 0.5242\n",
      "epoch:245, accruracy in testing dataset(200 labeled): 0.9278\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4285 - total_loss: 0.4285\n",
      "epoch:246, accruracy in testing dataset(200 labeled): 0.9316\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4322 - total_loss: 0.4322\n",
      "epoch:247, accruracy in testing dataset(200 labeled): 0.9226\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4630 - total_loss: 0.4630\n",
      "epoch:248, accruracy in testing dataset(200 labeled): 0.9267\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3931 - total_loss: 0.3931\n",
      "epoch:249, accruracy in testing dataset(200 labeled): 0.925\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3622 - total_loss: 0.3622\n",
      "epoch:250, accruracy in testing dataset(200 labeled): 0.9262\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4383 - total_loss: 0.4383\n",
      "epoch:251, accruracy in testing dataset(200 labeled): 0.9227\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4111 - total_loss: 0.4111\n",
      "epoch:252, accruracy in testing dataset(200 labeled): 0.9203\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5079 - total_loss: 0.5079\n",
      "epoch:253, accruracy in testing dataset(200 labeled): 0.9239\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4166 - total_loss: 0.4166\n",
      "epoch:254, accruracy in testing dataset(200 labeled): 0.9235\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5561 - total_loss: 0.5561\n",
      "epoch:255, accruracy in testing dataset(200 labeled): 0.9251\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5766 - total_loss: 0.5766\n",
      "epoch:256, accruracy in testing dataset(200 labeled): 0.9184\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4889 - total_loss: 0.4889\n",
      "epoch:257, accruracy in testing dataset(200 labeled): 0.9255\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3856 - total_loss: 0.3856\n",
      "epoch:258, accruracy in testing dataset(200 labeled): 0.9281\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4626 - total_loss: 0.4626\n",
      "epoch:259, accruracy in testing dataset(200 labeled): 0.9266\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3706 - total_loss: 0.3706\n",
      "epoch:260, accruracy in testing dataset(200 labeled): 0.9295\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3617 - total_loss: 0.3617\n",
      "epoch:261, accruracy in testing dataset(200 labeled): 0.9281\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5287 - total_loss: 0.5287\n",
      "epoch:262, accruracy in testing dataset(200 labeled): 0.9282\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4166 - total_loss: 0.4166\n",
      "epoch:263, accruracy in testing dataset(200 labeled): 0.9304\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3859 - total_loss: 0.3859\n",
      "epoch:264, accruracy in testing dataset(200 labeled): 0.9289\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4330 - total_loss: 0.4330\n",
      "epoch:265, accruracy in testing dataset(200 labeled): 0.9298\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4711 - total_loss: 0.4711\n",
      "epoch:266, accruracy in testing dataset(200 labeled): 0.927\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4442 - total_loss: 0.4442\n",
      "epoch:267, accruracy in testing dataset(200 labeled): 0.9271\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3391 - total_loss: 0.3391\n",
      "epoch:268, accruracy in testing dataset(200 labeled): 0.9258\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4921 - total_loss: 0.4921\n",
      "epoch:269, accruracy in testing dataset(200 labeled): 0.9282\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3607 - total_loss: 0.3607\n",
      "epoch:270, accruracy in testing dataset(200 labeled): 0.9273\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4420 - total_loss: 0.4420\n",
      "epoch:271, accruracy in testing dataset(200 labeled): 0.925\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4652 - total_loss: 0.4652\n",
      "epoch:272, accruracy in testing dataset(200 labeled): 0.9253\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4446 - total_loss: 0.4446\n",
      "epoch:273, accruracy in testing dataset(200 labeled): 0.926\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4203 - total_loss: 0.4203\n",
      "epoch:274, accruracy in testing dataset(200 labeled): 0.926\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3679 - total_loss: 0.3679\n",
      "epoch:275, accruracy in testing dataset(200 labeled): 0.928\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3993 - total_loss: 0.3993\n",
      "epoch:276, accruracy in testing dataset(200 labeled): 0.9284\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5668 - total_loss: 0.5668\n",
      "epoch:277, accruracy in testing dataset(200 labeled): 0.9281\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5082 - total_loss: 0.5082\n",
      "epoch:278, accruracy in testing dataset(200 labeled): 0.9273\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3999 - total_loss: 0.3999\n",
      "epoch:279, accruracy in testing dataset(200 labeled): 0.9293\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4059 - total_loss: 0.4059\n",
      "epoch:280, accruracy in testing dataset(200 labeled): 0.9283\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4276 - total_loss: 0.4276\n",
      "epoch:281, accruracy in testing dataset(200 labeled): 0.9301\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5148 - total_loss: 0.5148\n",
      "epoch:282, accruracy in testing dataset(200 labeled): 0.9305\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4640 - total_loss: 0.4640\n",
      "epoch:283, accruracy in testing dataset(200 labeled): 0.9262\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4612 - total_loss: 0.4612\n",
      "epoch:284, accruracy in testing dataset(200 labeled): 0.9243\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4268 - total_loss: 0.4268\n",
      "epoch:285, accruracy in testing dataset(200 labeled): 0.9274\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3920 - total_loss: 0.3920\n",
      "epoch:286, accruracy in testing dataset(200 labeled): 0.9297\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4285 - total_loss: 0.4285\n",
      "epoch:287, accruracy in testing dataset(200 labeled): 0.9291\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4262 - total_loss: 0.4262\n",
      "epoch:288, accruracy in testing dataset(200 labeled): 0.926\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5250 - total_loss: 0.5250\n",
      "epoch:289, accruracy in testing dataset(200 labeled): 0.9295\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4098 - total_loss: 0.4098\n",
      "epoch:290, accruracy in testing dataset(200 labeled): 0.9256\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4826 - total_loss: 0.4826\n",
      "epoch:291, accruracy in testing dataset(200 labeled): 0.9261\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3122 - total_loss: 0.3122\n",
      "epoch:292, accruracy in testing dataset(200 labeled): 0.925\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3231 - total_loss: 0.3231\n",
      "epoch:293, accruracy in testing dataset(200 labeled): 0.9279\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4679 - total_loss: 0.4679\n",
      "epoch:294, accruracy in testing dataset(200 labeled): 0.926\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4737 - total_loss: 0.4737\n",
      "epoch:295, accruracy in testing dataset(200 labeled): 0.93\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3151 - total_loss: 0.3151\n",
      "epoch:296, accruracy in testing dataset(200 labeled): 0.9285\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3609 - total_loss: 0.3609\n",
      "epoch:297, accruracy in testing dataset(200 labeled): 0.9294\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3329 - total_loss: 0.3329\n",
      "epoch:298, accruracy in testing dataset(200 labeled): 0.9302\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4779 - total_loss: 0.4779\n",
      "epoch:1, accruracy in testing dataset(200 labeled): 0.9249\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3898 - total_loss: 0.3898\n",
      "epoch:2, accruracy in testing dataset(200 labeled): 0.9228\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4325 - total_loss: 0.4325\n",
      "epoch:3, accruracy in testing dataset(200 labeled): 0.9185\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4315 - total_loss: 0.4315\n",
      "epoch:4, accruracy in testing dataset(200 labeled): 0.9198\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3231 - total_loss: 0.3231\n",
      "epoch:5, accruracy in testing dataset(200 labeled): 0.9235\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3404 - total_loss: 0.3404\n",
      "epoch:6, accruracy in testing dataset(200 labeled): 0.9274\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4610 - total_loss: 0.4610\n",
      "epoch:7, accruracy in testing dataset(200 labeled): 0.9239\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4717 - total_loss: 0.4717\n",
      "epoch:8, accruracy in testing dataset(200 labeled): 0.9273\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3729 - total_loss: 0.3729\n",
      "epoch:9, accruracy in testing dataset(200 labeled): 0.9282\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4419 - total_loss: 0.4419\n",
      "epoch:10, accruracy in testing dataset(200 labeled): 0.9219\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4377 - total_loss: 0.4377\n",
      "epoch:11, accruracy in testing dataset(200 labeled): 0.9253\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4166 - total_loss: 0.4166\n",
      "epoch:12, accruracy in testing dataset(200 labeled): 0.9268\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3720 - total_loss: 0.3720\n",
      "epoch:13, accruracy in testing dataset(200 labeled): 0.9278\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3546 - total_loss: 0.3546\n",
      "epoch:14, accruracy in testing dataset(200 labeled): 0.93\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5483 - total_loss: 0.5483\n",
      "epoch:15, accruracy in testing dataset(200 labeled): 0.9307\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4507 - total_loss: 0.4507\n",
      "epoch:16, accruracy in testing dataset(200 labeled): 0.9291\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4038 - total_loss: 0.4038\n",
      "epoch:17, accruracy in testing dataset(200 labeled): 0.9328\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3544 - total_loss: 0.3544\n",
      "epoch:18, accruracy in testing dataset(200 labeled): 0.9318\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3878 - total_loss: 0.3878\n",
      "epoch:19, accruracy in testing dataset(200 labeled): 0.9272\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4591 - total_loss: 0.4591\n",
      "epoch:20, accruracy in testing dataset(200 labeled): 0.9265\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4123 - total_loss: 0.4123\n",
      "epoch:21, accruracy in testing dataset(200 labeled): 0.9266\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4291 - total_loss: 0.4291\n",
      "epoch:22, accruracy in testing dataset(200 labeled): 0.9182\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5261 - total_loss: 0.5261\n",
      "epoch:23, accruracy in testing dataset(200 labeled): 0.9188\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4702 - total_loss: 0.4702\n",
      "epoch:24, accruracy in testing dataset(200 labeled): 0.9195\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3738 - total_loss: 0.3738\n",
      "epoch:25, accruracy in testing dataset(200 labeled): 0.9248\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3937 - total_loss: 0.3937\n",
      "epoch:26, accruracy in testing dataset(200 labeled): 0.9277\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3304 - total_loss: 0.3304\n",
      "epoch:27, accruracy in testing dataset(200 labeled): 0.9275\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6083 - total_loss: 0.6083\n",
      "epoch:28, accruracy in testing dataset(200 labeled): 0.9294\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3526 - total_loss: 0.3526\n",
      "epoch:29, accruracy in testing dataset(200 labeled): 0.9298\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2891 - total_loss: 0.2891\n",
      "epoch:30, accruracy in testing dataset(200 labeled): 0.9298\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3981 - total_loss: 0.3981\n",
      "epoch:31, accruracy in testing dataset(200 labeled): 0.9305\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3016 - total_loss: 0.3016\n",
      "epoch:32, accruracy in testing dataset(200 labeled): 0.9305\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5055 - total_loss: 0.5055\n",
      "epoch:33, accruracy in testing dataset(200 labeled): 0.9205\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4758 - total_loss: 0.4758\n",
      "epoch:34, accruracy in testing dataset(200 labeled): 0.9189\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4331 - total_loss: 0.4331\n",
      "epoch:35, accruracy in testing dataset(200 labeled): 0.9235\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3301 - total_loss: 0.3301\n",
      "epoch:36, accruracy in testing dataset(200 labeled): 0.9256\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4812 - total_loss: 0.4812\n",
      "epoch:37, accruracy in testing dataset(200 labeled): 0.9246\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4165 - total_loss: 0.4165\n",
      "epoch:38, accruracy in testing dataset(200 labeled): 0.9277\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4502 - total_loss: 0.4502\n",
      "epoch:39, accruracy in testing dataset(200 labeled): 0.9277\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4056 - total_loss: 0.4056\n",
      "epoch:40, accruracy in testing dataset(200 labeled): 0.9303\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4657 - total_loss: 0.4657\n",
      "epoch:41, accruracy in testing dataset(200 labeled): 0.9325\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3846 - total_loss: 0.3846\n",
      "epoch:42, accruracy in testing dataset(200 labeled): 0.9307\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4352 - total_loss: 0.4352\n",
      "epoch:43, accruracy in testing dataset(200 labeled): 0.9286\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3015 - total_loss: 0.3015\n",
      "epoch:44, accruracy in testing dataset(200 labeled): 0.9314\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4122 - total_loss: 0.4122\n",
      "epoch:45, accruracy in testing dataset(200 labeled): 0.9289\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3649 - total_loss: 0.3649\n",
      "epoch:46, accruracy in testing dataset(200 labeled): 0.925\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4823 - total_loss: 0.4823\n",
      "epoch:47, accruracy in testing dataset(200 labeled): 0.9196\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3329 - total_loss: 0.3329\n",
      "epoch:48, accruracy in testing dataset(200 labeled): 0.9199\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3270 - total_loss: 0.3270\n",
      "epoch:49, accruracy in testing dataset(200 labeled): 0.9279\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3162 - total_loss: 0.3162\n",
      "epoch:50, accruracy in testing dataset(200 labeled): 0.9272\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3880 - total_loss: 0.3880\n",
      "epoch:51, accruracy in testing dataset(200 labeled): 0.9314\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4047 - total_loss: 0.4047\n",
      "epoch:52, accruracy in testing dataset(200 labeled): 0.9336\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4230 - total_loss: 0.4230\n",
      "epoch:53, accruracy in testing dataset(200 labeled): 0.9327\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3438 - total_loss: 0.3438\n",
      "epoch:54, accruracy in testing dataset(200 labeled): 0.9337\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4051 - total_loss: 0.4051\n",
      "epoch:55, accruracy in testing dataset(200 labeled): 0.9279\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3725 - total_loss: 0.3725\n",
      "epoch:56, accruracy in testing dataset(200 labeled): 0.9265\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4028 - total_loss: 0.4028\n",
      "epoch:57, accruracy in testing dataset(200 labeled): 0.9319\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5196 - total_loss: 0.5196\n",
      "epoch:58, accruracy in testing dataset(200 labeled): 0.9289\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3442 - total_loss: 0.3442\n",
      "epoch:59, accruracy in testing dataset(200 labeled): 0.9304\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4806 - total_loss: 0.4806\n",
      "epoch:60, accruracy in testing dataset(200 labeled): 0.9342\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4968 - total_loss: 0.4968\n",
      "epoch:61, accruracy in testing dataset(200 labeled): 0.9299\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4783 - total_loss: 0.4783\n",
      "epoch:62, accruracy in testing dataset(200 labeled): 0.9284\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3559 - total_loss: 0.3559\n",
      "epoch:63, accruracy in testing dataset(200 labeled): 0.9347\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4426 - total_loss: 0.4426\n",
      "epoch:64, accruracy in testing dataset(200 labeled): 0.9384\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3843 - total_loss: 0.3843\n",
      "epoch:65, accruracy in testing dataset(200 labeled): 0.9374\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3261 - total_loss: 0.3261\n",
      "epoch:66, accruracy in testing dataset(200 labeled): 0.937\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4349 - total_loss: 0.4349\n",
      "epoch:67, accruracy in testing dataset(200 labeled): 0.9376\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3779 - total_loss: 0.3779\n",
      "epoch:68, accruracy in testing dataset(200 labeled): 0.9352\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4033 - total_loss: 0.4033\n",
      "epoch:69, accruracy in testing dataset(200 labeled): 0.9327\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3755 - total_loss: 0.3755\n",
      "epoch:70, accruracy in testing dataset(200 labeled): 0.9321\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3642 - total_loss: 0.3642: 0s - loss: 0.3495 - total_loss: \n",
      "epoch:71, accruracy in testing dataset(200 labeled): 0.9342\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4483 - total_loss: 0.4483\n",
      "epoch:72, accruracy in testing dataset(200 labeled): 0.9331\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3679 - total_loss: 0.3679\n",
      "epoch:73, accruracy in testing dataset(200 labeled): 0.9349\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4569 - total_loss: 0.4569\n",
      "epoch:74, accruracy in testing dataset(200 labeled): 0.9353\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4120 - total_loss: 0.4120\n",
      "epoch:75, accruracy in testing dataset(200 labeled): 0.9344\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3694 - total_loss: 0.3694\n",
      "epoch:76, accruracy in testing dataset(200 labeled): 0.9325\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4198 - total_loss: 0.4198\n",
      "epoch:77, accruracy in testing dataset(200 labeled): 0.9311\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3952 - total_loss: 0.3952\n",
      "epoch:78, accruracy in testing dataset(200 labeled): 0.9304\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3298 - total_loss: 0.3298\n",
      "epoch:79, accruracy in testing dataset(200 labeled): 0.9302\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3522 - total_loss: 0.3522\n",
      "epoch:80, accruracy in testing dataset(200 labeled): 0.9302\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3289 - total_loss: 0.3289\n",
      "epoch:81, accruracy in testing dataset(200 labeled): 0.9334\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3646 - total_loss: 0.3646\n",
      "epoch:82, accruracy in testing dataset(200 labeled): 0.9316\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3982 - total_loss: 0.3982\n",
      "epoch:83, accruracy in testing dataset(200 labeled): 0.9296\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3176 - total_loss: 0.3176\n",
      "epoch:84, accruracy in testing dataset(200 labeled): 0.9315\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5093 - total_loss: 0.5093\n",
      "epoch:85, accruracy in testing dataset(200 labeled): 0.9306\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3837 - total_loss: 0.3837\n",
      "epoch:86, accruracy in testing dataset(200 labeled): 0.9286\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4509 - total_loss: 0.4509\n",
      "epoch:87, accruracy in testing dataset(200 labeled): 0.9321\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4342 - total_loss: 0.4342\n",
      "epoch:88, accruracy in testing dataset(200 labeled): 0.9344\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3216 - total_loss: 0.3216\n",
      "epoch:89, accruracy in testing dataset(200 labeled): 0.9358\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3575 - total_loss: 0.3575\n",
      "epoch:90, accruracy in testing dataset(200 labeled): 0.9309\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3117 - total_loss: 0.3117\n",
      "epoch:91, accruracy in testing dataset(200 labeled): 0.9315\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3988 - total_loss: 0.3988\n",
      "epoch:92, accruracy in testing dataset(200 labeled): 0.9298\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2567 - total_loss: 0.2567\n",
      "epoch:93, accruracy in testing dataset(200 labeled): 0.9302\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3227 - total_loss: 0.3227\n",
      "epoch:94, accruracy in testing dataset(200 labeled): 0.9315\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4359 - total_loss: 0.4359\n",
      "epoch:95, accruracy in testing dataset(200 labeled): 0.9319\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4318 - total_loss: 0.4318\n",
      "epoch:96, accruracy in testing dataset(200 labeled): 0.9327\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4065 - total_loss: 0.4065\n",
      "epoch:97, accruracy in testing dataset(200 labeled): 0.9341\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4486 - total_loss: 0.4486\n",
      "epoch:98, accruracy in testing dataset(200 labeled): 0.9328\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2920 - total_loss: 0.2920\n",
      "epoch:99, accruracy in testing dataset(200 labeled): 0.9344\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2987 - total_loss: 0.2987\n",
      "epoch:100, accruracy in testing dataset(200 labeled): 0.9335\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3563 - total_loss: 0.3563\n",
      "epoch:101, accruracy in testing dataset(200 labeled): 0.9341\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3789 - total_loss: 0.3789\n",
      "epoch:102, accruracy in testing dataset(200 labeled): 0.9317\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3642 - total_loss: 0.3642\n",
      "epoch:103, accruracy in testing dataset(200 labeled): 0.9333\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3748 - total_loss: 0.3748\n",
      "epoch:104, accruracy in testing dataset(200 labeled): 0.9311\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4909 - total_loss: 0.4909\n",
      "epoch:105, accruracy in testing dataset(200 labeled): 0.9326\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5056 - total_loss: 0.5056\n",
      "epoch:106, accruracy in testing dataset(200 labeled): 0.9329\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4159 - total_loss: 0.4159\n",
      "epoch:107, accruracy in testing dataset(200 labeled): 0.9336\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3624 - total_loss: 0.3624\n",
      "epoch:108, accruracy in testing dataset(200 labeled): 0.9299\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4203 - total_loss: 0.4203\n",
      "epoch:109, accruracy in testing dataset(200 labeled): 0.9279\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4448 - total_loss: 0.4448\n",
      "epoch:110, accruracy in testing dataset(200 labeled): 0.9236\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4223 - total_loss: 0.4223\n",
      "epoch:111, accruracy in testing dataset(200 labeled): 0.9264\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3580 - total_loss: 0.3580\n",
      "epoch:112, accruracy in testing dataset(200 labeled): 0.9238\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4341 - total_loss: 0.4341\n",
      "epoch:113, accruracy in testing dataset(200 labeled): 0.9227\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3739 - total_loss: 0.3739\n",
      "epoch:114, accruracy in testing dataset(200 labeled): 0.9216\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3626 - total_loss: 0.3626\n",
      "epoch:115, accruracy in testing dataset(200 labeled): 0.9315\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3389 - total_loss: 0.3389\n",
      "epoch:116, accruracy in testing dataset(200 labeled): 0.9302\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4020 - total_loss: 0.4020\n",
      "epoch:117, accruracy in testing dataset(200 labeled): 0.9248\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3822 - total_loss: 0.3822\n",
      "epoch:118, accruracy in testing dataset(200 labeled): 0.9183\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3645 - total_loss: 0.3645\n",
      "epoch:119, accruracy in testing dataset(200 labeled): 0.9188\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5231 - total_loss: 0.5231\n",
      "epoch:120, accruracy in testing dataset(200 labeled): 0.9199\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5361 - total_loss: 0.5361\n",
      "epoch:121, accruracy in testing dataset(200 labeled): 0.9247\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4256 - total_loss: 0.4256\n",
      "epoch:122, accruracy in testing dataset(200 labeled): 0.9267\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3611 - total_loss: 0.3611\n",
      "epoch:123, accruracy in testing dataset(200 labeled): 0.9273\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4212 - total_loss: 0.4212\n",
      "epoch:124, accruracy in testing dataset(200 labeled): 0.9249\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3530 - total_loss: 0.3530\n",
      "epoch:125, accruracy in testing dataset(200 labeled): 0.9269\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3287 - total_loss: 0.3287\n",
      "epoch:126, accruracy in testing dataset(200 labeled): 0.9304\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3529 - total_loss: 0.3529\n",
      "epoch:127, accruracy in testing dataset(200 labeled): 0.9271\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2992 - total_loss: 0.2992\n",
      "epoch:128, accruracy in testing dataset(200 labeled): 0.9281\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3861 - total_loss: 0.3861\n",
      "epoch:129, accruracy in testing dataset(200 labeled): 0.9249\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3902 - total_loss: 0.3902\n",
      "epoch:130, accruracy in testing dataset(200 labeled): 0.9294\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3922 - total_loss: 0.3922\n",
      "epoch:131, accruracy in testing dataset(200 labeled): 0.9278\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2848 - total_loss: 0.2848\n",
      "epoch:132, accruracy in testing dataset(200 labeled): 0.9303\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3544 - total_loss: 0.3544\n",
      "epoch:133, accruracy in testing dataset(200 labeled): 0.9335\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4556 - total_loss: 0.4556\n",
      "epoch:134, accruracy in testing dataset(200 labeled): 0.9342\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4710 - total_loss: 0.4710\n",
      "epoch:135, accruracy in testing dataset(200 labeled): 0.9338\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3789 - total_loss: 0.3789\n",
      "epoch:136, accruracy in testing dataset(200 labeled): 0.9345\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5102 - total_loss: 0.5102\n",
      "epoch:137, accruracy in testing dataset(200 labeled): 0.936\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4370 - total_loss: 0.4370\n",
      "epoch:138, accruracy in testing dataset(200 labeled): 0.9337\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2995 - total_loss: 0.2995\n",
      "epoch:139, accruracy in testing dataset(200 labeled): 0.9334\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4175 - total_loss: 0.4175\n",
      "epoch:140, accruracy in testing dataset(200 labeled): 0.9376\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3194 - total_loss: 0.3194\n",
      "epoch:141, accruracy in testing dataset(200 labeled): 0.9356\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4097 - total_loss: 0.4097\n",
      "epoch:142, accruracy in testing dataset(200 labeled): 0.9351\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4703 - total_loss: 0.4703\n",
      "epoch:143, accruracy in testing dataset(200 labeled): 0.9363\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3718 - total_loss: 0.3718\n",
      "epoch:144, accruracy in testing dataset(200 labeled): 0.9344\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5333 - total_loss: 0.5333\n",
      "epoch:145, accruracy in testing dataset(200 labeled): 0.9315\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2291 - total_loss: 0.2291\n",
      "epoch:146, accruracy in testing dataset(200 labeled): 0.9326\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4519 - total_loss: 0.4519\n",
      "epoch:147, accruracy in testing dataset(200 labeled): 0.9306\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3672 - total_loss: 0.3672\n",
      "epoch:148, accruracy in testing dataset(200 labeled): 0.9344\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3846 - total_loss: 0.3846\n",
      "epoch:149, accruracy in testing dataset(200 labeled): 0.9331\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4417 - total_loss: 0.4417\n",
      "epoch:150, accruracy in testing dataset(200 labeled): 0.9363\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3681 - total_loss: 0.3681\n",
      "epoch:151, accruracy in testing dataset(200 labeled): 0.9321\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3699 - total_loss: 0.3699\n",
      "epoch:152, accruracy in testing dataset(200 labeled): 0.9305\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4332 - total_loss: 0.4332\n",
      "epoch:153, accruracy in testing dataset(200 labeled): 0.9274\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3696 - total_loss: 0.3696\n",
      "epoch:154, accruracy in testing dataset(200 labeled): 0.9174\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3140 - total_loss: 0.3140\n",
      "epoch:155, accruracy in testing dataset(200 labeled): 0.9222\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3137 - total_loss: 0.3137\n",
      "epoch:156, accruracy in testing dataset(200 labeled): 0.9203\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4957 - total_loss: 0.4957\n",
      "epoch:157, accruracy in testing dataset(200 labeled): 0.9224\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3202 - total_loss: 0.3202\n",
      "epoch:158, accruracy in testing dataset(200 labeled): 0.9224\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3994 - total_loss: 0.3994\n",
      "epoch:159, accruracy in testing dataset(200 labeled): 0.9265\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3868 - total_loss: 0.3868\n",
      "epoch:160, accruracy in testing dataset(200 labeled): 0.9253\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3151 - total_loss: 0.3151\n",
      "epoch:161, accruracy in testing dataset(200 labeled): 0.9263\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3151 - total_loss: 0.3151\n",
      "epoch:162, accruracy in testing dataset(200 labeled): 0.9282\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2995 - total_loss: 0.2995\n",
      "epoch:163, accruracy in testing dataset(200 labeled): 0.9274\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3699 - total_loss: 0.3699\n",
      "epoch:164, accruracy in testing dataset(200 labeled): 0.9318\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3222 - total_loss: 0.3222\n",
      "epoch:165, accruracy in testing dataset(200 labeled): 0.9313\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3896 - total_loss: 0.3896\n",
      "epoch:166, accruracy in testing dataset(200 labeled): 0.9322\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3776 - total_loss: 0.3776\n",
      "epoch:167, accruracy in testing dataset(200 labeled): 0.9321\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3924 - total_loss: 0.3924\n",
      "epoch:168, accruracy in testing dataset(200 labeled): 0.9341\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3211 - total_loss: 0.3211\n",
      "epoch:169, accruracy in testing dataset(200 labeled): 0.9345\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3074 - total_loss: 0.3074\n",
      "epoch:170, accruracy in testing dataset(200 labeled): 0.9356\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4166 - total_loss: 0.4166\n",
      "epoch:171, accruracy in testing dataset(200 labeled): 0.9359\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2873 - total_loss: 0.2873\n",
      "epoch:172, accruracy in testing dataset(200 labeled): 0.9366\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3937 - total_loss: 0.3937\n",
      "epoch:173, accruracy in testing dataset(200 labeled): 0.9356\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4360 - total_loss: 0.4360\n",
      "epoch:174, accruracy in testing dataset(200 labeled): 0.9372\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4118 - total_loss: 0.4118\n",
      "epoch:175, accruracy in testing dataset(200 labeled): 0.9375\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3122 - total_loss: 0.3122\n",
      "epoch:176, accruracy in testing dataset(200 labeled): 0.9397\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3538 - total_loss: 0.3538\n",
      "epoch:177, accruracy in testing dataset(200 labeled): 0.9369\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4354 - total_loss: 0.4354\n",
      "epoch:178, accruracy in testing dataset(200 labeled): 0.935\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2936 - total_loss: 0.2936\n",
      "epoch:179, accruracy in testing dataset(200 labeled): 0.9357\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2855 - total_loss: 0.2855\n",
      "epoch:180, accruracy in testing dataset(200 labeled): 0.9374\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3912 - total_loss: 0.3912\n",
      "epoch:181, accruracy in testing dataset(200 labeled): 0.9377\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3619 - total_loss: 0.3619\n",
      "epoch:182, accruracy in testing dataset(200 labeled): 0.9384\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3230 - total_loss: 0.3230\n",
      "epoch:183, accruracy in testing dataset(200 labeled): 0.936\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3387 - total_loss: 0.3387\n",
      "epoch:184, accruracy in testing dataset(200 labeled): 0.9349\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3296 - total_loss: 0.3296\n",
      "epoch:185, accruracy in testing dataset(200 labeled): 0.9334\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2956 - total_loss: 0.2956\n",
      "epoch:186, accruracy in testing dataset(200 labeled): 0.9341\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3597 - total_loss: 0.3597\n",
      "epoch:187, accruracy in testing dataset(200 labeled): 0.9348\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3404 - total_loss: 0.3404\n",
      "epoch:188, accruracy in testing dataset(200 labeled): 0.9364\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3915 - total_loss: 0.3915\n",
      "epoch:189, accruracy in testing dataset(200 labeled): 0.9331\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2889 - total_loss: 0.2889\n",
      "epoch:190, accruracy in testing dataset(200 labeled): 0.9341\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4681 - total_loss: 0.4681\n",
      "epoch:191, accruracy in testing dataset(200 labeled): 0.9336\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3599 - total_loss: 0.3599\n",
      "epoch:192, accruracy in testing dataset(200 labeled): 0.938\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2587 - total_loss: 0.2587\n",
      "epoch:193, accruracy in testing dataset(200 labeled): 0.937\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4930 - total_loss: 0.4930\n",
      "epoch:194, accruracy in testing dataset(200 labeled): 0.9339\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4000 - total_loss: 0.4000\n",
      "epoch:195, accruracy in testing dataset(200 labeled): 0.9338\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4343 - total_loss: 0.4343\n",
      "epoch:196, accruracy in testing dataset(200 labeled): 0.9366\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3818 - total_loss: 0.3818\n",
      "epoch:197, accruracy in testing dataset(200 labeled): 0.9367\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4301 - total_loss: 0.4301\n",
      "epoch:198, accruracy in testing dataset(200 labeled): 0.9338\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3221 - total_loss: 0.3221\n",
      "epoch:199, accruracy in testing dataset(200 labeled): 0.9349\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3652 - total_loss: 0.3652\n",
      "epoch:200, accruracy in testing dataset(200 labeled): 0.9362\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3821 - total_loss: 0.3821\n",
      "epoch:201, accruracy in testing dataset(200 labeled): 0.9365\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2986 - total_loss: 0.2986\n",
      "epoch:202, accruracy in testing dataset(200 labeled): 0.9353\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3826 - total_loss: 0.3826\n",
      "epoch:203, accruracy in testing dataset(200 labeled): 0.9354\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3322 - total_loss: 0.3322\n",
      "epoch:204, accruracy in testing dataset(200 labeled): 0.9354\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3476 - total_loss: 0.3476\n",
      "epoch:205, accruracy in testing dataset(200 labeled): 0.9345\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3228 - total_loss: 0.3228\n",
      "epoch:206, accruracy in testing dataset(200 labeled): 0.9337\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3464 - total_loss: 0.3464\n",
      "epoch:207, accruracy in testing dataset(200 labeled): 0.9333\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3500 - total_loss: 0.3500\n",
      "epoch:208, accruracy in testing dataset(200 labeled): 0.9337\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4148 - total_loss: 0.4148\n",
      "epoch:209, accruracy in testing dataset(200 labeled): 0.9356\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3720 - total_loss: 0.3720\n",
      "epoch:210, accruracy in testing dataset(200 labeled): 0.9344\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3490 - total_loss: 0.3490\n",
      "epoch:211, accruracy in testing dataset(200 labeled): 0.9366\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3474 - total_loss: 0.3474\n",
      "epoch:212, accruracy in testing dataset(200 labeled): 0.938\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4934 - total_loss: 0.4934\n",
      "epoch:213, accruracy in testing dataset(200 labeled): 0.9338\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3601 - total_loss: 0.3601\n",
      "epoch:214, accruracy in testing dataset(200 labeled): 0.9354\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4003 - total_loss: 0.4003\n",
      "epoch:215, accruracy in testing dataset(200 labeled): 0.9346\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4389 - total_loss: 0.4389\n",
      "epoch:216, accruracy in testing dataset(200 labeled): 0.9298\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3518 - total_loss: 0.3518\n",
      "epoch:217, accruracy in testing dataset(200 labeled): 0.9279\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3949 - total_loss: 0.3949\n",
      "epoch:218, accruracy in testing dataset(200 labeled): 0.9293\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4081 - total_loss: 0.4081\n",
      "epoch:219, accruracy in testing dataset(200 labeled): 0.9285\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3965 - total_loss: 0.3965\n",
      "epoch:220, accruracy in testing dataset(200 labeled): 0.9308\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3581 - total_loss: 0.3581\n",
      "epoch:221, accruracy in testing dataset(200 labeled): 0.9305\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3093 - total_loss: 0.3093\n",
      "epoch:222, accruracy in testing dataset(200 labeled): 0.932\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3241 - total_loss: 0.3241\n",
      "epoch:223, accruracy in testing dataset(200 labeled): 0.9329\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4613 - total_loss: 0.4613\n",
      "epoch:224, accruracy in testing dataset(200 labeled): 0.9261\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3587 - total_loss: 0.3587\n",
      "epoch:225, accruracy in testing dataset(200 labeled): 0.9215\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2968 - total_loss: 0.2968\n",
      "epoch:226, accruracy in testing dataset(200 labeled): 0.9254\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4177 - total_loss: 0.4177\n",
      "epoch:227, accruracy in testing dataset(200 labeled): 0.9276\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4138 - total_loss: 0.4138\n",
      "epoch:228, accruracy in testing dataset(200 labeled): 0.9295\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2862 - total_loss: 0.2862\n",
      "epoch:229, accruracy in testing dataset(200 labeled): 0.9288\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3505 - total_loss: 0.3505\n",
      "epoch:230, accruracy in testing dataset(200 labeled): 0.9295\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3119 - total_loss: 0.3119\n",
      "epoch:231, accruracy in testing dataset(200 labeled): 0.9344\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2594 - total_loss: 0.2594\n",
      "epoch:232, accruracy in testing dataset(200 labeled): 0.9343\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3642 - total_loss: 0.3642\n",
      "epoch:233, accruracy in testing dataset(200 labeled): 0.9349\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4484 - total_loss: 0.4484\n",
      "epoch:234, accruracy in testing dataset(200 labeled): 0.9311\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3060 - total_loss: 0.3060\n",
      "epoch:235, accruracy in testing dataset(200 labeled): 0.9318\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3168 - total_loss: 0.3168\n",
      "epoch:236, accruracy in testing dataset(200 labeled): 0.9333\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3845 - total_loss: 0.3845\n",
      "epoch:237, accruracy in testing dataset(200 labeled): 0.9331\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4393 - total_loss: 0.4393\n",
      "epoch:238, accruracy in testing dataset(200 labeled): 0.9353\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3354 - total_loss: 0.3354\n",
      "epoch:239, accruracy in testing dataset(200 labeled): 0.9353\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2842 - total_loss: 0.2842\n",
      "epoch:240, accruracy in testing dataset(200 labeled): 0.9361\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2868 - total_loss: 0.2868\n",
      "epoch:241, accruracy in testing dataset(200 labeled): 0.937\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2599 - total_loss: 0.2599\n",
      "epoch:242, accruracy in testing dataset(200 labeled): 0.9361\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3618 - total_loss: 0.3618\n",
      "epoch:243, accruracy in testing dataset(200 labeled): 0.9363\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2809 - total_loss: 0.2809\n",
      "epoch:244, accruracy in testing dataset(200 labeled): 0.9387\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3042 - total_loss: 0.3042\n",
      "epoch:245, accruracy in testing dataset(200 labeled): 0.9414\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2778 - total_loss: 0.2778\n",
      "epoch:246, accruracy in testing dataset(200 labeled): 0.9393\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4255 - total_loss: 0.4255\n",
      "epoch:247, accruracy in testing dataset(200 labeled): 0.9395\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3139 - total_loss: 0.3139\n",
      "epoch:248, accruracy in testing dataset(200 labeled): 0.9402\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2111 - total_loss: 0.2111\n",
      "epoch:249, accruracy in testing dataset(200 labeled): 0.9402\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3251 - total_loss: 0.3251\n",
      "epoch:250, accruracy in testing dataset(200 labeled): 0.9363\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3570 - total_loss: 0.3570\n",
      "epoch:251, accruracy in testing dataset(200 labeled): 0.9322\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4734 - total_loss: 0.4734\n",
      "epoch:252, accruracy in testing dataset(200 labeled): 0.9357\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3229 - total_loss: 0.3229\n",
      "epoch:253, accruracy in testing dataset(200 labeled): 0.9354\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3276 - total_loss: 0.3276\n",
      "epoch:254, accruracy in testing dataset(200 labeled): 0.9353\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3111 - total_loss: 0.3111\n",
      "epoch:255, accruracy in testing dataset(200 labeled): 0.9327\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5883 - total_loss: 0.5883\n",
      "epoch:256, accruracy in testing dataset(200 labeled): 0.9329\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3295 - total_loss: 0.3295\n",
      "epoch:257, accruracy in testing dataset(200 labeled): 0.9323\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2546 - total_loss: 0.2546\n",
      "epoch:258, accruracy in testing dataset(200 labeled): 0.9326\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3796 - total_loss: 0.3796\n",
      "epoch:259, accruracy in testing dataset(200 labeled): 0.9357\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3094 - total_loss: 0.3094\n",
      "epoch:260, accruracy in testing dataset(200 labeled): 0.9384\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3286 - total_loss: 0.3286\n",
      "epoch:261, accruracy in testing dataset(200 labeled): 0.9353\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4034 - total_loss: 0.4034\n",
      "epoch:262, accruracy in testing dataset(200 labeled): 0.9422\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3439 - total_loss: 0.3439\n",
      "epoch:263, accruracy in testing dataset(200 labeled): 0.9436\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3279 - total_loss: 0.3279\n",
      "epoch:264, accruracy in testing dataset(200 labeled): 0.9435\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3350 - total_loss: 0.3350\n",
      "epoch:265, accruracy in testing dataset(200 labeled): 0.9448\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3549 - total_loss: 0.3549\n",
      "epoch:266, accruracy in testing dataset(200 labeled): 0.9426\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3346 - total_loss: 0.3346\n",
      "epoch:267, accruracy in testing dataset(200 labeled): 0.9424\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3047 - total_loss: 0.3047\n",
      "epoch:268, accruracy in testing dataset(200 labeled): 0.9396\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3567 - total_loss: 0.3567\n",
      "epoch:269, accruracy in testing dataset(200 labeled): 0.9372\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2591 - total_loss: 0.2591\n",
      "epoch:270, accruracy in testing dataset(200 labeled): 0.935\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2892 - total_loss: 0.2892\n",
      "epoch:271, accruracy in testing dataset(200 labeled): 0.9372\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3101 - total_loss: 0.3101\n",
      "epoch:272, accruracy in testing dataset(200 labeled): 0.9383\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3589 - total_loss: 0.3589\n",
      "epoch:273, accruracy in testing dataset(200 labeled): 0.9368\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3326 - total_loss: 0.3326\n",
      "epoch:274, accruracy in testing dataset(200 labeled): 0.9387\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3172 - total_loss: 0.3172\n",
      "epoch:275, accruracy in testing dataset(200 labeled): 0.9398\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4243 - total_loss: 0.4243\n",
      "epoch:276, accruracy in testing dataset(200 labeled): 0.9387\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2807 - total_loss: 0.2807\n",
      "epoch:277, accruracy in testing dataset(200 labeled): 0.9382\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3599 - total_loss: 0.3599\n",
      "epoch:278, accruracy in testing dataset(200 labeled): 0.9377\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3634 - total_loss: 0.3634\n",
      "epoch:279, accruracy in testing dataset(200 labeled): 0.9391\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3226 - total_loss: 0.3226\n",
      "epoch:280, accruracy in testing dataset(200 labeled): 0.9382\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3084 - total_loss: 0.3084\n",
      "epoch:281, accruracy in testing dataset(200 labeled): 0.9387\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3644 - total_loss: 0.3644\n",
      "epoch:282, accruracy in testing dataset(200 labeled): 0.9394\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3776 - total_loss: 0.3776\n",
      "epoch:283, accruracy in testing dataset(200 labeled): 0.9347\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3385 - total_loss: 0.3385\n",
      "epoch:284, accruracy in testing dataset(200 labeled): 0.9339\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2596 - total_loss: 0.2596\n",
      "epoch:285, accruracy in testing dataset(200 labeled): 0.9371\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3845 - total_loss: 0.3845\n",
      "epoch:286, accruracy in testing dataset(200 labeled): 0.9362\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3407 - total_loss: 0.3407\n",
      "epoch:287, accruracy in testing dataset(200 labeled): 0.9385\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2429 - total_loss: 0.2429\n",
      "epoch:288, accruracy in testing dataset(200 labeled): 0.9394\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4196 - total_loss: 0.4196\n",
      "epoch:289, accruracy in testing dataset(200 labeled): 0.9378\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2641 - total_loss: 0.2641\n",
      "epoch:290, accruracy in testing dataset(200 labeled): 0.9362\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2968 - total_loss: 0.2968\n",
      "epoch:291, accruracy in testing dataset(200 labeled): 0.9361\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2856 - total_loss: 0.2856\n",
      "epoch:292, accruracy in testing dataset(200 labeled): 0.9334\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3246 - total_loss: 0.3246\n",
      "epoch:293, accruracy in testing dataset(200 labeled): 0.934\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3412 - total_loss: 0.3412\n",
      "epoch:294, accruracy in testing dataset(200 labeled): 0.9348\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3659 - total_loss: 0.3659\n",
      "epoch:295, accruracy in testing dataset(200 labeled): 0.9355\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2375 - total_loss: 0.2375\n",
      "epoch:296, accruracy in testing dataset(200 labeled): 0.9359\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3636 - total_loss: 0.3636\n",
      "epoch:297, accruracy in testing dataset(200 labeled): 0.9356\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2941 - total_loss: 0.2941\n",
      "epoch:298, accruracy in testing dataset(200 labeled): 0.9371\n"
     ]
    }
   ],
   "source": [
    "model_vat_mnist_200_acc_trace=[]\n",
    "model_vat_mnist_200_loss_trace=[]\n",
    "model_vat_mnist_200=vat_mnist_200(epsilon=10,alpha=0.33)\n",
    "for j in range(3):\n",
    "    for i in range(200,59800,200):\n",
    "        model_vat_mnist_200.fit([train_img7784[i:i+200,:],train_img7784[0:200,:],Y_train_cat[0:200,:]],None ,epochs=1)\n",
    "        y_pred  =model_vat_mnist_200.predict([test_img7784,test_img7784,test_label_useless]).argmax(-1)\n",
    "        acc=accuracy_score(test_label , y_pred)\n",
    "        print(f\"epoch:{i//200}, accruracy in testing dataset(200 labeled): {acc}\")\n",
    "        model_vat_mnist_200_acc_trace.append(acc)\n",
    "        model_vat_mnist_200_loss_trace.append(model_vat_mnist_200.history.history['total_loss'][-1])\n",
    "#del model_vat_mnist_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_img7784,test_img7784,Y_train_cat,test_label,y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVHN prediction using CNN with 3000 labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport to keep everything works \n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import * \n",
    "from keras.layers import *\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model for CNN on SVHN\n",
    "def build_model():\n",
    "    input_shape = (32,32,3)\n",
    "    \n",
    "    cnn_model=keras.Sequential()\n",
    "    # conv layer 1\n",
    "\n",
    "    cnn_model.add(keras.layers.Conv2D(filters=32, kernel_size = 3,strides = (1,1),\n",
    "                                  padding = 'same',activation = tensorflow.nn.relu,input_shape = input_shape))\n",
    "    cnn_model.add(keras.layers.BatchNormalization())\n",
    "    cnn_model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (1,1), padding = 'valid'))\n",
    "    cnn_model.add(keras.layers.Dropout(0.3))\n",
    "    # conv layer 2\n",
    "    cnn_model.add(keras.layers.Conv2D(filters=64,kernel_size = 3,strides = (1,1),\n",
    "                                      padding = 'same',activation = tensorflow.nn.relu))\n",
    "    cnn_model.add(keras.layers.BatchNormalization())\n",
    "    cnn_model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (1,1), padding = 'valid'))\n",
    "    cnn_model.add(keras.layers.Dropout(0.3))\n",
    "    # conv layer 3\n",
    "    cnn_model.add(keras.layers.Conv2D(filters=128,kernel_size = 3,strides = (1,1),\n",
    "                                      padding = 'same',activation = tensorflow.nn.relu))\n",
    "    cnn_model.add(keras.layers.MaxPool2D(pool_size=(2,2),padding = 'valid'))\n",
    "    \n",
    "    cnn_model.add(keras.layers.Dropout(0.5))\n",
    "    \n",
    "    cnn_model.add(keras.layers.Flatten())\n",
    "    cnn_model.add(keras.layers.Dense(units=1024,activation = tensorflow.nn.relu))\n",
    "    cnn_model.add(keras.layers.Dense(units=512,activation = tensorflow.nn.relu))\n",
    "    #cnn_model.add(keras.layers.Dropout(0.5))\n",
    "    cnn_model.add(keras.layers.Dense(units=11,activation = tensorflow.nn.softmax))\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adjusting hyperparameters multiple times, we found 3 CNN layers, each one with kernal size as 3, stride 1 works best for SVHN dataset, reLu as activation method, and set dropout ratio as 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reimport data if needed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, gzip\n",
    "import scipy.io as sio\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3, 3000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset, \n",
    "# take first ---3000--- labeled data samples for training\n",
    "train_contents = sio.loadmat('train_32_32.mat')\n",
    "test_contents = sio.loadmat('test_32_32.mat')\n",
    "Xtrain_3000 = train_contents['X'][:,:,:,0:3000].astype('float32')\n",
    "Ttrain_3000 = train_contents['y'][0:3000].astype('float32')\n",
    "Xtest_3000 = test_contents['X'].astype('float32')\n",
    "Ttest_3000 = test_contents['y'].astype('float32')\n",
    "Xtrain_3000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_3000 = Xtrain_3000.transpose((3,0,1,2))\n",
    "Xtest_3000 = Xtest_3000.transpose((3,0,1,2))\n",
    "\n",
    "Xtrain_3000 = np.array(Xtrain_3000)\n",
    "Ttrain_3000 = np.array(Ttrain_3000)\n",
    "Xtest_3000 = np.array(Xtest_3000)\n",
    "Ttest_3000 = np.array(Ttest_3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ttrain_3000 = Ttrain_3000.flatten()\n",
    "Ttest_3000 = Ttest_3000.flatten()\n",
    "Ttest_3000 = keras.utils.to_categorical(Ttest_3000)\n",
    "Ttrain_3000 = keras.utils.to_categorical(Ttrain_3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_3000label = build_model()\n",
    "cnn_model_3000label.compile(optimizer=tensorflow.optimizers.Adam(),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 9.3752 - accuracy: 0.1547\n",
      "Epoch 2/15\n",
      "3000/3000 [==============================] - 21s 7ms/step - loss: 2.2677 - accuracy: 0.2000\n",
      "Epoch 3/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 2.2121 - accuracy: 0.2037\n",
      "Epoch 4/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 2.0563 - accuracy: 0.2633\n",
      "Epoch 5/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 1.7065 - accuracy: 0.4143\n",
      "Epoch 6/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 1.3145 - accuracy: 0.5660\n",
      "Epoch 7/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 0.9280 - accuracy: 0.6950\n",
      "Epoch 8/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 0.7068 - accuracy: 0.7630\n",
      "Epoch 9/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 0.5111 - accuracy: 0.8303\n",
      "Epoch 10/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 0.4185 - accuracy: 0.8627\n",
      "Epoch 11/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 0.2921 - accuracy: 0.9027\n",
      "Epoch 12/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 0.2813 - accuracy: 0.9033\n",
      "Epoch 13/15\n",
      "3000/3000 [==============================] - 22s 7ms/step - loss: 0.1801 - accuracy: 0.9397\n",
      "Epoch 14/15\n",
      "3000/3000 [==============================] - 24s 8ms/step - loss: 0.1895 - accuracy: 0.9407\n",
      "Epoch 15/15\n",
      "3000/3000 [==============================] - 24s 8ms/step - loss: 0.1355 - accuracy: 0.9557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14e80dfd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_3000label.fit(Xtrain_3000, Ttrain_3000, batch_size=64, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26032/26032 [==============================] - 49s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "loss_3000label, accuracy_3000label = cnn_model_3000label.evaluate(Xtest_3000, Ttest_3000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:1.6298 accuracy:0.7056\n"
     ]
    }
   ],
   "source": [
    "print('loss:%.4f accuracy:%.4f' %(loss_3000label, accuracy_3000label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above experiment we use 3000 labeled data samples to train CNN model, then test the classification prediction of 26032 testing images. The result would fluctuate a little bit if we do experiment multiple times.  \n",
    "With CNN, using 3000 labeled images to predict 26032 images attain 70%~74% accuray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVHN prediction using CNN with 2000 labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3, 2000)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before start, import library and build_model() again if needed\n",
    "\n",
    "# Load the dataset, \n",
    "# take first ---2000--- labeled data samples for training\n",
    "train_contents = sio.loadmat('train_32_32.mat')\n",
    "test_contents = sio.loadmat('test_32_32.mat')\n",
    "Xtrain_2000 = train_contents['X'][:,:,:,0:2000].astype('float32')\n",
    "Ttrain_2000 = train_contents['y'][0:2000].astype('float32')\n",
    "Xtest_2000 = test_contents['X'].astype('float32')\n",
    "Ttest_2000 = test_contents['y'].astype('float32')\n",
    "Xtrain_2000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_2000 = Xtrain_2000.transpose((3,0,1,2))\n",
    "Xtest_2000 = Xtest_2000.transpose((3,0,1,2))\n",
    "\n",
    "Xtrain_2000 = np.array(Xtrain_2000)\n",
    "Ttrain_2000 = np.array(Ttrain_2000)\n",
    "Xtest_2000 = np.array(Xtest_2000)\n",
    "Ttest_2000 = np.array(Ttest_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ttrain_2000 = Ttrain_2000.flatten()\n",
    "Ttest_2000 = Ttest_2000.flatten()\n",
    "Ttest_2000 = keras.utils.to_categorical(Ttest_2000)\n",
    "Ttrain_2000 = keras.utils.to_categorical(Ttrain_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_2000label = build_model()\n",
    "cnn_model_2000label.compile(optimizer=tensorflow.optimizers.Adam(),loss=\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 14.9439 - accuracy: 0.1665\n",
      "Epoch 2/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2914 - accuracy: 0.1945\n",
      "Epoch 3/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 2.2168 - accuracy: 0.2255\n",
      "Epoch 4/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 2.0824 - accuracy: 0.2880\n",
      "Epoch 5/19\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.8217 - accuracy: 0.3690\n",
      "Epoch 6/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 1.4161 - accuracy: 0.5165\n",
      "Epoch 7/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.0877 - accuracy: 0.6415\n",
      "Epoch 8/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.8197 - accuracy: 0.7235\n",
      "Epoch 9/19\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.6410 - accuracy: 0.7890\n",
      "Epoch 10/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.4676 - accuracy: 0.8440\n",
      "Epoch 11/19\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.3462 - accuracy: 0.8800\n",
      "Epoch 12/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.2904 - accuracy: 0.9010\n",
      "Epoch 13/19\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.2064 - accuracy: 0.9320\n",
      "Epoch 14/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1400 - accuracy: 0.9555\n",
      "Epoch 15/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1774 - accuracy: 0.9435\n",
      "Epoch 16/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1152 - accuracy: 0.9635\n",
      "Epoch 17/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1149 - accuracy: 0.9630\n",
      "Epoch 18/19\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.1068 - accuracy: 0.9630\n",
      "Epoch 19/19\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1237 - accuracy: 0.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1519a7d50>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_2000label.fit(Xtrain_2000, Ttrain_2000, batch_size=64, epochs=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26032/26032 [==============================] - 49s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "loss_2000label, accuracy_2000label = cnn_model_2000label.evaluate(Xtest_2000, Ttest_2000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:3.8115 accuracy:0.5602\n"
     ]
    }
   ],
   "source": [
    "print('loss:%.4f accuracy:%.4f' %(loss_2000label, accuracy_2000label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 12.0229 - accuracy: 0.1225\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/2\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 9.8954 - accuracy: 0.1565\n",
      "Epoch 2/2\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2976 - accuracy: 0.2040\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/3\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 12.0349 - accuracy: 0.1390\n",
      "Epoch 2/3\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2992 - accuracy: 0.2090\n",
      "Epoch 3/3\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.1647 - accuracy: 0.2560\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/4\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 12.3505 - accuracy: 0.1115\n",
      "Epoch 2/4\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2834 - accuracy: 0.2125\n",
      "Epoch 3/4\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2059 - accuracy: 0.2115\n",
      "Epoch 4/4\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0586 - accuracy: 0.2900\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 14.1126 - accuracy: 0.1580\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2946 - accuracy: 0.2040\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2254 - accuracy: 0.1990\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0983 - accuracy: 0.2595\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8037 - accuracy: 0.4015\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/6\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 11.4575 - accuracy: 0.1445\n",
      "Epoch 2/6\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2606 - accuracy: 0.2070\n",
      "Epoch 3/6\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2341 - accuracy: 0.2090\n",
      "Epoch 4/6\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.1993 - accuracy: 0.2230\n",
      "Epoch 5/6\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0841 - accuracy: 0.2805\n",
      "Epoch 6/6\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7574 - accuracy: 0.4065\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/7\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 9.7419 - accuracy: 0.1295\n",
      "Epoch 2/7\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2809 - accuracy: 0.2035\n",
      "Epoch 3/7\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2037 - accuracy: 0.2260\n",
      "Epoch 4/7\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.9682 - accuracy: 0.3415\n",
      "Epoch 5/7\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.5809 - accuracy: 0.4740\n",
      "Epoch 6/7\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.2058 - accuracy: 0.6035\n",
      "Epoch 7/7\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.9350 - accuracy: 0.6865\n",
      "26032/26032 [==============================] - 49s 2ms/step\n",
      "Epoch 1/8\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 10.3143 - accuracy: 0.1215\n",
      "Epoch 2/8\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2810 - accuracy: 0.1920\n",
      "Epoch 3/8\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2244 - accuracy: 0.2020\n",
      "Epoch 4/8\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.1170 - accuracy: 0.2480\n",
      "Epoch 5/8\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.8029 - accuracy: 0.3825\n",
      "Epoch 6/8\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.3812 - accuracy: 0.5315\n",
      "Epoch 7/8\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.9113 - accuracy: 0.7015\n",
      "Epoch 8/8\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.6309 - accuracy: 0.7885\n",
      "26032/26032 [==============================] - 50s 2ms/step\n",
      "Epoch 1/9\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 10.5246 - accuracy: 0.1350\n",
      "Epoch 2/9\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.3079 - accuracy: 0.2055\n",
      "Epoch 3/9\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.1939 - accuracy: 0.2175\n",
      "Epoch 4/9\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8770 - accuracy: 0.3675\n",
      "Epoch 5/9\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.3350 - accuracy: 0.5560\n",
      "Epoch 6/9\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.9586 - accuracy: 0.6840\n",
      "Epoch 7/9\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.7190 - accuracy: 0.7645\n",
      "Epoch 8/9\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5431 - accuracy: 0.8210\n",
      "Epoch 9/9\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3588 - accuracy: 0.8805\n",
      "26032/26032 [==============================] - 49s 2ms/step\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 9.1614 - accuracy: 0.1595\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2966 - accuracy: 0.2045\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2424 - accuracy: 0.2035\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.1738 - accuracy: 0.2190\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.9237 - accuracy: 0.3290\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.5339 - accuracy: 0.4915\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.1320 - accuracy: 0.6200\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.8608 - accuracy: 0.7115\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 0.6369 - accuracy: 0.7825\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5170 - accuracy: 0.8295\n",
      "26032/26032 [==============================] - 46s 2ms/step\n",
      "Epoch 1/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 9.5963 - accuracy: 0.1490\n",
      "Epoch 2/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2956 - accuracy: 0.2025\n",
      "Epoch 3/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2381 - accuracy: 0.2025\n",
      "Epoch 4/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2206 - accuracy: 0.2025\n",
      "Epoch 5/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.1728 - accuracy: 0.2035\n",
      "Epoch 6/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0560 - accuracy: 0.2660\n",
      "Epoch 7/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.5495 - accuracy: 0.4690\n",
      "Epoch 8/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.1576 - accuracy: 0.6020\n",
      "Epoch 9/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.8775 - accuracy: 0.7135\n",
      "Epoch 10/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.6075 - accuracy: 0.7905\n",
      "Epoch 11/11\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5249 - accuracy: 0.8305\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/12\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 11.1800 - accuracy: 0.1265\n",
      "Epoch 2/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.3044 - accuracy: 0.2015\n",
      "Epoch 3/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2411 - accuracy: 0.2040\n",
      "Epoch 4/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.1673 - accuracy: 0.2325\n",
      "Epoch 5/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7981 - accuracy: 0.3750\n",
      "Epoch 6/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.3359 - accuracy: 0.5535\n",
      "Epoch 7/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.0142 - accuracy: 0.6715\n",
      "Epoch 8/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.7442 - accuracy: 0.7595\n",
      "Epoch 9/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5297 - accuracy: 0.8185\n",
      "Epoch 10/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4678 - accuracy: 0.8455\n",
      "Epoch 11/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2943 - accuracy: 0.9105\n",
      "Epoch 12/12\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2380 - accuracy: 0.9195\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/13\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 13.7979 - accuracy: 0.1365\n",
      "Epoch 2/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2891 - accuracy: 0.2040\n",
      "Epoch 3/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2562 - accuracy: 0.1940\n",
      "Epoch 4/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2026 - accuracy: 0.2170\n",
      "Epoch 5/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0505 - accuracy: 0.2865\n",
      "Epoch 6/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7820 - accuracy: 0.4025\n",
      "Epoch 7/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.4710 - accuracy: 0.5065\n",
      "Epoch 8/13\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.1838 - accuracy: 0.5940\n",
      "Epoch 9/13\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.9907 - accuracy: 0.6675\n",
      "Epoch 10/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.7286 - accuracy: 0.7595\n",
      "Epoch 11/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5387 - accuracy: 0.8255\n",
      "Epoch 12/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4922 - accuracy: 0.8370\n",
      "Epoch 13/13\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3182 - accuracy: 0.8890\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/14\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 10.8330 - accuracy: 0.1400\n",
      "Epoch 2/14\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2887 - accuracy: 0.2045\n",
      "Epoch 3/14\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.1171 - accuracy: 0.2545\n",
      "Epoch 4/14\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.6354 - accuracy: 0.4555\n",
      "Epoch 5/14\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.2638 - accuracy: 0.5775\n",
      "Epoch 6/14\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.0058 - accuracy: 0.6795\n",
      "Epoch 7/14\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.8179 - accuracy: 0.7340\n",
      "Epoch 8/14\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.6603 - accuracy: 0.7915\n",
      "Epoch 9/14\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.5656 - accuracy: 0.8185\n",
      "Epoch 10/14\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4106 - accuracy: 0.8645\n",
      "Epoch 11/14\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3295 - accuracy: 0.8890\n",
      "Epoch 12/14\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2649 - accuracy: 0.9085\n",
      "Epoch 13/14\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.2306 - accuracy: 0.9235\n",
      "Epoch 14/14\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2020 - accuracy: 0.9360\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 12.6387 - accuracy: 0.1155\n",
      "Epoch 2/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.3058 - accuracy: 0.2035\n",
      "Epoch 3/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2221 - accuracy: 0.2130\n",
      "Epoch 4/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0423 - accuracy: 0.2900\n",
      "Epoch 5/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.7508 - accuracy: 0.3940\n",
      "Epoch 6/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.3850 - accuracy: 0.5430\n",
      "Epoch 7/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.1236 - accuracy: 0.6265\n",
      "Epoch 8/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.7788 - accuracy: 0.7395\n",
      "Epoch 9/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.6284 - accuracy: 0.7870\n",
      "Epoch 10/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4906 - accuracy: 0.8335\n",
      "Epoch 11/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3429 - accuracy: 0.8820\n",
      "Epoch 12/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2623 - accuracy: 0.9200\n",
      "Epoch 13/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1928 - accuracy: 0.9345\n",
      "Epoch 14/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1764 - accuracy: 0.9430\n",
      "Epoch 15/15\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2348 - accuracy: 0.9265\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/16\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 13.8925 - accuracy: 0.1555\n",
      "Epoch 2/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2792 - accuracy: 0.1955\n",
      "Epoch 3/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2199 - accuracy: 0.2120\n",
      "Epoch 4/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.1614 - accuracy: 0.2390\n",
      "Epoch 5/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.9655 - accuracy: 0.3070\n",
      "Epoch 6/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.6190 - accuracy: 0.4615\n",
      "Epoch 7/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.2865 - accuracy: 0.5805\n",
      "Epoch 8/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.9942 - accuracy: 0.6710\n",
      "Epoch 9/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.6805 - accuracy: 0.7690\n",
      "Epoch 10/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5014 - accuracy: 0.8435\n",
      "Epoch 11/16\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.4147 - accuracy: 0.8700\n",
      "Epoch 12/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2703 - accuracy: 0.9105\n",
      "Epoch 13/16\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.2598 - accuracy: 0.9140\n",
      "Epoch 14/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1936 - accuracy: 0.9320\n",
      "Epoch 15/16\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1396 - accuracy: 0.9565\n",
      "Epoch 16/16\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1388 - accuracy: 0.9595\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/17\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 14.0457 - accuracy: 0.1130\n",
      "Epoch 2/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2905 - accuracy: 0.2030\n",
      "Epoch 3/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.1968 - accuracy: 0.2185\n",
      "Epoch 4/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.9652 - accuracy: 0.3275\n",
      "Epoch 5/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.6092 - accuracy: 0.4500\n",
      "Epoch 6/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.2850 - accuracy: 0.5760\n",
      "Epoch 7/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.0246 - accuracy: 0.6590\n",
      "Epoch 8/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.7768 - accuracy: 0.7490\n",
      "Epoch 9/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.6146 - accuracy: 0.7970\n",
      "Epoch 10/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4684 - accuracy: 0.8420\n",
      "Epoch 11/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3690 - accuracy: 0.8765\n",
      "Epoch 12/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2786 - accuracy: 0.9115\n",
      "Epoch 13/17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2280 - accuracy: 0.9245\n",
      "Epoch 14/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2263 - accuracy: 0.9250\n",
      "Epoch 15/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1760 - accuracy: 0.9450\n",
      "Epoch 16/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1801 - accuracy: 0.9505\n",
      "Epoch 17/17\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1360 - accuracy: 0.9540\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/18\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 12.0327 - accuracy: 0.1245\n",
      "Epoch 2/18\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.3024 - accuracy: 0.1880\n",
      "Epoch 3/18\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2231 - accuracy: 0.2070\n",
      "Epoch 4/18\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.0943 - accuracy: 0.2590\n",
      "Epoch 5/18\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.7566 - accuracy: 0.4105\n",
      "Epoch 6/18\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.3677 - accuracy: 0.5440\n",
      "Epoch 7/18\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.9457 - accuracy: 0.6830\n",
      "Epoch 8/18\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.6993 - accuracy: 0.7635\n",
      "Epoch 9/18\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5194 - accuracy: 0.8365\n",
      "Epoch 10/18\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 0.3864 - accuracy: 0.8710\n",
      "Epoch 11/18\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2650 - accuracy: 0.9145\n",
      "Epoch 12/18\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2268 - accuracy: 0.9285\n",
      "Epoch 13/18\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1823 - accuracy: 0.9430\n",
      "Epoch 14/18\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1511 - accuracy: 0.9495\n",
      "Epoch 15/18\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.2369 - accuracy: 0.9285\n",
      "Epoch 16/18\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1226 - accuracy: 0.9580\n",
      "Epoch 17/18\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0853 - accuracy: 0.9740\n",
      "Epoch 18/18\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0731 - accuracy: 0.9780\n",
      "26032/26032 [==============================] - 51s 2ms/step\n",
      "Epoch 1/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 9.3961 - accuracy: 0.1420\n",
      "Epoch 2/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2967 - accuracy: 0.2030\n",
      "Epoch 3/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2448 - accuracy: 0.2040\n",
      "Epoch 4/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2138 - accuracy: 0.2065\n",
      "Epoch 5/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0854 - accuracy: 0.2640\n",
      "Epoch 6/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.6884 - accuracy: 0.4295\n",
      "Epoch 7/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.2527 - accuracy: 0.5960\n",
      "Epoch 8/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.9533 - accuracy: 0.6860\n",
      "Epoch 9/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.6661 - accuracy: 0.7850\n",
      "Epoch 10/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5022 - accuracy: 0.8330\n",
      "Epoch 11/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.3533 - accuracy: 0.8910\n",
      "Epoch 12/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3434 - accuracy: 0.8910\n",
      "Epoch 13/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2535 - accuracy: 0.9155\n",
      "Epoch 14/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1934 - accuracy: 0.9365\n",
      "Epoch 15/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1400 - accuracy: 0.9520\n",
      "Epoch 16/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1026 - accuracy: 0.9690\n",
      "Epoch 17/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1275 - accuracy: 0.9620\n",
      "Epoch 18/19\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1136 - accuracy: 0.9670\n",
      "Epoch 19/19\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1256 - accuracy: 0.9640\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 10.6330 - accuracy: 0.1165\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2825 - accuracy: 0.1910\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2357 - accuracy: 0.2055\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2132 - accuracy: 0.2175\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.0355 - accuracy: 0.2955\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.5506 - accuracy: 0.4790\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.1861 - accuracy: 0.6105\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.7870 - accuracy: 0.7410\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.5876 - accuracy: 0.8010\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.4598 - accuracy: 0.8405\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3699 - accuracy: 0.8690\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.2560 - accuracy: 0.9100\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.2217 - accuracy: 0.9305\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.2073 - accuracy: 0.9305\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1352 - accuracy: 0.9550\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1482 - accuracy: 0.9455\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 0.0972 - accuracy: 0.9655\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0842 - accuracy: 0.9740\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0914 - accuracy: 0.9725\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0965 - accuracy: 0.9700\n",
      "26032/26032 [==============================] - 50s 2ms/step\n",
      "Epoch 1/21\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 10.5649 - accuracy: 0.1510\n",
      "Epoch 2/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2853 - accuracy: 0.1945\n",
      "Epoch 3/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.1909 - accuracy: 0.2355\n",
      "Epoch 4/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.8858 - accuracy: 0.3535\n",
      "Epoch 5/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.4863 - accuracy: 0.5010\n",
      "Epoch 6/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.2276 - accuracy: 0.5870\n",
      "Epoch 7/21\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.9598 - accuracy: 0.6805\n",
      "Epoch 8/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.7726 - accuracy: 0.7440\n",
      "Epoch 9/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5781 - accuracy: 0.8050\n",
      "Epoch 10/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4358 - accuracy: 0.8510\n",
      "Epoch 11/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3827 - accuracy: 0.8655\n",
      "Epoch 12/21\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.2855 - accuracy: 0.9120\n",
      "Epoch 13/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2102 - accuracy: 0.9340\n",
      "Epoch 14/21\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1397 - accuracy: 0.9535\n",
      "Epoch 15/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1391 - accuracy: 0.9510\n",
      "Epoch 16/21\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1289 - accuracy: 0.9585\n",
      "Epoch 17/21\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0825 - accuracy: 0.9725\n",
      "Epoch 18/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0603 - accuracy: 0.9795\n",
      "Epoch 19/21\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1431 - accuracy: 0.9605\n",
      "Epoch 20/21\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1479 - accuracy: 0.9525\n",
      "Epoch 21/21\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0927 - accuracy: 0.9710\n",
      "26032/26032 [==============================] - 49s 2ms/step\n",
      "Epoch 1/22\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 11.5979 - accuracy: 0.1285\n",
      "Epoch 2/22\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2913 - accuracy: 0.2045\n",
      "Epoch 3/22\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 2.2436 - accuracy: 0.2045\n",
      "Epoch 4/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2267 - accuracy: 0.2070\n",
      "Epoch 5/22\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.1803 - accuracy: 0.2285\n",
      "Epoch 6/22\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.0903 - accuracy: 0.2650\n",
      "Epoch 7/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.9052 - accuracy: 0.3405\n",
      "Epoch 8/22\n",
      "2000/2000 [==============================] - 13s 7ms/step - loss: 1.4971 - accuracy: 0.4980\n",
      "Epoch 9/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.0856 - accuracy: 0.6285\n",
      "Epoch 10/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.8383 - accuracy: 0.7275\n",
      "Epoch 11/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.6366 - accuracy: 0.7885\n",
      "Epoch 12/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4725 - accuracy: 0.8430\n",
      "Epoch 13/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3739 - accuracy: 0.8735\n",
      "Epoch 14/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2711 - accuracy: 0.9125\n",
      "Epoch 15/22\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.2210 - accuracy: 0.9275\n",
      "Epoch 16/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1669 - accuracy: 0.9440\n",
      "Epoch 17/22\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1377 - accuracy: 0.9540\n",
      "Epoch 18/22\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0898 - accuracy: 0.9695\n",
      "Epoch 19/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0808 - accuracy: 0.9715\n",
      "Epoch 20/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1934 - accuracy: 0.9415\n",
      "Epoch 21/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1328 - accuracy: 0.9570\n",
      "Epoch 22/22\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0960 - accuracy: 0.9725\n",
      "26032/26032 [==============================] - 51s 2ms/step\n",
      "Epoch 1/23\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 12.1708 - accuracy: 0.1625\n",
      "Epoch 2/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.3236 - accuracy: 0.1965\n",
      "Epoch 3/23\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2495 - accuracy: 0.2040\n",
      "Epoch 4/23\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2250 - accuracy: 0.2055\n",
      "Epoch 5/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.1490 - accuracy: 0.2495\n",
      "Epoch 6/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.8355 - accuracy: 0.3835\n",
      "Epoch 7/23\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.4268 - accuracy: 0.5260\n",
      "Epoch 8/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.0010 - accuracy: 0.6475\n",
      "Epoch 9/23\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.8349 - accuracy: 0.7235\n",
      "Epoch 10/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.5672 - accuracy: 0.8085\n",
      "Epoch 11/23\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3775 - accuracy: 0.8795\n",
      "Epoch 12/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.3190 - accuracy: 0.8970\n",
      "Epoch 13/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.2271 - accuracy: 0.9180\n",
      "Epoch 14/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1860 - accuracy: 0.9370\n",
      "Epoch 15/23\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1318 - accuracy: 0.9545\n",
      "Epoch 16/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1289 - accuracy: 0.9570\n",
      "Epoch 17/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1052 - accuracy: 0.9645\n",
      "Epoch 18/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1226 - accuracy: 0.9645\n",
      "Epoch 19/23\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1008 - accuracy: 0.9665\n",
      "Epoch 20/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1419 - accuracy: 0.9610\n",
      "Epoch 21/23\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0671 - accuracy: 0.9790\n",
      "Epoch 22/23\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0280 - accuracy: 0.9915\n",
      "Epoch 23/23\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0475 - accuracy: 0.9865\n",
      "26032/26032 [==============================] - 49s 2ms/step\n",
      "Epoch 1/24\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 9.1736 - accuracy: 0.1675\n",
      "Epoch 2/24\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2686 - accuracy: 0.2035\n",
      "Epoch 3/24\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2216 - accuracy: 0.2040\n",
      "Epoch 4/24\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.0466 - accuracy: 0.2875\n",
      "Epoch 5/24\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.7409 - accuracy: 0.4115\n",
      "Epoch 6/24\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.3369 - accuracy: 0.5700\n",
      "Epoch 7/24\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 1.0217 - accuracy: 0.6655\n",
      "Epoch 8/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.7003 - accuracy: 0.7700\n",
      "Epoch 9/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5075 - accuracy: 0.8310\n",
      "Epoch 10/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4916 - accuracy: 0.8415\n",
      "Epoch 11/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3480 - accuracy: 0.8815\n",
      "Epoch 12/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2455 - accuracy: 0.9155\n",
      "Epoch 13/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1942 - accuracy: 0.9355\n",
      "Epoch 14/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2027 - accuracy: 0.9400\n",
      "Epoch 15/24\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1319 - accuracy: 0.9555\n",
      "Epoch 16/24\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.0969 - accuracy: 0.9715\n",
      "Epoch 17/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1342 - accuracy: 0.9610\n",
      "Epoch 18/24\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1009 - accuracy: 0.9725\n",
      "Epoch 19/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0947 - accuracy: 0.9675\n",
      "Epoch 20/24\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0726 - accuracy: 0.9765\n",
      "Epoch 21/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0411 - accuracy: 0.9855\n",
      "Epoch 22/24\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0931 - accuracy: 0.9755\n",
      "Epoch 23/24\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0545 - accuracy: 0.9825\n",
      "Epoch 24/24\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0578 - accuracy: 0.9815\n",
      "26032/26032 [==============================] - 50s 2ms/step\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 15s 8ms/step - loss: 11.2341 - accuracy: 0.1115\n",
      "Epoch 2/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2769 - accuracy: 0.1905\n",
      "Epoch 3/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.1211 - accuracy: 0.2670\n",
      "Epoch 4/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.7750 - accuracy: 0.4020\n",
      "Epoch 5/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.3984 - accuracy: 0.5300\n",
      "Epoch 6/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.0594 - accuracy: 0.6530\n",
      "Epoch 7/25\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.7855 - accuracy: 0.7400\n",
      "Epoch 8/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5704 - accuracy: 0.8055\n",
      "Epoch 9/25\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.4141 - accuracy: 0.8710\n",
      "Epoch 10/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3539 - accuracy: 0.8825\n",
      "Epoch 11/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2601 - accuracy: 0.9140\n",
      "Epoch 12/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1831 - accuracy: 0.9430\n",
      "Epoch 13/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1341 - accuracy: 0.9580\n",
      "Epoch 14/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1056 - accuracy: 0.9670\n",
      "Epoch 15/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0719 - accuracy: 0.9770\n",
      "Epoch 16/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0681 - accuracy: 0.9760\n",
      "Epoch 17/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0797 - accuracy: 0.9725\n",
      "Epoch 18/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1023 - accuracy: 0.9660\n",
      "Epoch 19/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0777 - accuracy: 0.9735\n",
      "Epoch 20/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0880 - accuracy: 0.9725\n",
      "Epoch 21/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1117 - accuracy: 0.9630\n",
      "Epoch 22/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0807 - accuracy: 0.9745\n",
      "Epoch 23/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0482 - accuracy: 0.9890\n",
      "Epoch 24/25\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0450 - accuracy: 0.9835\n",
      "Epoch 25/25\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0642 - accuracy: 0.9810\n",
      "26032/26032 [==============================] - 50s 2ms/step\n",
      "Epoch 1/26\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 13.9079 - accuracy: 0.1540\n",
      "Epoch 2/26\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.3068 - accuracy: 0.2040\n",
      "Epoch 3/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2389 - accuracy: 0.2040\n",
      "Epoch 4/26\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.1575 - accuracy: 0.2220\n",
      "Epoch 5/26\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.9172 - accuracy: 0.3390\n",
      "Epoch 6/26\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 1.5302 - accuracy: 0.4825\n",
      "Epoch 7/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.1398 - accuracy: 0.6135\n",
      "Epoch 8/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.9376 - accuracy: 0.6825\n",
      "Epoch 9/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.6731 - accuracy: 0.7855\n",
      "Epoch 10/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.5444 - accuracy: 0.8160\n",
      "Epoch 11/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4484 - accuracy: 0.8505\n",
      "Epoch 12/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3146 - accuracy: 0.8890\n",
      "Epoch 13/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2575 - accuracy: 0.9175\n",
      "Epoch 14/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2555 - accuracy: 0.9175\n",
      "Epoch 15/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1725 - accuracy: 0.9470\n",
      "Epoch 16/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1126 - accuracy: 0.9600\n",
      "Epoch 17/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1198 - accuracy: 0.9590\n",
      "Epoch 18/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0893 - accuracy: 0.9685\n",
      "Epoch 19/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0873 - accuracy: 0.9700\n",
      "Epoch 20/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0664 - accuracy: 0.9790\n",
      "Epoch 21/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0918 - accuracy: 0.9750\n",
      "Epoch 22/26\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0988 - accuracy: 0.9735\n",
      "Epoch 23/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0487 - accuracy: 0.9830\n",
      "Epoch 24/26\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0813 - accuracy: 0.9750\n",
      "Epoch 25/26\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1105 - accuracy: 0.9655\n",
      "Epoch 26/26\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0864 - accuracy: 0.9720\n",
      "26032/26032 [==============================] - 50s 2ms/step\n",
      "Epoch 1/27\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 10.3124 - accuracy: 0.1310\n",
      "Epoch 2/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.2916 - accuracy: 0.1980\n",
      "Epoch 3/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.1400 - accuracy: 0.2570\n",
      "Epoch 4/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 1.7206 - accuracy: 0.4070\n",
      "Epoch 5/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.3274 - accuracy: 0.5540\n",
      "Epoch 6/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 1.0005 - accuracy: 0.6650\n",
      "Epoch 7/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.7541 - accuracy: 0.7555\n",
      "Epoch 8/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.6545 - accuracy: 0.7875\n",
      "Epoch 9/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.4841 - accuracy: 0.8385\n",
      "Epoch 10/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.4086 - accuracy: 0.8655\n",
      "Epoch 11/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.3009 - accuracy: 0.8960\n",
      "Epoch 12/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1880 - accuracy: 0.9355\n",
      "Epoch 13/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1672 - accuracy: 0.9415\n",
      "Epoch 14/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1612 - accuracy: 0.9465\n",
      "Epoch 15/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1397 - accuracy: 0.9580\n",
      "Epoch 16/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1085 - accuracy: 0.9650\n",
      "Epoch 17/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0926 - accuracy: 0.9720\n",
      "Epoch 18/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0740 - accuracy: 0.9785\n",
      "Epoch 19/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0831 - accuracy: 0.9770\n",
      "Epoch 20/27\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.0550 - accuracy: 0.9810\n",
      "Epoch 21/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0406 - accuracy: 0.9855\n",
      "Epoch 22/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0502 - accuracy: 0.9850\n",
      "Epoch 23/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1367 - accuracy: 0.9605\n",
      "Epoch 24/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0658 - accuracy: 0.9815\n",
      "Epoch 25/27\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0663 - accuracy: 0.9790\n",
      "Epoch 26/27\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0667 - accuracy: 0.9790\n",
      "Epoch 27/27\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0733 - accuracy: 0.9755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26032/26032 [==============================] - 50s 2ms/step\n",
      "Epoch 1/28\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 12.3049 - accuracy: 0.1125\n",
      "Epoch 2/28\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 2.3103 - accuracy: 0.1955\n",
      "Epoch 3/28\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 2.2066 - accuracy: 0.2215\n",
      "Epoch 4/28\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.9759 - accuracy: 0.3340\n",
      "Epoch 5/28\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.6049 - accuracy: 0.4715\n",
      "Epoch 6/28\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 1.1926 - accuracy: 0.5945\n",
      "Epoch 7/28\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.9103 - accuracy: 0.7030\n",
      "Epoch 8/28\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.6700 - accuracy: 0.7740\n",
      "Epoch 9/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.5012 - accuracy: 0.8355\n",
      "Epoch 10/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.3461 - accuracy: 0.8875\n",
      "Epoch 11/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.2823 - accuracy: 0.9105\n",
      "Epoch 12/28\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.2053 - accuracy: 0.9340\n",
      "Epoch 13/28\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.2080 - accuracy: 0.9300\n",
      "Epoch 14/28\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1685 - accuracy: 0.9540\n",
      "Epoch 15/28\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.1097 - accuracy: 0.9605\n",
      "Epoch 16/28\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.0885 - accuracy: 0.9700\n",
      "Epoch 17/28\n",
      "2000/2000 [==============================] - 17s 8ms/step - loss: 0.0862 - accuracy: 0.9720\n",
      "Epoch 18/28\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0829 - accuracy: 0.9755\n",
      "Epoch 19/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0703 - accuracy: 0.9735\n",
      "Epoch 20/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0708 - accuracy: 0.9760\n",
      "Epoch 21/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1163 - accuracy: 0.9665\n",
      "Epoch 22/28\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.0647 - accuracy: 0.9835\n",
      "Epoch 23/28\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.0260 - accuracy: 0.9920\n",
      "Epoch 24/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0389 - accuracy: 0.9860\n",
      "Epoch 25/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1141 - accuracy: 0.9685\n",
      "Epoch 26/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0781 - accuracy: 0.9785\n",
      "Epoch 27/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0619 - accuracy: 0.9855\n",
      "Epoch 28/28\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0542 - accuracy: 0.9790\n",
      "26032/26032 [==============================] - 51s 2ms/step\n",
      "Epoch 1/29\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 11.4815 - accuracy: 0.1480\n",
      "Epoch 2/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.3286 - accuracy: 0.2035\n",
      "Epoch 3/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 2.2490 - accuracy: 0.2105\n",
      "Epoch 4/29\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 2.1383 - accuracy: 0.2510\n",
      "Epoch 5/29\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 1.8388 - accuracy: 0.3835\n",
      "Epoch 6/29\n",
      "2000/2000 [==============================] - 17s 9ms/step - loss: 1.4309 - accuracy: 0.5250\n",
      "Epoch 7/29\n",
      "2000/2000 [==============================] - 16s 8ms/step - loss: 0.9873 - accuracy: 0.6640\n",
      "Epoch 8/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.7512 - accuracy: 0.7525\n",
      "Epoch 9/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.5829 - accuracy: 0.8145\n",
      "Epoch 10/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.4239 - accuracy: 0.8545\n",
      "Epoch 11/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.2562 - accuracy: 0.9110\n",
      "Epoch 12/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.2759 - accuracy: 0.9115\n",
      "Epoch 13/29\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.2042 - accuracy: 0.9335\n",
      "Epoch 14/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1441 - accuracy: 0.9530\n",
      "Epoch 15/29\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1045 - accuracy: 0.9680\n",
      "Epoch 16/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1111 - accuracy: 0.9610\n",
      "Epoch 17/29\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.1413 - accuracy: 0.9590\n",
      "Epoch 18/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.1217 - accuracy: 0.9590\n",
      "Epoch 19/29\n",
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.0828 - accuracy: 0.9690\n",
      "Epoch 20/29\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.0557 - accuracy: 0.9845\n",
      "Epoch 21/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0678 - accuracy: 0.9785\n",
      "Epoch 22/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0444 - accuracy: 0.9855\n",
      "Epoch 23/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0488 - accuracy: 0.9860\n",
      "Epoch 24/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0632 - accuracy: 0.9800\n",
      "Epoch 25/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0922 - accuracy: 0.9685\n",
      "Epoch 26/29\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.0600 - accuracy: 0.9815\n",
      "Epoch 27/29\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.0666 - accuracy: 0.9815\n",
      "Epoch 28/29\n",
      "2000/2000 [==============================] - 15s 8ms/step - loss: 0.0793 - accuracy: 0.9800\n",
      "Epoch 29/29\n",
      "2000/2000 [==============================] - 15s 7ms/step - loss: 0.0463 - accuracy: 0.9840\n",
      "26032/26032 [==============================] - 51s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_model_2000label_acc = []\n",
    "for i in range(30):\n",
    "    cnn_model_2000label = build_model()\n",
    "    cnn_model_2000label.compile(optimizer=tensorflow.optimizers.Adam(),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    cnn_model_2000label.fit(Xtrain_2000, Ttrain_2000, batch_size=64, epochs=i)\n",
    "    loss_2000label, accuracy_2000label = cnn_model_2000label.evaluate(Xtest_2000, Ttest_2000, batch_size=64)\n",
    "    cnn_model_2000label_acc.append(accuracy_2000label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The less we use labeled data for training in CNN, the lower accuracy we get. Also, we will need more epoches to train networks. The predction result on testing dataset would fluctuate between 56% ~ 64%, depending on which training data subset we pick and different training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVHN prediction using CNN with 1000 labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3, 1000)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before start, import library and build_model() again if needed\n",
    "\n",
    "# Load the dataset, \n",
    "# take first ---1000--- labeled data samples for training\n",
    "train_contents = sio.loadmat('train_32_32.mat')\n",
    "test_contents = sio.loadmat('test_32_32.mat')\n",
    "Xtrain_1000 = train_contents['X'][:,:,:,0:1000].astype('float32')\n",
    "Ttrain_1000 = train_contents['y'][0:1000].astype('float32')\n",
    "Xtest_1000 = test_contents['X'].astype('float32')\n",
    "Ttest_1000 = test_contents['y'].astype('float32')\n",
    "Xtrain_1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_1000 = Xtrain_1000.transpose((3,0,1,2))\n",
    "Xtest_1000 = Xtest_1000.transpose((3,0,1,2))\n",
    "\n",
    "Xtrain_1000 = np.array(Xtrain_1000)\n",
    "Ttrain_1000 = np.array(Ttrain_1000)\n",
    "Xtest_1000 = np.array(Xtest_1000)\n",
    "Ttest_1000 = np.array(Ttest_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ttrain_1000 = Ttrain_1000.flatten()\n",
    "Ttest_1000 = Ttest_1000.flatten()\n",
    "Ttest_1000 = keras.utils.to_categorical(Ttest_1000)\n",
    "Ttrain_1000 = keras.utils.to_categorical(Ttrain_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 23.3092 - accuracy: 0.1300\n",
      "26032/26032 [==============================] - 49s 2ms/step\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 25.2384 - accuracy: 0.1410\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3804 - accuracy: 0.1350\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/3\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 29.3957 - accuracy: 0.0890\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.3935 - accuracy: 0.1730\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.2895 - accuracy: 0.1830\n",
      "26032/26032 [==============================] - 49s 2ms/step\n",
      "Epoch 1/4\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 20.0334 - accuracy: 0.1270\n",
      "Epoch 2/4\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3789 - accuracy: 0.2020\n",
      "Epoch 3/4\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3208 - accuracy: 0.2060\n",
      "Epoch 4/4\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.2502 - accuracy: 0.2110\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 25.4685 - accuracy: 0.1180\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.4058 - accuracy: 0.2020\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3289 - accuracy: 0.2070\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2674 - accuracy: 0.2080\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2422 - accuracy: 0.2090\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/6\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 22.8435 - accuracy: 0.1210\n",
      "Epoch 2/6\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3978 - accuracy: 0.2000\n",
      "Epoch 3/6\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3423 - accuracy: 0.2090\n",
      "Epoch 4/6\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2883 - accuracy: 0.2110\n",
      "Epoch 5/6\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2161 - accuracy: 0.2180\n",
      "Epoch 6/6\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1430 - accuracy: 0.2380\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/7\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 20.1155 - accuracy: 0.1450\n",
      "Epoch 2/7\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3676 - accuracy: 0.1640\n",
      "Epoch 3/7\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3005 - accuracy: 0.2090\n",
      "Epoch 4/7\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2288 - accuracy: 0.2060\n",
      "Epoch 5/7\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1303 - accuracy: 0.2410\n",
      "Epoch 6/7\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9744 - accuracy: 0.3090\n",
      "Epoch 7/7\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7265 - accuracy: 0.4100\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/8\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 19.7841 - accuracy: 0.1000\n",
      "Epoch 2/8\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3644 - accuracy: 0.2030\n",
      "Epoch 3/8\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2885 - accuracy: 0.2070\n",
      "Epoch 4/8\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2561 - accuracy: 0.2070\n",
      "Epoch 5/8\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2297 - accuracy: 0.2070\n",
      "Epoch 6/8\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2207 - accuracy: 0.2070\n",
      "Epoch 7/8\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2186 - accuracy: 0.2100\n",
      "Epoch 8/8\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1949 - accuracy: 0.2110\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/9\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 24.1032 - accuracy: 0.1140\n",
      "Epoch 2/9\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.4578 - accuracy: 0.2010\n",
      "Epoch 3/9\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3582 - accuracy: 0.2060\n",
      "Epoch 4/9\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3059 - accuracy: 0.2010\n",
      "Epoch 5/9\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2620 - accuracy: 0.2150\n",
      "Epoch 6/9\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2293 - accuracy: 0.2160\n",
      "Epoch 7/9\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1898 - accuracy: 0.2290\n",
      "Epoch 8/9\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1367 - accuracy: 0.2320\n",
      "Epoch 9/9\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0335 - accuracy: 0.2840\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 25.8475 - accuracy: 0.1060\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.4140 - accuracy: 0.1990\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3484 - accuracy: 0.2020\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2748 - accuracy: 0.2060\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1767 - accuracy: 0.2340\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.8985 - accuracy: 0.3690\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.4952 - accuracy: 0.4990\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.1962 - accuracy: 0.6020\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.9986 - accuracy: 0.6820\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7348 - accuracy: 0.7470\n",
      "26032/26032 [==============================] - 49s 2ms/step\n",
      "Epoch 1/11\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 18.5544 - accuracy: 0.1320\n",
      "Epoch 2/11\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.3964 - accuracy: 0.0980\n",
      "Epoch 3/11\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3094 - accuracy: 0.1670\n",
      "Epoch 4/11\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.2670 - accuracy: 0.2070\n",
      "Epoch 5/11\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.1854 - accuracy: 0.2250\n",
      "Epoch 6/11\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.1010 - accuracy: 0.2590\n",
      "Epoch 7/11\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.8946 - accuracy: 0.3230\n",
      "Epoch 8/11\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.6623 - accuracy: 0.4300\n",
      "Epoch 9/11\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.3884 - accuracy: 0.5140\n",
      "Epoch 10/11\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.1496 - accuracy: 0.6240\n",
      "Epoch 11/11\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.8833 - accuracy: 0.7040\n",
      "26032/26032 [==============================] - 50s 2ms/step\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 20.5385 - accuracy: 0.1200\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3546 - accuracy: 0.1700\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2949 - accuracy: 0.2070\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2529 - accuracy: 0.2070\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2350 - accuracy: 0.2120\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2148 - accuracy: 0.2160\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1551 - accuracy: 0.2250\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0715 - accuracy: 0.2700\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9018 - accuracy: 0.3460\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.6306 - accuracy: 0.4130\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.3481 - accuracy: 0.5150\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.1539 - accuracy: 0.5990\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/13\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 28.8030 - accuracy: 0.0870\n",
      "Epoch 2/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.4374 - accuracy: 0.1990\n",
      "Epoch 3/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3108 - accuracy: 0.2070\n",
      "Epoch 4/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2774 - accuracy: 0.2080\n",
      "Epoch 5/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2363 - accuracy: 0.2060\n",
      "Epoch 6/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2247 - accuracy: 0.2030\n",
      "Epoch 7/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2067 - accuracy: 0.2180\n",
      "Epoch 8/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1759 - accuracy: 0.2210\n",
      "Epoch 9/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0805 - accuracy: 0.2490\n",
      "Epoch 10/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.8757 - accuracy: 0.3400\n",
      "Epoch 11/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7075 - accuracy: 0.4270\n",
      "Epoch 12/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.4859 - accuracy: 0.4980\n",
      "Epoch 13/13\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2595 - accuracy: 0.5690\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/14\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 28.2647 - accuracy: 0.0960\n",
      "Epoch 2/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3723 - accuracy: 0.1970\n",
      "Epoch 3/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3244 - accuracy: 0.2070\n",
      "Epoch 4/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2828 - accuracy: 0.2080\n",
      "Epoch 5/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2104 - accuracy: 0.2260\n",
      "Epoch 6/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9928 - accuracy: 0.3280\n",
      "Epoch 7/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.6231 - accuracy: 0.4540\n",
      "Epoch 8/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2783 - accuracy: 0.5760\n",
      "Epoch 9/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0041 - accuracy: 0.6590\n",
      "Epoch 10/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.8540 - accuracy: 0.7150\n",
      "Epoch 11/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7202 - accuracy: 0.7610\n",
      "Epoch 12/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4493 - accuracy: 0.8550\n",
      "Epoch 13/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3791 - accuracy: 0.8730\n",
      "Epoch 14/14\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2952 - accuracy: 0.9040\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 22.7345 - accuracy: 0.1390\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3541 - accuracy: 0.1460\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3044 - accuracy: 0.1920\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2519 - accuracy: 0.2160\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2083 - accuracy: 0.2330\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1517 - accuracy: 0.2580\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9671 - accuracy: 0.3180\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7763 - accuracy: 0.4080\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.5184 - accuracy: 0.4870\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.3135 - accuracy: 0.5670\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0606 - accuracy: 0.6380\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.8394 - accuracy: 0.7220\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7317 - accuracy: 0.7570\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6045 - accuracy: 0.7970\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4584 - accuracy: 0.8540\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 22.2738 - accuracy: 0.1070\n",
      "Epoch 2/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3649 - accuracy: 0.2020\n",
      "Epoch 3/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2950 - accuracy: 0.2070\n",
      "Epoch 4/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2704 - accuracy: 0.1900\n",
      "Epoch 5/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2201 - accuracy: 0.2110\n",
      "Epoch 6/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1864 - accuracy: 0.2250\n",
      "Epoch 7/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1117 - accuracy: 0.2490\n",
      "Epoch 8/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9378 - accuracy: 0.3320\n",
      "Epoch 9/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7417 - accuracy: 0.3890\n",
      "Epoch 10/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.5277 - accuracy: 0.4590\n",
      "Epoch 11/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.3461 - accuracy: 0.5560\n",
      "Epoch 12/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.1400 - accuracy: 0.6310\n",
      "Epoch 13/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.9187 - accuracy: 0.6950\n",
      "Epoch 14/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7445 - accuracy: 0.7530\n",
      "Epoch 15/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.5212 - accuracy: 0.8220\n",
      "Epoch 16/16\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4200 - accuracy: 0.8560\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/17\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 21.5190 - accuracy: 0.1330\n",
      "Epoch 2/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3680 - accuracy: 0.1040\n",
      "Epoch 3/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3209 - accuracy: 0.1540\n",
      "Epoch 4/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2554 - accuracy: 0.2250\n",
      "Epoch 5/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1680 - accuracy: 0.2460\n",
      "Epoch 6/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9737 - accuracy: 0.3230\n",
      "Epoch 7/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7224 - accuracy: 0.4150\n",
      "Epoch 8/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.4917 - accuracy: 0.4960\n",
      "Epoch 9/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.1278 - accuracy: 0.6300\n",
      "Epoch 10/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.9029 - accuracy: 0.6960\n",
      "Epoch 11/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6610 - accuracy: 0.7770\n",
      "Epoch 12/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.5074 - accuracy: 0.8310\n",
      "Epoch 13/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3694 - accuracy: 0.8820\n",
      "Epoch 14/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3174 - accuracy: 0.8940\n",
      "Epoch 15/17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2327 - accuracy: 0.9240\n",
      "Epoch 16/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1737 - accuracy: 0.9490\n",
      "Epoch 17/17\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1129 - accuracy: 0.9660\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/18\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 29.8642 - accuracy: 0.0980\n",
      "Epoch 2/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3920 - accuracy: 0.1680\n",
      "Epoch 3/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3267 - accuracy: 0.2070\n",
      "Epoch 4/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2608 - accuracy: 0.2050\n",
      "Epoch 5/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1906 - accuracy: 0.2190\n",
      "Epoch 6/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0725 - accuracy: 0.2710\n",
      "Epoch 7/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.8762 - accuracy: 0.3520\n",
      "Epoch 8/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.6651 - accuracy: 0.4280\n",
      "Epoch 9/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.4506 - accuracy: 0.5020\n",
      "Epoch 10/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2259 - accuracy: 0.5840\n",
      "Epoch 11/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0469 - accuracy: 0.6560\n",
      "Epoch 12/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.8460 - accuracy: 0.7090\n",
      "Epoch 13/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7014 - accuracy: 0.7580\n",
      "Epoch 14/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.5185 - accuracy: 0.8420\n",
      "Epoch 15/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4034 - accuracy: 0.8750\n",
      "Epoch 16/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3523 - accuracy: 0.8990\n",
      "Epoch 17/18\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2855 - accuracy: 0.9120\n",
      "Epoch 18/18\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2375 - accuracy: 0.9230\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/19\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 15.4461 - accuracy: 0.1060\n",
      "Epoch 2/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3500 - accuracy: 0.1320\n",
      "Epoch 3/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2767 - accuracy: 0.2090\n",
      "Epoch 4/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2367 - accuracy: 0.2110\n",
      "Epoch 5/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1610 - accuracy: 0.2560\n",
      "Epoch 6/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9363 - accuracy: 0.3340\n",
      "Epoch 7/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.6277 - accuracy: 0.4530\n",
      "Epoch 8/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2528 - accuracy: 0.5660\n",
      "Epoch 9/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.9233 - accuracy: 0.6870\n",
      "Epoch 10/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7134 - accuracy: 0.7690\n",
      "Epoch 11/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4816 - accuracy: 0.8420\n",
      "Epoch 12/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3741 - accuracy: 0.8750\n",
      "Epoch 13/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2634 - accuracy: 0.9100\n",
      "Epoch 14/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1983 - accuracy: 0.9310\n",
      "Epoch 15/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1761 - accuracy: 0.9480\n",
      "Epoch 16/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2175 - accuracy: 0.9390\n",
      "Epoch 17/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1202 - accuracy: 0.9620\n",
      "Epoch 18/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0862 - accuracy: 0.9750\n",
      "Epoch 19/19\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1078 - accuracy: 0.9660\n",
      "26032/26032 [==============================] - 46s 2ms/step\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 24.4085 - accuracy: 0.1220\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3456 - accuracy: 0.2050\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3168 - accuracy: 0.2080\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2582 - accuracy: 0.2080\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2208 - accuracy: 0.2280\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1605 - accuracy: 0.2490\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0498 - accuracy: 0.2950\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.8019 - accuracy: 0.3950\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.5783 - accuracy: 0.4430\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2303 - accuracy: 0.5900\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.9596 - accuracy: 0.6830\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7770 - accuracy: 0.7350\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.5948 - accuracy: 0.8050\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4677 - accuracy: 0.8390\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3385 - accuracy: 0.8810\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2934 - accuracy: 0.9020\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2443 - accuracy: 0.9260\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2241 - accuracy: 0.9290\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1853 - accuracy: 0.9470\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1624 - accuracy: 0.9550\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/21\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 26.0496 - accuracy: 0.1460\n",
      "Epoch 2/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3838 - accuracy: 0.0930\n",
      "Epoch 3/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3283 - accuracy: 0.1430\n",
      "Epoch 4/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2751 - accuracy: 0.2000\n",
      "Epoch 5/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2286 - accuracy: 0.2070\n",
      "Epoch 6/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1961 - accuracy: 0.2130\n",
      "Epoch 7/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1100 - accuracy: 0.2470\n",
      "Epoch 8/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9435 - accuracy: 0.2950\n",
      "Epoch 9/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.6287 - accuracy: 0.4340\n",
      "Epoch 10/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2897 - accuracy: 0.5570\n",
      "Epoch 11/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0744 - accuracy: 0.6270\n",
      "Epoch 12/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.9007 - accuracy: 0.7040\n",
      "Epoch 13/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6796 - accuracy: 0.7680\n",
      "Epoch 14/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4821 - accuracy: 0.8380\n",
      "Epoch 15/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4360 - accuracy: 0.8570\n",
      "Epoch 16/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3824 - accuracy: 0.8850\n",
      "Epoch 17/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2647 - accuracy: 0.9060\n",
      "Epoch 18/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1821 - accuracy: 0.9400\n",
      "Epoch 19/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2019 - accuracy: 0.9380\n",
      "Epoch 20/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1623 - accuracy: 0.9550\n",
      "Epoch 21/21\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1651 - accuracy: 0.9470\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 15.8398 - accuracy: 0.1200\n",
      "Epoch 2/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3961 - accuracy: 0.1840\n",
      "Epoch 3/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2810 - accuracy: 0.2000\n",
      "Epoch 4/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2394 - accuracy: 0.2080\n",
      "Epoch 5/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1859 - accuracy: 0.2370\n",
      "Epoch 6/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0245 - accuracy: 0.2920\n",
      "Epoch 7/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7186 - accuracy: 0.4100\n",
      "Epoch 8/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.4278 - accuracy: 0.5190\n",
      "Epoch 9/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.1479 - accuracy: 0.6220\n",
      "Epoch 10/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.9581 - accuracy: 0.6810\n",
      "Epoch 11/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6608 - accuracy: 0.7910\n",
      "Epoch 12/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.5068 - accuracy: 0.8380\n",
      "Epoch 13/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3589 - accuracy: 0.8740\n",
      "Epoch 14/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2816 - accuracy: 0.9260\n",
      "Epoch 15/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1889 - accuracy: 0.9410\n",
      "Epoch 16/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1808 - accuracy: 0.9410\n",
      "Epoch 17/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1704 - accuracy: 0.9480\n",
      "Epoch 18/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1149 - accuracy: 0.9690\n",
      "Epoch 19/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1148 - accuracy: 0.9650\n",
      "Epoch 20/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0893 - accuracy: 0.9670\n",
      "Epoch 21/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0582 - accuracy: 0.9780\n",
      "Epoch 22/22\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0398 - accuracy: 0.9900\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 32.8483 - accuracy: 0.1100\n",
      "Epoch 2/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.4964 - accuracy: 0.1050\n",
      "Epoch 3/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3597 - accuracy: 0.1560\n",
      "Epoch 4/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2950 - accuracy: 0.2150\n",
      "Epoch 5/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2591 - accuracy: 0.2140\n",
      "Epoch 6/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2231 - accuracy: 0.2200\n",
      "Epoch 7/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1860 - accuracy: 0.2340\n",
      "Epoch 8/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1272 - accuracy: 0.2500\n",
      "Epoch 9/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9941 - accuracy: 0.3020\n",
      "Epoch 10/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7730 - accuracy: 0.3770\n",
      "Epoch 11/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.5523 - accuracy: 0.4540\n",
      "Epoch 12/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2983 - accuracy: 0.5640\n",
      "Epoch 13/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0531 - accuracy: 0.6340\n",
      "Epoch 14/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.9132 - accuracy: 0.7020\n",
      "Epoch 15/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7017 - accuracy: 0.7610\n",
      "Epoch 16/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.5819 - accuracy: 0.8110\n",
      "Epoch 17/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4260 - accuracy: 0.8560\n",
      "Epoch 18/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3567 - accuracy: 0.8810\n",
      "Epoch 19/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2743 - accuracy: 0.9150\n",
      "Epoch 20/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2651 - accuracy: 0.9210\n",
      "Epoch 21/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1799 - accuracy: 0.9470\n",
      "Epoch 22/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1815 - accuracy: 0.9460\n",
      "Epoch 23/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1315 - accuracy: 0.9560\n",
      "26032/26032 [==============================] - 46s 2ms/step\n",
      "Epoch 1/24\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 21.1853 - accuracy: 0.1340\n",
      "Epoch 2/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.4166 - accuracy: 0.1800\n",
      "Epoch 3/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2880 - accuracy: 0.2000\n",
      "Epoch 4/24\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.2197 - accuracy: 0.2240\n",
      "Epoch 5/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1208 - accuracy: 0.2480\n",
      "Epoch 6/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.9695 - accuracy: 0.3020\n",
      "Epoch 7/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7601 - accuracy: 0.4020\n",
      "Epoch 8/24\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.5060 - accuracy: 0.4830\n",
      "Epoch 9/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2452 - accuracy: 0.5890\n",
      "Epoch 10/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.9571 - accuracy: 0.6740\n",
      "Epoch 11/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7020 - accuracy: 0.7600\n",
      "Epoch 12/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4693 - accuracy: 0.8290\n",
      "Epoch 13/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3923 - accuracy: 0.8630\n",
      "Epoch 14/24\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2329 - accuracy: 0.9270\n",
      "Epoch 15/24\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2054 - accuracy: 0.9420\n",
      "Epoch 16/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1606 - accuracy: 0.9460\n",
      "Epoch 17/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1830 - accuracy: 0.9370\n",
      "Epoch 18/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1081 - accuracy: 0.9660\n",
      "Epoch 19/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1040 - accuracy: 0.9670\n",
      "Epoch 20/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1073 - accuracy: 0.9710\n",
      "Epoch 21/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0603 - accuracy: 0.9800\n",
      "Epoch 22/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0894 - accuracy: 0.9740\n",
      "Epoch 23/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1262 - accuracy: 0.9680\n",
      "Epoch 24/24\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0737 - accuracy: 0.9780\n",
      "26032/26032 [==============================] - 45s 2ms/step\n",
      "Epoch 1/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 14.6322 - accuracy: 0.1360\n",
      "Epoch 2/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.4768 - accuracy: 0.1490\n",
      "Epoch 3/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3473 - accuracy: 0.1940\n",
      "Epoch 4/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2984 - accuracy: 0.2060\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2266 - accuracy: 0.2120\n",
      "Epoch 6/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1659 - accuracy: 0.2320\n",
      "Epoch 7/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0310 - accuracy: 0.3240\n",
      "Epoch 8/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7452 - accuracy: 0.3950\n",
      "Epoch 9/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.5122 - accuracy: 0.4910\n",
      "Epoch 10/25\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.0992 - accuracy: 0.6240\n",
      "Epoch 11/25\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.8372 - accuracy: 0.7130\n",
      "Epoch 12/25\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6869 - accuracy: 0.7680\n",
      "Epoch 13/25\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.4839 - accuracy: 0.8300\n",
      "Epoch 14/25\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.3481 - accuracy: 0.8860\n",
      "Epoch 15/25\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.3321 - accuracy: 0.8900\n",
      "Epoch 16/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1961 - accuracy: 0.9350\n",
      "Epoch 17/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1472 - accuracy: 0.9560\n",
      "Epoch 18/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1105 - accuracy: 0.9670\n",
      "Epoch 19/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0856 - accuracy: 0.9710\n",
      "Epoch 20/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1109 - accuracy: 0.9710\n",
      "Epoch 21/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1126 - accuracy: 0.9660\n",
      "Epoch 22/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0934 - accuracy: 0.9730\n",
      "Epoch 23/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0504 - accuracy: 0.9850\n",
      "Epoch 24/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0684 - accuracy: 0.9740\n",
      "Epoch 25/25\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0907 - accuracy: 0.9700\n",
      "26032/26032 [==============================] - 47s 2ms/step\n",
      "Epoch 1/26\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 22.6671 - accuracy: 0.1150\n",
      "Epoch 2/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3849 - accuracy: 0.2010\n",
      "Epoch 3/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3161 - accuracy: 0.2070\n",
      "Epoch 4/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2667 - accuracy: 0.2070\n",
      "Epoch 5/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2322 - accuracy: 0.2060\n",
      "Epoch 6/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1857 - accuracy: 0.2180\n",
      "Epoch 7/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0611 - accuracy: 0.2740\n",
      "Epoch 8/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.7121 - accuracy: 0.4110\n",
      "Epoch 9/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.3647 - accuracy: 0.5400\n",
      "Epoch 10/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.1052 - accuracy: 0.6110\n",
      "Epoch 11/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.8610 - accuracy: 0.7100\n",
      "Epoch 12/26\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5915 - accuracy: 0.7960\n",
      "Epoch 13/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.5200 - accuracy: 0.8390\n",
      "Epoch 14/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4138 - accuracy: 0.8600\n",
      "Epoch 15/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3440 - accuracy: 0.8740\n",
      "Epoch 16/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2388 - accuracy: 0.9310\n",
      "Epoch 17/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1653 - accuracy: 0.9450\n",
      "Epoch 18/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1428 - accuracy: 0.9510\n",
      "Epoch 19/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1417 - accuracy: 0.9550\n",
      "Epoch 20/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1363 - accuracy: 0.9610\n",
      "Epoch 21/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1031 - accuracy: 0.9660\n",
      "Epoch 22/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0836 - accuracy: 0.9670\n",
      "Epoch 23/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1713 - accuracy: 0.9520\n",
      "Epoch 24/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1184 - accuracy: 0.9610\n",
      "Epoch 25/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0759 - accuracy: 0.9760\n",
      "Epoch 26/26\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0664 - accuracy: 0.9790\n",
      "26032/26032 [==============================] - 49s 2ms/step\n",
      "Epoch 1/27\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 18.5589 - accuracy: 0.1470\n",
      "Epoch 2/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3312 - accuracy: 0.1980\n",
      "Epoch 3/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2867 - accuracy: 0.2070\n",
      "Epoch 4/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2617 - accuracy: 0.2070\n",
      "Epoch 5/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2244 - accuracy: 0.2070\n",
      "Epoch 6/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1995 - accuracy: 0.2100\n",
      "Epoch 7/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1587 - accuracy: 0.2230\n",
      "Epoch 8/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0561 - accuracy: 0.2950\n",
      "Epoch 9/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.8146 - accuracy: 0.3850\n",
      "Epoch 10/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.4548 - accuracy: 0.5130\n",
      "Epoch 11/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0768 - accuracy: 0.6580\n",
      "Epoch 12/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.8023 - accuracy: 0.7370\n",
      "Epoch 13/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6556 - accuracy: 0.7730\n",
      "Epoch 14/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4424 - accuracy: 0.8560\n",
      "Epoch 15/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2878 - accuracy: 0.9060\n",
      "Epoch 16/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1949 - accuracy: 0.9300\n",
      "Epoch 17/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1831 - accuracy: 0.9310\n",
      "Epoch 18/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1850 - accuracy: 0.9520\n",
      "Epoch 19/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1063 - accuracy: 0.9660\n",
      "Epoch 20/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1205 - accuracy: 0.9610\n",
      "Epoch 21/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1140 - accuracy: 0.9650\n",
      "Epoch 22/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1011 - accuracy: 0.9680\n",
      "Epoch 23/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0539 - accuracy: 0.9810\n",
      "Epoch 24/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0588 - accuracy: 0.9810\n",
      "Epoch 25/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0488 - accuracy: 0.9840\n",
      "Epoch 26/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0568 - accuracy: 0.9890\n",
      "Epoch 27/27\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0521 - accuracy: 0.9850\n",
      "26032/26032 [==============================] - 48s 2ms/step\n",
      "Epoch 1/28\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 30.1008 - accuracy: 0.1090\n",
      "Epoch 2/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3968 - accuracy: 0.2020\n",
      "Epoch 3/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3384 - accuracy: 0.1920\n",
      "Epoch 4/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2829 - accuracy: 0.1990\n",
      "Epoch 5/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2462 - accuracy: 0.2070\n",
      "Epoch 6/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2122 - accuracy: 0.2110\n",
      "Epoch 7/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1447 - accuracy: 0.2550\n",
      "Epoch 8/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0362 - accuracy: 0.2760\n",
      "Epoch 9/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.8882 - accuracy: 0.3280\n",
      "Epoch 10/28\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.6688 - accuracy: 0.4230\n",
      "Epoch 11/28\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.4559 - accuracy: 0.4930\n",
      "Epoch 12/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.2296 - accuracy: 0.5880\n",
      "Epoch 13/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0511 - accuracy: 0.6370\n",
      "Epoch 14/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.8206 - accuracy: 0.7100\n",
      "Epoch 15/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7263 - accuracy: 0.7580\n",
      "Epoch 16/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6178 - accuracy: 0.7920\n",
      "Epoch 17/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4613 - accuracy: 0.8520\n",
      "Epoch 18/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3575 - accuracy: 0.8930\n",
      "Epoch 19/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2488 - accuracy: 0.9180\n",
      "Epoch 20/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2350 - accuracy: 0.9220\n",
      "Epoch 21/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1691 - accuracy: 0.9430\n",
      "Epoch 22/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1506 - accuracy: 0.9540\n",
      "Epoch 23/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1311 - accuracy: 0.9570\n",
      "Epoch 24/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0833 - accuracy: 0.9650\n",
      "Epoch 25/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0653 - accuracy: 0.9800\n",
      "Epoch 26/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0771 - accuracy: 0.9770\n",
      "Epoch 27/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0622 - accuracy: 0.9800\n",
      "Epoch 28/28\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0929 - accuracy: 0.9730\n",
      "26032/26032 [==============================] - 46s 2ms/step\n",
      "Epoch 1/29\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 24.3919 - accuracy: 0.1140\n",
      "Epoch 2/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3717 - accuracy: 0.2040\n",
      "Epoch 3/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3067 - accuracy: 0.2070\n",
      "Epoch 4/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2621 - accuracy: 0.2060\n",
      "Epoch 5/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2355 - accuracy: 0.2110\n",
      "Epoch 6/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1851 - accuracy: 0.2220\n",
      "Epoch 7/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.0705 - accuracy: 0.2780\n",
      "Epoch 8/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.8820 - accuracy: 0.3350\n",
      "Epoch 9/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.6005 - accuracy: 0.4580\n",
      "Epoch 10/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.3468 - accuracy: 0.5250\n",
      "Epoch 11/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0617 - accuracy: 0.6390\n",
      "Epoch 12/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.8228 - accuracy: 0.7310\n",
      "Epoch 13/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6040 - accuracy: 0.7960\n",
      "Epoch 14/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.4623 - accuracy: 0.8520\n",
      "Epoch 15/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3213 - accuracy: 0.8900\n",
      "Epoch 16/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.3195 - accuracy: 0.8890\n",
      "Epoch 17/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.2627 - accuracy: 0.9230\n",
      "Epoch 18/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1852 - accuracy: 0.9420\n",
      "Epoch 19/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1546 - accuracy: 0.9460\n",
      "Epoch 20/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1228 - accuracy: 0.9640\n",
      "Epoch 21/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.1160 - accuracy: 0.9680\n",
      "Epoch 22/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0763 - accuracy: 0.9770\n",
      "Epoch 23/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0751 - accuracy: 0.9770\n",
      "Epoch 24/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0772 - accuracy: 0.9780\n",
      "Epoch 25/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0570 - accuracy: 0.9780\n",
      "Epoch 26/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0615 - accuracy: 0.9780\n",
      "Epoch 27/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0583 - accuracy: 0.9820\n",
      "Epoch 28/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0762 - accuracy: 0.9780\n",
      "Epoch 29/29\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0622 - accuracy: 0.9860\n",
      "26032/26032 [==============================] - 48s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_model_1000label_acc = []\n",
    "for i in range(30):\n",
    "    cnn_model_1000label = build_model()\n",
    "    cnn_model_1000label.compile(optimizer=tensorflow.optimizers.Adam(),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    cnn_model_1000label.fit(Xtrain_1000, Ttrain_1000, batch_size=64, epochs=i)\n",
    "    loss_1000label, accuracy_1000label = cnn_model_1000label.evaluate(Xtest_1000, Ttest_1000, batch_size=64)\n",
    "    cnn_model_1000label_acc.append(accuracy_1000label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06830055266618729,\n",
       " 0.06165488809347153,\n",
       " 0.17513060569763184,\n",
       " 0.16214658319950104,\n",
       " 0.1958358883857727,\n",
       " 0.19587430357933044,\n",
       " 0.207360178232193,\n",
       " 0.2803472578525543,\n",
       " 0.19633528590202332,\n",
       " 0.1415565460920334,\n",
       " 0.48221421241760254,\n",
       " 0.45248156785964966,\n",
       " 0.36904579401016235,\n",
       " 0.2985556125640869,\n",
       " 0.3261754810810089,\n",
       " 0.43242931365966797,\n",
       " 0.40546250343322754,\n",
       " 0.47526121139526367,\n",
       " 0.45267364382743835,\n",
       " 0.5115242600440979,\n",
       " 0.49170252680778503,\n",
       " 0.4972725808620453,\n",
       " 0.48958972096443176,\n",
       " 0.4532114267349243,\n",
       " 0.4998847544193268,\n",
       " 0.5391057133674622,\n",
       " 0.328403502702713,\n",
       " 0.43938228487968445,\n",
       " 0.44798707962036133,\n",
       " 0.47825753688812256]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_1000label_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 31.2884 - accuracy: 0.1100\n",
      "Epoch 2/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.3975 - accuracy: 0.2020\n",
      "Epoch 3/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.3366 - accuracy: 0.2080\n",
      "Epoch 4/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.2658 - accuracy: 0.2060\n",
      "Epoch 5/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.2108 - accuracy: 0.2120\n",
      "Epoch 6/23\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.1506 - accuracy: 0.2400\n",
      "Epoch 7/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 2.0480 - accuracy: 0.2860\n",
      "Epoch 8/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.8376 - accuracy: 0.3490\n",
      "Epoch 9/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.5735 - accuracy: 0.4560\n",
      "Epoch 10/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.3293 - accuracy: 0.5500\n",
      "Epoch 11/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 1.1062 - accuracy: 0.6290\n",
      "Epoch 12/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.8618 - accuracy: 0.7190\n",
      "Epoch 13/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.6904 - accuracy: 0.7610\n",
      "Epoch 14/23\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.5692 - accuracy: 0.8100\n",
      "Epoch 15/23\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.4501 - accuracy: 0.8530\n",
      "Epoch 16/23\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4233 - accuracy: 0.8620\n",
      "Epoch 17/23\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2806 - accuracy: 0.9050\n",
      "Epoch 18/23\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2594 - accuracy: 0.9150\n",
      "Epoch 19/23\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2410 - accuracy: 0.9250\n",
      "Epoch 20/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2354 - accuracy: 0.9150\n",
      "Epoch 21/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.2629 - accuracy: 0.9180\n",
      "Epoch 22/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.1663 - accuracy: 0.9460\n",
      "Epoch 23/23\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 0.1253 - accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14c7f3b90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model_1000label = build_model()\n",
    "cnn_model_1000label.compile(optimizer=tensorflow.optimizers.Adam(),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "cnn_model_1000label.fit(Xtrain_1000, Ttrain_1000, batch_size=64, epochs=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26032/26032 [==============================] - 48s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "loss_1000label, accuracy_1000label = cnn_model_1000label.evaluate(Xtest_1000, Ttest_1000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.4710 accuracy:0.5326\n"
     ]
    }
   ],
   "source": [
    "print('loss:%.4f accuracy:%.4f' %(loss_1000label, accuracy_1000label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we only use 1000 labeled data samples to train CNN, during the training process, the adjustment of weights does not guarantee every epoch will increase the performance of training. For instance, in the second, third, fourth epoch, the loss value doesn't go down at all, which shows that 1000 sample is far from adequate to train an comprehensive convolutional network. From the prediction result we can also find the accuracy is very low, it fluctuate between 49%~54% depending on different training process and traning test samples we use.\n",
    "\n",
    "In real world, we encounter this kind of issue very often. Although most of the datasets have inadequate labeled data, most of them have unlabeled data samples, and we could use these unlabeled data to boost prediction accuracy by using semi-supervised learning algorithms.\n",
    "\n",
    "In the next part, we'll implement VAT algorithm and explore if VAT can actually solve inadequate labeled data issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAT on SVHN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing our concepts of building VAT based models, we want to try building more complex model on a more difficult dataset. We choose SVHN dataset[1]. This dataset use label 10 for number 0, and there is no label 0 in the dataset, which is strange. As a result, we change label 10 to label 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "svhn_train=loadmat('train_32x32.mat')\n",
    "svhn_test=loadmat('test_32x32.mat')\n",
    "svhn_train_img3072=svhn_train['X'][:,:,:,:].transpose((3,0,1,2))\n",
    "svhn_test_img3072=svhn_test['X'][:,:,:,:].transpose((3,0,1,2))\n",
    "svhn_train_label=svhn_train['y'][:,0]\n",
    "svhn_test_label=svhn_test['y'][:,0]\n",
    "svhn_train_label[svhn_train_label== 10] = 0#change label 10 to label 0\n",
    "svhn_test_label[svhn_test_label == 10] = 0\n",
    "svhn_test_label_onehot=keras.utils.to_categorical(svhn_test_label)\n",
    "svhn_train_label_onehot=keras.utils.to_categorical(svhn_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At present, label 0-9 represent number 0-9. After reading the dataset, the data format is (image_index, row, column, channel).  \n",
    "Using RGB images to train the model is too slow, as a resutl, we use the method in [3] to convert RGB images to grayscale images. Now these images only have 1 channel which is faster for training the model. We also do standardization on the grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(images):\n",
    "    return np.expand_dims(np.dot(images, [0.3, 0.59, 0.11]), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febfa863e80>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_greyscale = rgb2gray(svhn_train_img3072).astype(np.float32).reshape(-1,1024)\n",
    "test_greyscale = rgb2gray(svhn_test_img3072).astype(np.float32).reshape(-1,1024)\n",
    "\n",
    "for i in range(12):\n",
    "    plt.figure(0,figsize=(20,20))\n",
    "    rffwp=random.randint(0,len(test_greyscale)-1)\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.imshow(test_greyscale[rffwp].reshape(32,32),cmap ='gray')\n",
    "    plt.title(f'label:{svhn_test_label[rffwp]}')\n",
    "\n",
    "train_mean = np.mean(train_greyscale, axis=0)\n",
    "\n",
    "# Calculate the std on the training data\n",
    "train_std = np.std(train_greyscale, axis=0)\n",
    "\n",
    "# Subtract it equally from all splits\n",
    "train_greyscale = (train_greyscale - train_mean) / train_std\n",
    "test_greyscale = (test_greyscale - train_mean)  / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we do in M3.2, we shuffle the dataset and use the first 3000 images in training dataset as labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(train_greyscale.shape[0])\n",
    "train_greyscale = train_greyscale[permutation, :]\n",
    "svhn_train_label = svhn_train_label[permutation]\n",
    "svhn_train_label_onehot = svhn_train_label_onehot[permutation,:]\n",
    "train_greyscale_3000label=train_greyscale[0:3000]\n",
    "svhn_train_label_onehot_3000label=svhn_train_label_onehot[0:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we prepare the training dataset and testing dataset, we implement our VAT based model for SVHN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vat_svhn_1(epsilon=1,alpha=0.3):\n",
    "    network_svhn = Sequential()\n",
    "    input_shape_svhn=(1024,)\n",
    "    network_svhn.add(keras.layers.Reshape((32,32,1),input_shape = input_shape_svhn))\n",
    "    network_svhn.add(keras.layers.Conv2D(filters=32,kernel_size = [5,5],padding = 'same',activation = tf.nn.relu))\n",
    "    network_svhn.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = 2))\n",
    "    network_svhn.add(keras.layers.Conv2D(filters=64,kernel_size = [5,5],padding = 'same',activation = tf.nn.relu))\n",
    "    network_svhn.add(keras.layers.MaxPool2D(pool_size=(2,2), strides =2))\n",
    "    network_svhn.add(keras.layers.Dropout(0.25))\n",
    "    network_svhn.add(keras.layers.Reshape((-1, 8 * 8 * 64)))\n",
    "    #network_svhn.add(keras.layers.Dropout(0.25))\n",
    "    #network_svhn.add(keras.layers.Flatten())\n",
    "    network_svhn.add(keras.layers.Dense(units=256,activation = tf.nn.relu))\n",
    "    #network_svhn.add(keras.layers.Dropout(0.5))\n",
    "    network_svhn.add(keras.layers.Dense(units=10))\n",
    "\n",
    "    model_input_n_unlabeled = Input((1024,))\n",
    "    model_input_nlabels=Input((10,))\n",
    "    model_input_n_labeled=Input((1024,))\n",
    "\n",
    "    p_logit_svhn = network_svhn( model_input_n_unlabeled )\n",
    "    p_svhn = Activation('softmax')( p_logit_svhn )\n",
    "    logit_svhn = network_svhn(model_input_n_labeled)\n",
    "    _svhn = Activation('softmax')( logit_svhn )\n",
    "    nll_loss_svhn = ce_loss(logit_svhn, model_input_nlabels)\n",
    "    ul_logit_svhn = network_svhn(model_input_n_unlabeled)\n",
    "    r_svhn = tf.random_normal(shape=tf.shape( model_input_n_unlabeled ))\n",
    "    r_svhn = unit_norm( r_svhn )\n",
    "    p_logit_r_svhn = network_svhn( model_input_n_unlabeled + epsilon*r_svhn  )\n",
    "    kld_svhn = tf.reduce_mean(kl_divergence( p_logit_svhn , p_logit_r_svhn ))\n",
    "    grad_kld_svhn = tf.gradients( kld_svhn , [r_svhn])[0]\n",
    "    r_vadv_svhn = tf.stop_gradient(grad_kld_svhn)\n",
    "    r_vadv_svhn = unit_norm( r_vadv_svhn )*alpha\n",
    "    p_logit_no_gradient_svhn = tf.stop_gradient(p_logit_svhn)\n",
    "    logit_no_gradient_svhn=tf.stop_gradient(logit_svhn)\n",
    "    p_logit_r_adv_svhn = network_svhn( model_input_n_unlabeled + r_vadv_svhn )\n",
    "    p_logit_r_adv_svhn_2 = network_svhn( model_input_n_labeled+ r_vadv_svhn )\n",
    "    vat_loss_svhn =  tf.reduce_mean(kl_divergence( p_logit_no_gradient_svhn, p_logit_r_adv_svhn ))\n",
    "    vat_loss_svhn_2=tf.reduce_mean(kl_divergence( _svhn, model_input_nlabels))\n",
    "    ent_loss_svhn = entropy_y_x(ul_logit_svhn)\n",
    "    additional_loss_svhn = vat_loss_svhn + ent_loss_svhn+vat_loss_svhn_2\n",
    "    loss_f_svhn = nll_loss_svhn + additional_loss_svhn\n",
    "    model_vat_svhn = Model([model_input_n_unlabeled,model_input_n_labeled,model_input_nlabels], p_svhn )\n",
    "    model_vat_svhn.add_loss(loss_f_svhn)\n",
    "    model_vat_svhn.compile( 'sgd',None,metrics=['accuracy'])\n",
    "    model_vat_svhn.metrics_names.append('total_loss_svhn')\n",
    "    model_vat_svhn.metrics_tensors.append(loss_f_svhn )\n",
    "    return model_vat_svhn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model has three inputs (n unlabeled images,n labeled images, n labels of the labeled images), when we use our model for prediction, we have to give it an useless label as the third input. This label does not influence the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn_test_label_onehot_useless=keras.utils.to_categorical([9]*len(svhn_test_label_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 12s 4ms/step - loss: 2.4303 - total_loss_svhn: 2.4303\n",
      "epoch:1, accruracy in testing dataset(3000 labeled): 0.19656576521204672\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.3956 - total_loss_svhn: 2.3956\n",
      "epoch:2, accruracy in testing dataset(3000 labeled): 0.20905039950829749\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.3612 - total_loss_svhn: 2.3612\n",
      "epoch:3, accruracy in testing dataset(3000 labeled): 0.19741087891825446\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.2915 - total_loss_svhn: 2.2915\n",
      "epoch:4, accruracy in testing dataset(3000 labeled): 0.2694376152427781\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.1701 - total_loss_svhn: 2.1701\n",
      "epoch:5, accruracy in testing dataset(3000 labeled): 0.43319760295021514\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.9649 - total_loss_svhn: 1.9649\n",
      "epoch:6, accruracy in testing dataset(3000 labeled): 0.48955132145052244\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.7257 - total_loss_svhn: 1.7257\n",
      "epoch:7, accruracy in testing dataset(3000 labeled): 0.5816303011677935\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.5145 - total_loss_svhn: 1.5145\n",
      "epoch:8, accruracy in testing dataset(3000 labeled): 0.6269975414874002\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.3580 - total_loss_svhn: 1.3580\n",
      "epoch:9, accruracy in testing dataset(3000 labeled): 0.6561539643515673\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2477 - total_loss_svhn: 1.2477\n",
      "epoch:10, accruracy in testing dataset(3000 labeled): 0.7136985248924401\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1650 - total_loss_svhn: 1.1650\n",
      "epoch:11, accruracy in testing dataset(3000 labeled): 0.71220036877689\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.0919 - total_loss_svhn: 1.0919\n",
      "epoch:12, accruracy in testing dataset(3000 labeled): 0.7280654578979717\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.0395 - total_loss_svhn: 1.0395\n",
      "epoch:13, accruracy in testing dataset(3000 labeled): 0.7259142593730793\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.9901 - total_loss_svhn: 0.9901\n",
      "epoch:14, accruracy in testing dataset(3000 labeled): 0.7331745543945912\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.9474 - total_loss_svhn: 0.9474\n",
      "epoch:15, accruracy in testing dataset(3000 labeled): 0.766479717271051\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.9206 - total_loss_svhn: 0.9206\n",
      "epoch:16, accruracy in testing dataset(3000 labeled): 0.7751229256299939\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.8952 - total_loss_svhn: 0.8952\n",
      "epoch:17, accruracy in testing dataset(3000 labeled): 0.7741625691456668\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.8590 - total_loss_svhn: 0.8590\n",
      "epoch:18, accruracy in testing dataset(3000 labeled): 0.7735479409956976\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.8415 - total_loss_svhn: 0.8415\n",
      "epoch:19, accruracy in testing dataset(3000 labeled): 0.7759296250768285\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.8188 - total_loss_svhn: 0.8188\n",
      "epoch:20, accruracy in testing dataset(3000 labeled): 0.7882606023355869\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7962 - total_loss_svhn: 0.7962\n",
      "epoch:21, accruracy in testing dataset(3000 labeled): 0.7791948371235402\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7815 - total_loss_svhn: 0.7815\n",
      "epoch:22, accruracy in testing dataset(3000 labeled): 0.793139213275968\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7672 - total_loss_svhn: 0.7672\n",
      "epoch:23, accruracy in testing dataset(3000 labeled): 0.78779963122311\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.7433 - total_loss_svhn: 0.7433\n",
      "epoch:1, accruracy in testing dataset(3000 labeled): 0.7914874001229256\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.7276 - total_loss_svhn: 0.7276\n",
      "epoch:2, accruracy in testing dataset(3000 labeled): 0.7917178856791641\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7177 - total_loss_svhn: 0.7177\n",
      "epoch:3, accruracy in testing dataset(3000 labeled): 0.796020282728949\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.6945 - total_loss_svhn: 0.6945\n",
      "epoch:4, accruracy in testing dataset(3000 labeled): 0.8005531653349723\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7009 - total_loss_svhn: 0.7009\n",
      "epoch:5, accruracy in testing dataset(3000 labeled): 0.7910264290104487\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6714 - total_loss_svhn: 0.6714\n",
      "epoch:6, accruracy in testing dataset(3000 labeled): 0.7949446834665027\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6687 - total_loss_svhn: 0.6687\n",
      "epoch:7, accruracy in testing dataset(3000 labeled): 0.7976720958819914\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6559 - total_loss_svhn: 0.6559\n",
      "epoch:8, accruracy in testing dataset(3000 labeled): 0.7954440688383528\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6469 - total_loss_svhn: 0.6469\n",
      "epoch:9, accruracy in testing dataset(3000 labeled): 0.7970574677320221\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6377 - total_loss_svhn: 0.6377\n",
      "epoch:10, accruracy in testing dataset(3000 labeled): 0.7984019668100799\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6214 - total_loss_svhn: 0.6214\n",
      "epoch:11, accruracy in testing dataset(3000 labeled): 0.8016671788567916\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6114 - total_loss_svhn: 0.6114\n",
      "epoch:12, accruracy in testing dataset(3000 labeled): 0.8005915795943455\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6066 - total_loss_svhn: 0.6066\n",
      "epoch:13, accruracy in testing dataset(3000 labeled): 0.800361094038107\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6020 - total_loss_svhn: 0.6020\n",
      "epoch:14, accruracy in testing dataset(3000 labeled): 0.7856868469575906\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5966 - total_loss_svhn: 0.5966\n",
      "epoch:15, accruracy in testing dataset(3000 labeled): 0.7886447449293178\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5837 - total_loss_svhn: 0.5837\n",
      "epoch:16, accruracy in testing dataset(3000 labeled): 0.7940227412415488\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5750 - total_loss_svhn: 0.5750\n",
      "epoch:17, accruracy in testing dataset(3000 labeled): 0.8031653349723418\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5763 - total_loss_svhn: 0.5763\n",
      "epoch:18, accruracy in testing dataset(3000 labeled): 0.8009373079287031\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5624 - total_loss_svhn: 0.5624\n",
      "epoch:19, accruracy in testing dataset(3000 labeled): 0.8026275353411186\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5503 - total_loss_svhn: 0.5503\n",
      "epoch:20, accruracy in testing dataset(3000 labeled): 0.798939766441303\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5617 - total_loss_svhn: 0.5617\n",
      "epoch:21, accruracy in testing dataset(3000 labeled): 0.7987092808850645\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5514 - total_loss_svhn: 0.5514\n",
      "epoch:22, accruracy in testing dataset(3000 labeled): 0.8015519360786724\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5391 - total_loss_svhn: 0.5391\n",
      "epoch:23, accruracy in testing dataset(3000 labeled): 0.8035494775660725\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5355 - total_loss_svhn: 0.5355\n",
      "epoch:1, accruracy in testing dataset(3000 labeled): 0.8024354640442533\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5282 - total_loss_svhn: 0.5282\n",
      "epoch:2, accruracy in testing dataset(3000 labeled): 0.8058927473878303\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5281 - total_loss_svhn: 0.5281\n",
      "epoch:3, accruracy in testing dataset(3000 labeled): 0.8031269207129687\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5242 - total_loss_svhn: 0.5242\n",
      "epoch:4, accruracy in testing dataset(3000 labeled): 0.7988629379225568\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5149 - total_loss_svhn: 0.5149\n",
      "epoch:5, accruracy in testing dataset(3000 labeled): 0.8025891210817455\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5164 - total_loss_svhn: 0.5164\n",
      "epoch:6, accruracy in testing dataset(3000 labeled): 0.8027043638598648\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5036 - total_loss_svhn: 0.5036\n",
      "epoch:7, accruracy in testing dataset(3000 labeled): 0.7826521204671174\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5080 - total_loss_svhn: 0.5080\n",
      "epoch:8, accruracy in testing dataset(3000 labeled): 0.8070835894283959\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5060 - total_loss_svhn: 0.5060\n",
      "epoch:9, accruracy in testing dataset(3000 labeled): 0.8018976644130301\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.5032 - total_loss_svhn: 0.5032\n",
      "epoch:10, accruracy in testing dataset(3000 labeled): 0.8043561770129072\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5074 - total_loss_svhn: 0.5074\n",
      "epoch:11, accruracy in testing dataset(3000 labeled): 0.8025122925629994\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4970 - total_loss_svhn: 0.4970\n",
      "epoch:12, accruracy in testing dataset(3000 labeled): 0.8053549477566072\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4902 - total_loss_svhn: 0.4902\n",
      "epoch:13, accruracy in testing dataset(3000 labeled): 0.8028964351567301\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.4883 - total_loss_svhn: 0.4883\n",
      "epoch:14, accruracy in testing dataset(3000 labeled): 0.8024354640442533\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.4948 - total_loss_svhn: 0.4948\n",
      "epoch:15, accruracy in testing dataset(3000 labeled): 0.803779963122311\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4851 - total_loss_svhn: 0.4851\n",
      "epoch:16, accruracy in testing dataset(3000 labeled): 0.8041256914566687\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4886 - total_loss_svhn: 0.4886\n",
      "epoch:17, accruracy in testing dataset(3000 labeled): 0.8059695759065765\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.4857 - total_loss_svhn: 0.4857\n",
      "epoch:18, accruracy in testing dataset(3000 labeled): 0.8014366933005531\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4784 - total_loss_svhn: 0.4784\n",
      "epoch:19, accruracy in testing dataset(3000 labeled): 0.8053549477566072\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4749 - total_loss_svhn: 0.4749\n",
      "epoch:20, accruracy in testing dataset(3000 labeled): 0.8050092194222496\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4823 - total_loss_svhn: 0.4823\n",
      "epoch:21, accruracy in testing dataset(3000 labeled): 0.8040488629379225\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.4789 - total_loss_svhn: 0.4789\n",
      "epoch:22, accruracy in testing dataset(3000 labeled): 0.8034726490473264\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 10s 3ms/step - loss: 0.4747 - total_loss_svhn: 0.4747\n",
      "epoch:23, accruracy in testing dataset(3000 labeled): 0.8075061462814997\n"
     ]
    }
   ],
   "source": [
    "model_vat_svhn_3000_acc_trace=[]\n",
    "model_vat_svhn_3000_loss_trace=[]\n",
    "model_vat_svhn_3000=vat_svhn_1(epsilon=10,alpha=0.3)\n",
    "for j in range(3):\n",
    "    for i in range(3000,70256,3000):\n",
    "        model_vat_svhn_3000.fit([train_greyscale[i:i+3000],train_greyscale_3000label,svhn_train_label_onehot_3000label], None,epochs=1 )\n",
    "        y_pred_svhn = model_vat_svhn_3000.predict( [test_greyscale,test_greyscale,svhn_test_label_onehot_useless] ).argmax(-1)\n",
    "        acc=accuracy_score(svhn_test_label , y_pred_svhn)\n",
    "        print(f\"epoch:{i//3000}, accruracy in testing dataset(3000 labeled): {acc}\")\n",
    "        model_vat_svhn_3000_acc_trace.append(acc)\n",
    "        model_vat_svhn_3000_loss_trace.append(model_vat_svhn_3000.history.history['total_loss_svhn'][-1])\n",
    "#del model_vat_svhn_3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_greyscale_3000label,svhn_train_label_onehot_3000label\n",
    "train_greyscale_2000label=train_greyscale[0:2000]\n",
    "svhn_train_label_onehot_2000label=svhn_train_label_onehot[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 2.4494 - total_loss_svhn: 2.4494\n",
      "epoch:1, accruracy in testing dataset(2000 labeled): 0.20666871542716655\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 2.4129 - total_loss_svhn: 2.4129\n",
      "epoch:2, accruracy in testing dataset(2000 labeled): 0.22249539028887522\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 2.3929 - total_loss_svhn: 2.3929\n",
      "epoch:3, accruracy in testing dataset(2000 labeled): 0.22464658881376767\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 2.3673 - total_loss_svhn: 2.3673\n",
      "epoch:4, accruracy in testing dataset(2000 labeled): 0.2381684081130916\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 2.3283 - total_loss_svhn: 2.3283\n",
      "epoch:5, accruracy in testing dataset(2000 labeled): 0.24489090350338044\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 2.2785 - total_loss_svhn: 2.2785\n",
      "epoch:6, accruracy in testing dataset(2000 labeled): 0.32502304855562386\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 2.1980 - total_loss_svhn: 2.1980\n",
      "epoch:7, accruracy in testing dataset(2000 labeled): 0.37987861094038106\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 2.0803 - total_loss_svhn: 2.0803\n",
      "epoch:8, accruracy in testing dataset(2000 labeled): 0.42078979717271053\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 1.9476 - total_loss_svhn: 1.9476\n",
      "epoch:9, accruracy in testing dataset(2000 labeled): 0.4697295636140135\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 1.7774 - total_loss_svhn: 1.7774\n",
      "epoch:10, accruracy in testing dataset(2000 labeled): 0.48935925015365705\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 1.6182 - total_loss_svhn: 1.6182\n",
      "epoch:11, accruracy in testing dataset(2000 labeled): 0.529540565457898\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 1.4817 - total_loss_svhn: 1.4817\n",
      "epoch:12, accruracy in testing dataset(2000 labeled): 0.5337661339889367\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 1.3485 - total_loss_svhn: 1.3485\n",
      "epoch:13, accruracy in testing dataset(2000 labeled): 0.634181007990166\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 1.2628 - total_loss_svhn: 1.2628\n",
      "epoch:14, accruracy in testing dataset(2000 labeled): 0.6643746158574063\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 1.1909 - total_loss_svhn: 1.1909\n",
      "epoch:15, accruracy in testing dataset(2000 labeled): 0.68899815611555\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 1.1111 - total_loss_svhn: 1.1111\n",
      "epoch:16, accruracy in testing dataset(2000 labeled): 0.682659803318992\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 1.0677 - total_loss_svhn: 1.0677\n",
      "epoch:17, accruracy in testing dataset(2000 labeled): 0.6852335586969883\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 1.0220 - total_loss_svhn: 1.0220\n",
      "epoch:18, accruracy in testing dataset(2000 labeled): 0.7130838967424709\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.9576 - total_loss_svhn: 0.9576\n",
      "epoch:19, accruracy in testing dataset(2000 labeled): 0.7012138905961893\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.9214 - total_loss_svhn: 0.9214\n",
      "epoch:20, accruracy in testing dataset(2000 labeled): 0.719998463429625\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.9037 - total_loss_svhn: 0.9037\n",
      "epoch:21, accruracy in testing dataset(2000 labeled): 0.73740012292563\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.8621 - total_loss_svhn: 0.8621\n",
      "epoch:22, accruracy in testing dataset(2000 labeled): 0.6962584511370621\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.8337 - total_loss_svhn: 0.8337\n",
      "epoch:23, accruracy in testing dataset(2000 labeled): 0.7336739397664414\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.8279 - total_loss_svhn: 0.8279\n",
      "epoch:24, accruracy in testing dataset(2000 labeled): 0.7198832206515058\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.7929 - total_loss_svhn: 0.7929\n",
      "epoch:25, accruracy in testing dataset(2000 labeled): 0.7523816840811309\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.7671 - total_loss_svhn: 0.7671\n",
      "epoch:26, accruracy in testing dataset(2000 labeled): 0.7462354025814383\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.7511 - total_loss_svhn: 0.7511\n",
      "epoch:27, accruracy in testing dataset(2000 labeled): 0.7602566072526121\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.7227 - total_loss_svhn: 0.7227\n",
      "epoch:28, accruracy in testing dataset(2000 labeled): 0.7696681007990166\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.7325 - total_loss_svhn: 0.7325\n",
      "epoch:29, accruracy in testing dataset(2000 labeled): 0.7465811309157959\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.7010 - total_loss_svhn: 0.7010\n",
      "epoch:30, accruracy in testing dataset(2000 labeled): 0.7613322065150584\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.6842 - total_loss_svhn: 0.6842\n",
      "epoch:31, accruracy in testing dataset(2000 labeled): 0.7699754148740012\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.6624 - total_loss_svhn: 0.6624\n",
      "epoch:32, accruracy in testing dataset(2000 labeled): 0.770321143208359\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.6633 - total_loss_svhn: 0.6633\n",
      "epoch:33, accruracy in testing dataset(2000 labeled): 0.7641364474492932\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.6575 - total_loss_svhn: 0.6575\n",
      "epoch:34, accruracy in testing dataset(2000 labeled): 0.7541871542716656\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.6339 - total_loss_svhn: 0.6339\n",
      "epoch:35, accruracy in testing dataset(2000 labeled): 0.7677858020897357\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.6317 - total_loss_svhn: 0.6317\n",
      "epoch:1, accruracy in testing dataset(2000 labeled): 0.7617547633681623\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.6069 - total_loss_svhn: 0.6069\n",
      "epoch:2, accruracy in testing dataset(2000 labeled): 0.7466963736939152\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5952 - total_loss_svhn: 0.5952\n",
      "epoch:3, accruracy in testing dataset(2000 labeled): 0.7693223724646588\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5974 - total_loss_svhn: 0.5974\n",
      "epoch:4, accruracy in testing dataset(2000 labeled): 0.7739704978488015\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5993 - total_loss_svhn: 0.5993\n",
      "epoch:5, accruracy in testing dataset(2000 labeled): 0.7728948985863553\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5841 - total_loss_svhn: 0.5841\n",
      "epoch:6, accruracy in testing dataset(2000 labeled): 0.7602181929932391\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5790 - total_loss_svhn: 0.5790\n",
      "epoch:7, accruracy in testing dataset(2000 labeled): 0.7731637984019668\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5765 - total_loss_svhn: 0.5765\n",
      "epoch:8, accruracy in testing dataset(2000 labeled): 0.7702827289489859\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5742 - total_loss_svhn: 0.5742\n",
      "epoch:9, accruracy in testing dataset(2000 labeled): 0.7425860479409957\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5634 - total_loss_svhn: 0.5634\n",
      "epoch:10, accruracy in testing dataset(2000 labeled): 0.7759296250768285\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5454 - total_loss_svhn: 0.5454\n",
      "epoch:11, accruracy in testing dataset(2000 labeled): 0.7702443146896127\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5500 - total_loss_svhn: 0.5500\n",
      "epoch:12, accruracy in testing dataset(2000 labeled): 0.7735095267363246\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5446 - total_loss_svhn: 0.5446\n",
      "epoch:13, accruracy in testing dataset(2000 labeled): 0.7740473263675476\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5325 - total_loss_svhn: 0.5325\n",
      "epoch:14, accruracy in testing dataset(2000 labeled): 0.769360786724032\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5354 - total_loss_svhn: 0.5354\n",
      "epoch:15, accruracy in testing dataset(2000 labeled): 0.7693992009834051\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5276 - total_loss_svhn: 0.5276\n",
      "epoch:16, accruracy in testing dataset(2000 labeled): 0.7664028887523049\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5212 - total_loss_svhn: 0.5212\n",
      "epoch:17, accruracy in testing dataset(2000 labeled): 0.7596419791026429\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5284 - total_loss_svhn: 0.5284\n",
      "epoch:18, accruracy in testing dataset(2000 labeled): 0.7658266748617086\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5132 - total_loss_svhn: 0.5132\n",
      "epoch:19, accruracy in testing dataset(2000 labeled): 0.7645974185617701\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5067 - total_loss_svhn: 0.5067\n",
      "epoch:20, accruracy in testing dataset(2000 labeled): 0.7775046097111248\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.5109 - total_loss_svhn: 0.5109\n",
      "epoch:21, accruracy in testing dataset(2000 labeled): 0.7787722802704364\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5178 - total_loss_svhn: 0.5178\n",
      "epoch:22, accruracy in testing dataset(2000 labeled): 0.7781576521204672\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.5101 - total_loss_svhn: 0.5101\n",
      "epoch:23, accruracy in testing dataset(2000 labeled): 0.775161339889367\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4993 - total_loss_svhn: 0.4993\n",
      "epoch:24, accruracy in testing dataset(2000 labeled): 0.7776966810079902\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4979 - total_loss_svhn: 0.4979\n",
      "epoch:25, accruracy in testing dataset(2000 labeled): 0.7743930547019053\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4998 - total_loss_svhn: 0.4998\n",
      "epoch:26, accruracy in testing dataset(2000 labeled): 0.775660725261217\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4890 - total_loss_svhn: 0.4890\n",
      "epoch:27, accruracy in testing dataset(2000 labeled): 0.7733942839582053\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4862 - total_loss_svhn: 0.4862\n",
      "epoch:28, accruracy in testing dataset(2000 labeled): 0.7736247695144438\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4949 - total_loss_svhn: 0.4949\n",
      "epoch:29, accruracy in testing dataset(2000 labeled): 0.7740473263675476\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4832 - total_loss_svhn: 0.4832\n",
      "epoch:30, accruracy in testing dataset(2000 labeled): 0.7667102028272895\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4842 - total_loss_svhn: 0.4842\n",
      "epoch:31, accruracy in testing dataset(2000 labeled): 0.7765826674861709\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4807 - total_loss_svhn: 0.4807\n",
      "epoch:32, accruracy in testing dataset(2000 labeled): 0.7772741241548863\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4761 - total_loss_svhn: 0.4761\n",
      "epoch:33, accruracy in testing dataset(2000 labeled): 0.7727412415488629\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4819 - total_loss_svhn: 0.4819\n",
      "epoch:34, accruracy in testing dataset(2000 labeled): 0.7711278426551936\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4811 - total_loss_svhn: 0.4811\n",
      "epoch:35, accruracy in testing dataset(2000 labeled): 0.7696296865396435\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4797 - total_loss_svhn: 0.4797\n",
      "epoch:1, accruracy in testing dataset(2000 labeled): 0.7817685925015365\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4707 - total_loss_svhn: 0.4707\n",
      "epoch:2, accruracy in testing dataset(2000 labeled): 0.7801551936078672\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4796 - total_loss_svhn: 0.4796\n",
      "epoch:3, accruracy in testing dataset(2000 labeled): 0.7684388444990781\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4753 - total_loss_svhn: 0.4753\n",
      "epoch:4, accruracy in testing dataset(2000 labeled): 0.77519975414874\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4679 - total_loss_svhn: 0.4679\n",
      "epoch:5, accruracy in testing dataset(2000 labeled): 0.7762753534111863\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4704 - total_loss_svhn: 0.4704\n",
      "epoch:6, accruracy in testing dataset(2000 labeled): 0.7786954517516902\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4664 - total_loss_svhn: 0.4664\n",
      "epoch:7, accruracy in testing dataset(2000 labeled): 0.7803088506453596\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4735 - total_loss_svhn: 0.4735\n",
      "epoch:8, accruracy in testing dataset(2000 labeled): 0.7748924400737554\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4621 - total_loss_svhn: 0.4621\n",
      "epoch:9, accruracy in testing dataset(2000 labeled): 0.7806929932390904\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4660 - total_loss_svhn: 0.4660\n",
      "epoch:10, accruracy in testing dataset(2000 labeled): 0.7773509526736324\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4712 - total_loss_svhn: 0.4712\n",
      "epoch:11, accruracy in testing dataset(2000 labeled): 0.7648663183773817\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4671 - total_loss_svhn: 0.4671\n",
      "epoch:12, accruracy in testing dataset(2000 labeled): 0.7828826060233559\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4690 - total_loss_svhn: 0.4690\n",
      "epoch:13, accruracy in testing dataset(2000 labeled): 0.7828057775046097\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4625 - total_loss_svhn: 0.4625\n",
      "epoch:14, accruracy in testing dataset(2000 labeled): 0.780500921942225\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4606 - total_loss_svhn: 0.4606\n",
      "epoch:15, accruracy in testing dataset(2000 labeled): 0.7798862937922557\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4597 - total_loss_svhn: 0.4597\n",
      "epoch:16, accruracy in testing dataset(2000 labeled): 0.7773125384142594\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4647 - total_loss_svhn: 0.4647\n",
      "epoch:17, accruracy in testing dataset(2000 labeled): 0.7768515673017824\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 3ms/step - loss: 0.4713 - total_loss_svhn: 0.4713\n",
      "epoch:18, accruracy in testing dataset(2000 labeled): 0.7831515058389674\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4588 - total_loss_svhn: 0.4588\n",
      "epoch:19, accruracy in testing dataset(2000 labeled): 0.7815765212046711\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4616 - total_loss_svhn: 0.4616\n",
      "epoch:20, accruracy in testing dataset(2000 labeled): 0.780039950829748\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4584 - total_loss_svhn: 0.4584\n",
      "epoch:21, accruracy in testing dataset(2000 labeled): 0.7818454210202828\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4573 - total_loss_svhn: 0.4573\n",
      "epoch:22, accruracy in testing dataset(2000 labeled): 0.7805777504609711\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4569 - total_loss_svhn: 0.4569\n",
      "epoch:23, accruracy in testing dataset(2000 labeled): 0.7815765212046711\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4555 - total_loss_svhn: 0.4555\n",
      "epoch:24, accruracy in testing dataset(2000 labeled): 0.7778503380454825\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4555 - total_loss_svhn: 0.4555\n",
      "epoch:25, accruracy in testing dataset(2000 labeled): 0.7757759680393362\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4551 - total_loss_svhn: 0.4551\n",
      "epoch:26, accruracy in testing dataset(2000 labeled): 0.7835356484326982\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4550 - total_loss_svhn: 0.4550\n",
      "epoch:27, accruracy in testing dataset(2000 labeled): 0.778119237861094\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4557 - total_loss_svhn: 0.4557\n",
      "epoch:28, accruracy in testing dataset(2000 labeled): 0.780500921942225\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4563 - total_loss_svhn: 0.4563\n",
      "epoch:29, accruracy in testing dataset(2000 labeled): 0.7778887523048555\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4584 - total_loss_svhn: 0.4584\n",
      "epoch:30, accruracy in testing dataset(2000 labeled): 0.7805777504609711\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4550 - total_loss_svhn: 0.4550\n",
      "epoch:31, accruracy in testing dataset(2000 labeled): 0.7842655193607867\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 8s 4ms/step - loss: 0.4584 - total_loss_svhn: 0.4584\n",
      "epoch:32, accruracy in testing dataset(2000 labeled): 0.7832667486170867\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4493 - total_loss_svhn: 0.4493\n",
      "epoch:33, accruracy in testing dataset(2000 labeled): 0.7803472649047326\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4508 - total_loss_svhn: 0.4508\n",
      "epoch:34, accruracy in testing dataset(2000 labeled): 0.7819222495390289\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 7s 4ms/step - loss: 0.4533 - total_loss_svhn: 0.4533\n",
      "epoch:35, accruracy in testing dataset(2000 labeled): 0.7816533497234174\n"
     ]
    }
   ],
   "source": [
    "model_vat_svhn_2000_acc_trace=[]\n",
    "model_vat_svhn_2000_loss_trace=[]\n",
    "model_vat_svhn_2000=vat_svhn_1(epsilon=10,alpha=0.3)\n",
    "for j in range(3):\n",
    "    for i in range(2000,71256,2000):\n",
    "        model_vat_svhn_2000.fit([train_greyscale[i:i+2000],train_greyscale_2000label,svhn_train_label_onehot_2000label], None,epochs=1 )\n",
    "        y_pred_svhn = model_vat_svhn_2000.predict( [test_greyscale,test_greyscale,svhn_test_label_onehot_useless] ).argmax(-1)\n",
    "        acc=accuracy_score(svhn_test_label , y_pred_svhn)\n",
    "        print(f\"epoch:{i//2000}, accruracy in testing dataset(2000 labeled): {acc}\")\n",
    "        model_vat_svhn_2000_acc_trace.append(acc)\n",
    "        model_vat_svhn_2000_loss_trace.append(model_vat_svhn_2000.history.history['total_loss_svhn'][-1])\n",
    "#del model_vat_svhn_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_greyscale_2000label,svhn_train_label_onehot_2000label\n",
    "train_greyscale_1000label=train_greyscale[0:1000]\n",
    "svhn_train_label_onehot_1000label=svhn_train_label_onehot[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 2.4635 - total_loss_svhn: 2.4635\n",
      "epoch:1, accruracy in testing dataset(1000 labeled): 0.18942071296865395\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.4270 - total_loss_svhn: 2.4270\n",
      "epoch:2, accruracy in testing dataset(1000 labeled): 0.19114935464044253\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.4141 - total_loss_svhn: 2.4141\n",
      "epoch:3, accruracy in testing dataset(1000 labeled): 0.1951060233558697\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3990 - total_loss_svhn: 2.3990\n",
      "epoch:4, accruracy in testing dataset(1000 labeled): 0.19794867854947756\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3900 - total_loss_svhn: 2.3900\n",
      "epoch:5, accruracy in testing dataset(1000 labeled): 0.19145666871542716\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3702 - total_loss_svhn: 2.3702\n",
      "epoch:6, accruracy in testing dataset(1000 labeled): 0.20301936078672403\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3515 - total_loss_svhn: 2.3515\n",
      "epoch:7, accruracy in testing dataset(1000 labeled): 0.2362861094038107\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3311 - total_loss_svhn: 2.3311\n",
      "epoch:8, accruracy in testing dataset(1000 labeled): 0.23336662569145666\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.3030 - total_loss_svhn: 2.3030\n",
      "epoch:9, accruracy in testing dataset(1000 labeled): 0.2223417332513829\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.2739 - total_loss_svhn: 2.2739\n",
      "epoch:10, accruracy in testing dataset(1000 labeled): 0.27642901044867857\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.2388 - total_loss_svhn: 2.2388\n",
      "epoch:11, accruracy in testing dataset(1000 labeled): 0.28787645974185616\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.1813 - total_loss_svhn: 2.1813\n",
      "epoch:12, accruracy in testing dataset(1000 labeled): 0.29978488014751076\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.1098 - total_loss_svhn: 2.1098\n",
      "epoch:13, accruracy in testing dataset(1000 labeled): 0.3334741856177013\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.0514 - total_loss_svhn: 2.0514\n",
      "epoch:14, accruracy in testing dataset(1000 labeled): 0.3397741241548863\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.9513 - total_loss_svhn: 1.9513\n",
      "epoch:15, accruracy in testing dataset(1000 labeled): 0.3334741856177013\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.8488 - total_loss_svhn: 1.8488\n",
      "epoch:16, accruracy in testing dataset(1000 labeled): 0.43780731407498463\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.7516 - total_loss_svhn: 1.7516\n",
      "epoch:17, accruracy in testing dataset(1000 labeled): 0.4237861094038107\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.6619 - total_loss_svhn: 1.6619\n",
      "epoch:18, accruracy in testing dataset(1000 labeled): 0.457360172095882\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.5560 - total_loss_svhn: 1.5560\n",
      "epoch:19, accruracy in testing dataset(1000 labeled): 0.48229102642901045\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.4446 - total_loss_svhn: 1.4446\n",
      "epoch:20, accruracy in testing dataset(1000 labeled): 0.473839889366933\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3698 - total_loss_svhn: 1.3698\n",
      "epoch:21, accruracy in testing dataset(1000 labeled): 0.5107559926244623\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.3258 - total_loss_svhn: 1.3258\n",
      "epoch:22, accruracy in testing dataset(1000 labeled): 0.54990012292563\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.2385 - total_loss_svhn: 1.2385\n",
      "epoch:23, accruracy in testing dataset(1000 labeled): 0.5603488014751076\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.1849 - total_loss_svhn: 1.1849\n",
      "epoch:24, accruracy in testing dataset(1000 labeled): 0.5906576521204672\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.1500 - total_loss_svhn: 1.1500\n",
      "epoch:25, accruracy in testing dataset(1000 labeled): 0.6014520590043024\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0699 - total_loss_svhn: 1.0699\n",
      "epoch:26, accruracy in testing dataset(1000 labeled): 0.6358328211432084\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0147 - total_loss_svhn: 1.0147\n",
      "epoch:27, accruracy in testing dataset(1000 labeled): 0.6196220036877689\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.0195 - total_loss_svhn: 1.0195\n",
      "epoch:28, accruracy in testing dataset(1000 labeled): 0.5903119237861094\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9580 - total_loss_svhn: 0.9580\n",
      "epoch:29, accruracy in testing dataset(1000 labeled): 0.6392132759680393\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.9420 - total_loss_svhn: 0.9420\n",
      "epoch:30, accruracy in testing dataset(1000 labeled): 0.6376767055931162\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8964 - total_loss_svhn: 0.8964\n",
      "epoch:31, accruracy in testing dataset(1000 labeled): 0.6573063921327597\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8408 - total_loss_svhn: 0.8408\n",
      "epoch:32, accruracy in testing dataset(1000 labeled): 0.66180086047941\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.8465 - total_loss_svhn: 0.8465\n",
      "epoch:33, accruracy in testing dataset(1000 labeled): 0.655039950829748\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.8299 - total_loss_svhn: 0.8299\n",
      "epoch:34, accruracy in testing dataset(1000 labeled): 0.654540565457898\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.7876 - total_loss_svhn: 0.7876\n",
      "epoch:35, accruracy in testing dataset(1000 labeled): 0.670059926244622\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7766 - total_loss_svhn: 0.7766\n",
      "epoch:36, accruracy in testing dataset(1000 labeled): 0.6744775660725261\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7680 - total_loss_svhn: 0.7680\n",
      "epoch:37, accruracy in testing dataset(1000 labeled): 0.6510448678549478\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.7428 - total_loss_svhn: 0.7428\n",
      "epoch:38, accruracy in testing dataset(1000 labeled): 0.6477412415488629\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7342 - total_loss_svhn: 0.7342\n",
      "epoch:39, accruracy in testing dataset(1000 labeled): 0.6698294406883836\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.7142 - total_loss_svhn: 0.7142\n",
      "epoch:40, accruracy in testing dataset(1000 labeled): 0.6923786109403811\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6911 - total_loss_svhn: 0.6911\n",
      "epoch:41, accruracy in testing dataset(1000 labeled): 0.6621081745543946\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6855 - total_loss_svhn: 0.6855\n",
      "epoch:42, accruracy in testing dataset(1000 labeled): 0.6647971727105101\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6785 - total_loss_svhn: 0.6785\n",
      "epoch:43, accruracy in testing dataset(1000 labeled): 0.6759757221880762\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6722 - total_loss_svhn: 0.6722\n",
      "epoch:44, accruracy in testing dataset(1000 labeled): 0.6663721573448064\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6668 - total_loss_svhn: 0.6668\n",
      "epoch:45, accruracy in testing dataset(1000 labeled): 0.6780116779348494\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6691 - total_loss_svhn: 0.6691\n",
      "epoch:46, accruracy in testing dataset(1000 labeled): 0.7142363245236631\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6302 - total_loss_svhn: 0.6302\n",
      "epoch:47, accruracy in testing dataset(1000 labeled): 0.7163875230485556\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6305 - total_loss_svhn: 0.6305\n",
      "epoch:48, accruracy in testing dataset(1000 labeled): 0.6988322065150584\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6169 - total_loss_svhn: 0.6169\n",
      "epoch:49, accruracy in testing dataset(1000 labeled): 0.672979409956976\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6250 - total_loss_svhn: 0.6250\n",
      "epoch:50, accruracy in testing dataset(1000 labeled): 0.6881146281499693\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5980 - total_loss_svhn: 0.5980\n",
      "epoch:51, accruracy in testing dataset(1000 labeled): 0.7012138905961893\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5938 - total_loss_svhn: 0.5938\n",
      "epoch:52, accruracy in testing dataset(1000 labeled): 0.6440534726490473\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6028 - total_loss_svhn: 0.6028\n",
      "epoch:53, accruracy in testing dataset(1000 labeled): 0.6848878303626306\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.6066 - total_loss_svhn: 0.6066\n",
      "epoch:54, accruracy in testing dataset(1000 labeled): 0.6568070067609096\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5906 - total_loss_svhn: 0.5906\n",
      "epoch:55, accruracy in testing dataset(1000 labeled): 0.6947218807621389\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5913 - total_loss_svhn: 0.5913\n",
      "epoch:56, accruracy in testing dataset(1000 labeled): 0.6885371850030731\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5762 - total_loss_svhn: 0.5762\n",
      "epoch:57, accruracy in testing dataset(1000 labeled): 0.7236862323294407\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5724 - total_loss_svhn: 0.5724\n",
      "epoch:58, accruracy in testing dataset(1000 labeled): 0.7145820528580209\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5622 - total_loss_svhn: 0.5622\n",
      "epoch:59, accruracy in testing dataset(1000 labeled): 0.7254917025199754\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5527 - total_loss_svhn: 0.5527\n",
      "epoch:60, accruracy in testing dataset(1000 labeled): 0.7218423478795328\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5598 - total_loss_svhn: 0.5598\n",
      "epoch:61, accruracy in testing dataset(1000 labeled): 0.7161954517516902\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5547 - total_loss_svhn: 0.5547\n",
      "epoch:62, accruracy in testing dataset(1000 labeled): 0.7183850645359557\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5442 - total_loss_svhn: 0.5442\n",
      "epoch:63, accruracy in testing dataset(1000 labeled): 0.72480024585126\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5523 - total_loss_svhn: 0.5523\n",
      "epoch:64, accruracy in testing dataset(1000 labeled): 0.7266441303011678\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5450 - total_loss_svhn: 0.5450\n",
      "epoch:65, accruracy in testing dataset(1000 labeled): 0.71269975414874\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5473 - total_loss_svhn: 0.5473\n",
      "epoch:66, accruracy in testing dataset(1000 labeled): 0.7199216349108789\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5461 - total_loss_svhn: 0.5461\n",
      "epoch:67, accruracy in testing dataset(1000 labeled): 0.7138905961893055\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5510 - total_loss_svhn: 0.5510\n",
      "epoch:68, accruracy in testing dataset(1000 labeled): 0.7162722802704364\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5404 - total_loss_svhn: 0.5404\n",
      "epoch:69, accruracy in testing dataset(1000 labeled): 0.6744775660725261\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5359 - total_loss_svhn: 0.5359\n",
      "epoch:70, accruracy in testing dataset(1000 labeled): 0.7271435156730178\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5261 - total_loss_svhn: 0.5261\n",
      "epoch:71, accruracy in testing dataset(1000 labeled): 0.7285648432698217\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5330 - total_loss_svhn: 0.5330\n",
      "epoch:72, accruracy in testing dataset(1000 labeled): 0.7231868469575906\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5260 - total_loss_svhn: 0.5260\n",
      "epoch:1, accruracy in testing dataset(1000 labeled): 0.7256069452980947\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5096 - total_loss_svhn: 0.5096\n",
      "epoch:2, accruracy in testing dataset(1000 labeled): 0.7217271051014137\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5273 - total_loss_svhn: 0.5273\n",
      "epoch:3, accruracy in testing dataset(1000 labeled): 0.727719729563614\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5148 - total_loss_svhn: 0.5148\n",
      "epoch:4, accruracy in testing dataset(1000 labeled): 0.722841118623233\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5203 - total_loss_svhn: 0.5203\n",
      "epoch:5, accruracy in testing dataset(1000 labeled): 0.7103564843269822\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5274 - total_loss_svhn: 0.5274\n",
      "epoch:6, accruracy in testing dataset(1000 labeled): 0.7281038721573448\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5187 - total_loss_svhn: 0.5187\n",
      "epoch:7, accruracy in testing dataset(1000 labeled): 0.72430086047941\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5196 - total_loss_svhn: 0.5196\n",
      "epoch:8, accruracy in testing dataset(1000 labeled): 0.7277581438229871\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5160 - total_loss_svhn: 0.5160\n",
      "epoch:9, accruracy in testing dataset(1000 labeled): 0.7214966195451752\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5185 - total_loss_svhn: 0.5185\n",
      "epoch:10, accruracy in testing dataset(1000 labeled): 0.7269130301167793\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5118 - total_loss_svhn: 0.5118\n",
      "epoch:11, accruracy in testing dataset(1000 labeled): 0.729179471419791\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5116 - total_loss_svhn: 0.5116\n",
      "epoch:12, accruracy in testing dataset(1000 labeled): 0.7306007990165949\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5034 - total_loss_svhn: 0.5034\n",
      "epoch:13, accruracy in testing dataset(1000 labeled): 0.718999692685925\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5103 - total_loss_svhn: 0.5103\n",
      "epoch:14, accruracy in testing dataset(1000 labeled): 0.7282191149354641\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5009 - total_loss_svhn: 0.5009\n",
      "epoch:15, accruracy in testing dataset(1000 labeled): 0.7268362015980332\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5174 - total_loss_svhn: 0.5174\n",
      "epoch:16, accruracy in testing dataset(1000 labeled): 0.7267209588199139\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4988 - total_loss_svhn: 0.4988\n",
      "epoch:17, accruracy in testing dataset(1000 labeled): 0.7291026429010449\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5007 - total_loss_svhn: 0.5007\n",
      "epoch:18, accruracy in testing dataset(1000 labeled): 0.7252228027043639\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5042 - total_loss_svhn: 0.5042\n",
      "epoch:19, accruracy in testing dataset(1000 labeled): 0.72430086047941\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5016 - total_loss_svhn: 0.5016\n",
      "epoch:20, accruracy in testing dataset(1000 labeled): 0.7285264290104487\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5036 - total_loss_svhn: 0.5036\n",
      "epoch:21, accruracy in testing dataset(1000 labeled): 0.7306776275353412\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5019 - total_loss_svhn: 0.5019\n",
      "epoch:22, accruracy in testing dataset(1000 labeled): 0.7320221266133989\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4966 - total_loss_svhn: 0.4966\n",
      "epoch:23, accruracy in testing dataset(1000 labeled): 0.7328288260602336\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5020 - total_loss_svhn: 0.5020\n",
      "epoch:24, accruracy in testing dataset(1000 labeled): 0.7316379840196681\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5039 - total_loss_svhn: 0.5039\n",
      "epoch:25, accruracy in testing dataset(1000 labeled): 0.7253764597418562\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4999 - total_loss_svhn: 0.4999\n",
      "epoch:26, accruracy in testing dataset(1000 labeled): 0.7309465273509527\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4984 - total_loss_svhn: 0.4984\n",
      "epoch:27, accruracy in testing dataset(1000 labeled): 0.7310233558696988\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5008 - total_loss_svhn: 0.5008\n",
      "epoch:28, accruracy in testing dataset(1000 labeled): 0.7321373693915181\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5004 - total_loss_svhn: 0.5004\n",
      "epoch:29, accruracy in testing dataset(1000 labeled): 0.7314843269821758\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4959 - total_loss_svhn: 0.4959\n",
      "epoch:30, accruracy in testing dataset(1000 labeled): 0.7341349108789182\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4971 - total_loss_svhn: 0.4971\n",
      "epoch:31, accruracy in testing dataset(1000 labeled): 0.7258374308543332\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5008 - total_loss_svhn: 0.5008\n",
      "epoch:32, accruracy in testing dataset(1000 labeled): 0.7321757836508912\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.5009 - total_loss_svhn: 0.5009\n",
      "epoch:33, accruracy in testing dataset(1000 labeled): 0.7299861708666256\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4991 - total_loss_svhn: 0.4991\n",
      "epoch:34, accruracy in testing dataset(1000 labeled): 0.7324830977258758\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4932 - total_loss_svhn: 0.4932\n",
      "epoch:35, accruracy in testing dataset(1000 labeled): 0.7346727105101414\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4914 - total_loss_svhn: 0.4914\n",
      "epoch:36, accruracy in testing dataset(1000 labeled): 0.7313690842040566\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4942 - total_loss_svhn: 0.4942\n",
      "epoch:37, accruracy in testing dataset(1000 labeled): 0.7350568531038721\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4997 - total_loss_svhn: 0.4997\n",
      "epoch:38, accruracy in testing dataset(1000 labeled): 0.7329824830977258\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4887 - total_loss_svhn: 0.4887\n",
      "epoch:39, accruracy in testing dataset(1000 labeled): 0.7330977258758451\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4949 - total_loss_svhn: 0.4949\n",
      "epoch:40, accruracy in testing dataset(1000 labeled): 0.7336739397664414\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5000 - total_loss_svhn: 0.5000\n",
      "epoch:41, accruracy in testing dataset(1000 labeled): 0.7373617086662569\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4886 - total_loss_svhn: 0.4886\n",
      "epoch:42, accruracy in testing dataset(1000 labeled): 0.7323294406883836\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4893 - total_loss_svhn: 0.4893\n",
      "epoch:43, accruracy in testing dataset(1000 labeled): 0.7308312845728334\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4908 - total_loss_svhn: 0.4908\n",
      "epoch:44, accruracy in testing dataset(1000 labeled): 0.7338660110633067\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4869 - total_loss_svhn: 0.4869\n",
      "epoch:45, accruracy in testing dataset(1000 labeled): 0.7344422249539029\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4920 - total_loss_svhn: 0.4920\n",
      "epoch:46, accruracy in testing dataset(1000 labeled): 0.7220728334357713\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4928 - total_loss_svhn: 0.4928\n",
      "epoch:47, accruracy in testing dataset(1000 labeled): 0.7352873386601106\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4919 - total_loss_svhn: 0.4919\n",
      "epoch:48, accruracy in testing dataset(1000 labeled): 0.7323294406883836\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4907 - total_loss_svhn: 0.4907\n",
      "epoch:49, accruracy in testing dataset(1000 labeled): 0.7330593116164721\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4814 - total_loss_svhn: 0.4814\n",
      "epoch:50, accruracy in testing dataset(1000 labeled): 0.7354409956976029\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4895 - total_loss_svhn: 0.4895\n",
      "epoch:51, accruracy in testing dataset(1000 labeled): 0.7383220651505839\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4835 - total_loss_svhn: 0.4835\n",
      "epoch:52, accruracy in testing dataset(1000 labeled): 0.7421250768285187\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4935 - total_loss_svhn: 0.4935\n",
      "epoch:53, accruracy in testing dataset(1000 labeled): 0.7362092808850645\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4865 - total_loss_svhn: 0.4865\n",
      "epoch:54, accruracy in testing dataset(1000 labeled): 0.7357098955132145\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4905 - total_loss_svhn: 0.4905\n",
      "epoch:55, accruracy in testing dataset(1000 labeled): 0.7390519360786724\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4866 - total_loss_svhn: 0.4866\n",
      "epoch:56, accruracy in testing dataset(1000 labeled): 0.7375153657037492\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4844 - total_loss_svhn: 0.4844\n",
      "epoch:57, accruracy in testing dataset(1000 labeled): 0.7380915795943455\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4860 - total_loss_svhn: 0.4860\n",
      "epoch:58, accruracy in testing dataset(1000 labeled): 0.7377074370006146\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4808 - total_loss_svhn: 0.4808\n",
      "epoch:59, accruracy in testing dataset(1000 labeled): 0.7332129686539643\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4900 - total_loss_svhn: 0.4900\n",
      "epoch:60, accruracy in testing dataset(1000 labeled): 0.7390519360786724\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4790 - total_loss_svhn: 0.4790\n",
      "epoch:61, accruracy in testing dataset(1000 labeled): 0.7400891210817455\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4849 - total_loss_svhn: 0.4849\n",
      "epoch:62, accruracy in testing dataset(1000 labeled): 0.7356330669944684\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4834 - total_loss_svhn: 0.4834\n",
      "epoch:63, accruracy in testing dataset(1000 labeled): 0.7372848801475107\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4858 - total_loss_svhn: 0.4858\n",
      "epoch:64, accruracy in testing dataset(1000 labeled): 0.7357483097725875\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4794 - total_loss_svhn: 0.4794\n",
      "epoch:65, accruracy in testing dataset(1000 labeled): 0.7384373079287031\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4819 - total_loss_svhn: 0.4819\n",
      "epoch:66, accruracy in testing dataset(1000 labeled): 0.7298709280885064\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4888 - total_loss_svhn: 0.4888\n",
      "epoch:67, accruracy in testing dataset(1000 labeled): 0.7351336816226183\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4822 - total_loss_svhn: 0.4822\n",
      "epoch:68, accruracy in testing dataset(1000 labeled): 0.7380147510755992\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4815 - total_loss_svhn: 0.4815\n",
      "epoch:69, accruracy in testing dataset(1000 labeled): 0.739320835894284\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4810 - total_loss_svhn: 0.4810\n",
      "epoch:70, accruracy in testing dataset(1000 labeled): 0.7396281499692686\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4792 - total_loss_svhn: 0.4792\n",
      "epoch:71, accruracy in testing dataset(1000 labeled): 0.728680086047941\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4856 - total_loss_svhn: 0.4856\n",
      "epoch:72, accruracy in testing dataset(1000 labeled): 0.740319606637984\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4762 - total_loss_svhn: 0.4762\n",
      "epoch:1, accruracy in testing dataset(1000 labeled): 0.7400122925629994\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4821 - total_loss_svhn: 0.4821\n",
      "epoch:2, accruracy in testing dataset(1000 labeled): 0.7385141364474493\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4846 - total_loss_svhn: 0.4846\n",
      "epoch:3, accruracy in testing dataset(1000 labeled): 0.7433927473878303\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4760 - total_loss_svhn: 0.4760\n",
      "epoch:4, accruracy in testing dataset(1000 labeled): 0.7400507068223725\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4768 - total_loss_svhn: 0.4768\n",
      "epoch:5, accruracy in testing dataset(1000 labeled): 0.7400122925629994\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4825 - total_loss_svhn: 0.4825\n",
      "epoch:6, accruracy in testing dataset(1000 labeled): 0.7357483097725875\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4778 - total_loss_svhn: 0.4778\n",
      "epoch:7, accruracy in testing dataset(1000 labeled): 0.7411263060848187\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4824 - total_loss_svhn: 0.4824\n",
      "epoch:8, accruracy in testing dataset(1000 labeled): 0.734480639213276\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4909 - total_loss_svhn: 0.4909\n",
      "epoch:9, accruracy in testing dataset(1000 labeled): 0.7334818684695759\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4785 - total_loss_svhn: 0.4785\n",
      "epoch:10, accruracy in testing dataset(1000 labeled): 0.7380531653349723\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4790 - total_loss_svhn: 0.4790\n",
      "epoch:11, accruracy in testing dataset(1000 labeled): 0.7358635525507068\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4858 - total_loss_svhn: 0.4858\n",
      "epoch:12, accruracy in testing dataset(1000 labeled): 0.7146972956361402\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4871 - total_loss_svhn: 0.4871\n",
      "epoch:13, accruracy in testing dataset(1000 labeled): 0.7392824216349109\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4798 - total_loss_svhn: 0.4798\n",
      "epoch:14, accruracy in testing dataset(1000 labeled): 0.7395513214505224\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4817 - total_loss_svhn: 0.4817\n",
      "epoch:15, accruracy in testing dataset(1000 labeled): 0.7371696373693916\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4765 - total_loss_svhn: 0.4765\n",
      "epoch:16, accruracy in testing dataset(1000 labeled): 0.7390903503380455\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4758 - total_loss_svhn: 0.4758\n",
      "epoch:17, accruracy in testing dataset(1000 labeled): 0.738859864781807\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4728 - total_loss_svhn: 0.4728\n",
      "epoch:18, accruracy in testing dataset(1000 labeled): 0.7380147510755992\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4746 - total_loss_svhn: 0.4746\n",
      "epoch:19, accruracy in testing dataset(1000 labeled): 0.7423171481253842\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4776 - total_loss_svhn: 0.4776\n",
      "epoch:20, accruracy in testing dataset(1000 labeled): 0.7435464044253227\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4695 - total_loss_svhn: 0.4695\n",
      "epoch:21, accruracy in testing dataset(1000 labeled): 0.7368239090350338\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4762 - total_loss_svhn: 0.4762\n",
      "epoch:22, accruracy in testing dataset(1000 labeled): 0.741779348494161\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4786 - total_loss_svhn: 0.4786\n",
      "epoch:23, accruracy in testing dataset(1000 labeled): 0.7379379225568531\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4771 - total_loss_svhn: 0.4771\n",
      "epoch:24, accruracy in testing dataset(1000 labeled): 0.7396281499692686\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4831 - total_loss_svhn: 0.4831\n",
      "epoch:25, accruracy in testing dataset(1000 labeled): 0.7397818070067609\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4767 - total_loss_svhn: 0.4767\n",
      "epoch:26, accruracy in testing dataset(1000 labeled): 0.7413567916410572\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4751 - total_loss_svhn: 0.4751\n",
      "epoch:27, accruracy in testing dataset(1000 labeled): 0.7445835894283959\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4770 - total_loss_svhn: 0.4770\n",
      "epoch:28, accruracy in testing dataset(1000 labeled): 0.7457360172095882\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4782 - total_loss_svhn: 0.4782\n",
      "epoch:29, accruracy in testing dataset(1000 labeled): 0.7449293177627535\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4655 - total_loss_svhn: 0.4655\n",
      "epoch:30, accruracy in testing dataset(1000 labeled): 0.7449677320221266\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4768 - total_loss_svhn: 0.4768\n",
      "epoch:31, accruracy in testing dataset(1000 labeled): 0.7447372464658881\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4809 - total_loss_svhn: 0.4809\n",
      "epoch:32, accruracy in testing dataset(1000 labeled): 0.7427781192378611\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4767 - total_loss_svhn: 0.4767\n",
      "epoch:33, accruracy in testing dataset(1000 labeled): 0.7402427781192379\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4725 - total_loss_svhn: 0.4725\n",
      "epoch:34, accruracy in testing dataset(1000 labeled): 0.743200676090965\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4776 - total_loss_svhn: 0.4776\n",
      "epoch:35, accruracy in testing dataset(1000 labeled): 0.7430470190534727\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4765 - total_loss_svhn: 0.4765\n",
      "epoch:36, accruracy in testing dataset(1000 labeled): 0.7436232329440688\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4716 - total_loss_svhn: 0.4716\n",
      "epoch:37, accruracy in testing dataset(1000 labeled): 0.7419330055316533\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4698 - total_loss_svhn: 0.4698\n",
      "epoch:38, accruracy in testing dataset(1000 labeled): 0.7440457897971727\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4709 - total_loss_svhn: 0.4709\n",
      "epoch:39, accruracy in testing dataset(1000 labeled): 0.740319606637984\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4705 - total_loss_svhn: 0.4705\n",
      "epoch:40, accruracy in testing dataset(1000 labeled): 0.735978795328826\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4723 - total_loss_svhn: 0.4723\n",
      "epoch:41, accruracy in testing dataset(1000 labeled): 0.744199446834665\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4769 - total_loss_svhn: 0.4769\n",
      "epoch:42, accruracy in testing dataset(1000 labeled): 0.7434695759065765\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4758 - total_loss_svhn: 0.4758\n",
      "epoch:43, accruracy in testing dataset(1000 labeled): 0.7439689612784266\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4729 - total_loss_svhn: 0.4729\n",
      "epoch:44, accruracy in testing dataset(1000 labeled): 0.743661647203442\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4778 - total_loss_svhn: 0.4778\n",
      "epoch:45, accruracy in testing dataset(1000 labeled): 0.7427397049784881\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4730 - total_loss_svhn: 0.4730\n",
      "epoch:46, accruracy in testing dataset(1000 labeled): 0.7428933620159803\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4727 - total_loss_svhn: 0.4727\n",
      "epoch:47, accruracy in testing dataset(1000 labeled): 0.7451982175783651\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4739 - total_loss_svhn: 0.4739\n",
      "epoch:48, accruracy in testing dataset(1000 labeled): 0.7416641057160418\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4727 - total_loss_svhn: 0.4727\n",
      "epoch:49, accruracy in testing dataset(1000 labeled): 0.7415872771972957\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4649 - total_loss_svhn: 0.4649\n",
      "epoch:50, accruracy in testing dataset(1000 labeled): 0.7427397049784881\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4754 - total_loss_svhn: 0.4754\n",
      "epoch:51, accruracy in testing dataset(1000 labeled): 0.7428549477566072\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4687 - total_loss_svhn: 0.4687\n",
      "epoch:52, accruracy in testing dataset(1000 labeled): 0.7410494775660725\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4727 - total_loss_svhn: 0.4727\n",
      "epoch:53, accruracy in testing dataset(1000 labeled): 0.7399354640442533\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4703 - total_loss_svhn: 0.4703\n",
      "epoch:54, accruracy in testing dataset(1000 labeled): 0.7343653964351567\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4728 - total_loss_svhn: 0.4728\n",
      "epoch:55, accruracy in testing dataset(1000 labeled): 0.7366318377381684\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4714 - total_loss_svhn: 0.4714\n",
      "epoch:56, accruracy in testing dataset(1000 labeled): 0.7429317762753535\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4742 - total_loss_svhn: 0.4742\n",
      "epoch:57, accruracy in testing dataset(1000 labeled): 0.7419330055316533\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4674 - total_loss_svhn: 0.4674\n",
      "epoch:58, accruracy in testing dataset(1000 labeled): 0.7419330055316533\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4669 - total_loss_svhn: 0.4669\n",
      "epoch:59, accruracy in testing dataset(1000 labeled): 0.7431622618315918\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4729 - total_loss_svhn: 0.4729\n",
      "epoch:60, accruracy in testing dataset(1000 labeled): 0.7440842040565457\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4693 - total_loss_svhn: 0.4693\n",
      "epoch:61, accruracy in testing dataset(1000 labeled): 0.7383220651505839\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4677 - total_loss_svhn: 0.4677\n",
      "epoch:62, accruracy in testing dataset(1000 labeled): 0.7405885064535955\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4713 - total_loss_svhn: 0.4713\n",
      "epoch:63, accruracy in testing dataset(1000 labeled): 0.7456976029502151\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4712 - total_loss_svhn: 0.4712\n",
      "epoch:64, accruracy in testing dataset(1000 labeled): 0.7461969883220652\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4670 - total_loss_svhn: 0.4670\n",
      "epoch:65, accruracy in testing dataset(1000 labeled): 0.7420098340503996\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4652 - total_loss_svhn: 0.4652\n",
      "epoch:66, accruracy in testing dataset(1000 labeled): 0.7423171481253842\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4721 - total_loss_svhn: 0.4721\n",
      "epoch:67, accruracy in testing dataset(1000 labeled): 0.7440073755377996\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4650 - total_loss_svhn: 0.4650\n",
      "epoch:68, accruracy in testing dataset(1000 labeled): 0.7463122311001844\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4690 - total_loss_svhn: 0.4690\n",
      "epoch:69, accruracy in testing dataset(1000 labeled): 0.7456976029502151\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4664 - total_loss_svhn: 0.4664\n",
      "epoch:70, accruracy in testing dataset(1000 labeled): 0.7444683466502766\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4685 - total_loss_svhn: 0.4685\n",
      "epoch:71, accruracy in testing dataset(1000 labeled): 0.7437768899815611\n",
      "Epoch 1/1\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4714 - total_loss_svhn: 0.4714\n",
      "epoch:72, accruracy in testing dataset(1000 labeled): 0.7416641057160418\n"
     ]
    }
   ],
   "source": [
    "model_vat_svhn_1000_acc_trace=[]\n",
    "model_vat_svhn_1000_loss_trace=[]\n",
    "model_vat_svhn_1000=vat_svhn_1(epsilon=10,alpha=1)\n",
    "for j in range(3):\n",
    "    for i in range(1000,72256,1000):\n",
    "        model_vat_svhn_1000.fit([train_greyscale[i:i+1000],train_greyscale_1000label,svhn_train_label_onehot_1000label], None,epochs=1 )\n",
    "        y_pred_svhn = model_vat_svhn_1000.predict( [test_greyscale,test_greyscale,svhn_test_label_onehot_useless] ).argmax(-1)\n",
    "        acc=accuracy_score(svhn_test_label , y_pred_svhn)\n",
    "        print(f\"epoch:{i//1000}, accruracy in testing dataset(1000 labeled): {acc}\")\n",
    "        model_vat_svhn_1000_acc_trace.append(acc)\n",
    "        model_vat_svhn_1000_loss_trace.append(model_vat_svhn_1000.history.history['total_loss_svhn'][-1])\n",
    "#del model_vat_svhn_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between our VAT performance and authoritative sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying that, we are confused because the accuracy of VAT-based model in 200 labeled MNIST data can only reach 93%. We want to explore the limit of VAT. We want to know whether our VAT-based model is good enough. In order to explore the reasons why there is a difference between our experiments and authoritative sources, we use another program collected from github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using these codes, we change the number of labeled data in online program. We use 240 labeled data in online program (where in our model we use 200 labeled data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled data:(200, 28, 28, 1)\n",
      "label:(200, 10)\n",
      "unlabeled data:(59800, 28, 28, 1)\n",
      "epoch 1/5: sup_loss: 2.6832541232446023 sup_acc: 0.147048982869379 unsup_loss: 1.6496170908970806e-05 \n",
      "epoch 2/5: sup_loss: 4.705180652646998 sup_acc: 0.12372858672376874 unsup_loss: 0.0001703039551058054 \n",
      "epoch 3/5: sup_loss: 6.096926467576935 sup_acc: 0.20001338329764454 unsup_loss: 0.0063674367143003755 \n",
      "epoch 4/5: sup_loss: 1.7049837243250656 sup_acc: 0.6603653640256959 unsup_loss: 0.013867899823367467 \n",
      "epoch 5/5: sup_loss: 0.4253772716386926 sup_acc: 0.853001204496788 unsup_loss: 0.0071618772997786104 \n",
      "10000/10000 [==============================] - 1s 141us/step\n",
      "[1.433732829761505, 0.6443]\n",
      "epoch 1/5: sup_loss: 0.3038772919576224 sup_acc: 0.9022851980728052 unsup_loss: 0.007884932496949902 \n",
      "epoch 2/5: sup_loss: 0.2197537413405402 sup_acc: 0.935241568522484 unsup_loss: 0.008420286076921147 \n",
      "epoch 3/5: sup_loss: 0.18846991120556436 sup_acc: 0.9423347162740899 unsup_loss: 0.01307891381703062 \n",
      "epoch 4/5: sup_loss: 0.1334272559883763 sup_acc: 0.9676960653104925 unsup_loss: 0.012589672348029289 \n",
      "epoch 5/5: sup_loss: 0.11143792009806736 sup_acc: 0.979490096359743 unsup_loss: 0.012506494655551058 \n",
      "10000/10000 [==============================] - 1s 85us/step\n",
      "[1.2088113548994064, 0.7291]\n",
      "epoch 1/5: sup_loss: 0.07767299744569771 sup_acc: 0.9929403104925053 unsup_loss: 0.011568991027122296 \n",
      "epoch 2/5: sup_loss: 0.060842939670450936 sup_acc: 0.9954664079229122 unsup_loss: 0.011391629358231065 \n",
      "epoch 3/5: sup_loss: 0.05508781732229528 sup_acc: 0.9960184689507494 unsup_loss: 0.01139133287956679 \n",
      "epoch 4/5: sup_loss: 0.04205619322028354 sup_acc: 0.9975742773019272 unsup_loss: 0.013830405334409573 \n",
      "epoch 5/5: sup_loss: 0.03491916628335094 sup_acc: 0.9988456905781584 unsup_loss: 0.01489183727783456 \n",
      "10000/10000 [==============================] - 1s 86us/step\n",
      "[1.171796832537651, 0.7565]\n",
      "epoch 1/5: sup_loss: 0.029139559837636525 sup_acc: 0.9990129817987152 unsup_loss: 0.015460618514352233 \n",
      "epoch 2/5: sup_loss: 0.022957898849896623 sup_acc: 0.9998494379014989 unsup_loss: 0.01554444326263328 \n",
      "epoch 3/5: sup_loss: 0.018663939952100497 sup_acc: 0.9999163543897216 unsup_loss: 0.016015163935896415 \n",
      "epoch 4/5: sup_loss: 0.015996178490462726 sup_acc: 0.9997992505353319 unsup_loss: 0.017010301545489683 \n",
      "epoch 5/5: sup_loss: 0.011724510494364342 sup_acc: 0.9999330835117773 unsup_loss: 0.016917378346014936 \n",
      "10000/10000 [==============================] - 1s 86us/step\n",
      "[1.0773654196262359, 0.7761]\n",
      "epoch 1/5: sup_loss: 0.010463173908023942 sup_acc: 0.9999832708779444 unsup_loss: 0.019258804244390137 \n",
      "epoch 2/5: sup_loss: 0.008503056282867584 sup_acc: 1.0 unsup_loss: 0.018537880367214533 \n",
      "epoch 3/5: sup_loss: 0.00735326301470016 sup_acc: 1.0 unsup_loss: 0.021283246433198833 \n",
      "epoch 4/5: sup_loss: 0.005635628499672998 sup_acc: 1.0 unsup_loss: 0.021696101387511994 \n",
      "epoch 5/5: sup_loss: 0.0053203925555193 sup_acc: 1.0 unsup_loss: 0.02356986201426106 \n",
      "10000/10000 [==============================] - 1s 82us/step\n",
      "[1.1014491821289063, 0.7831]\n",
      "epoch 1/5: sup_loss: 0.0038011412109334093 sup_acc: 1.0 unsup_loss: 0.023394949899735108 \n",
      "epoch 2/5: sup_loss: 0.003174016283071207 sup_acc: 1.0 unsup_loss: 0.024618073257894856 \n",
      "epoch 3/5: sup_loss: 0.0024190299343557057 sup_acc: 1.0 unsup_loss: 0.02591009695697137 \n",
      "epoch 4/5: sup_loss: 0.002111788095614217 sup_acc: 1.0 unsup_loss: 0.028026311859743754 \n",
      "epoch 5/5: sup_loss: 0.0016929387268002048 sup_acc: 1.0 unsup_loss: 0.027933783103299088 \n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "[0.9814356377631426, 0.8048]\n",
      "epoch 1/5: sup_loss: 0.0014559065403837543 sup_acc: 1.0 unsup_loss: 0.028890094292056957 \n",
      "epoch 2/5: sup_loss: 0.0011531935463314456 sup_acc: 1.0 unsup_loss: 0.03150198874273188 \n",
      "epoch 3/5: sup_loss: 0.0010099567812458638 sup_acc: 1.0 unsup_loss: 0.03201315935493728 \n",
      "epoch 4/5: sup_loss: 0.0008025198354240212 sup_acc: 1.0 unsup_loss: 0.03387705672221215 \n",
      "epoch 5/5: sup_loss: 0.0005975698583437294 sup_acc: 1.0 unsup_loss: 0.03345112642771873 \n",
      "10000/10000 [==============================] - 1s 84us/step\n",
      "[1.0107229563400149, 0.808]\n",
      "epoch 1/5: sup_loss: 0.0004888947644795449 sup_acc: 1.0 unsup_loss: 0.03708988163077793 \n",
      "epoch 2/5: sup_loss: 0.0003905877128766443 sup_acc: 1.0 unsup_loss: 0.03741161542898742 \n",
      "epoch 3/5: sup_loss: 0.00031053565880179016 sup_acc: 1.0 unsup_loss: 0.039507805407685434 \n",
      "epoch 4/5: sup_loss: 0.00024592773299574516 sup_acc: 1.0 unsup_loss: 0.04164178048238573 \n",
      "epoch 5/5: sup_loss: 0.00019700621803851032 sup_acc: 1.0 unsup_loss: 0.042346730018612916 \n",
      "10000/10000 [==============================] - 1s 87us/step\n",
      "[0.9714243233188987, 0.8236]\n",
      "epoch 1/5: sup_loss: 0.0001577239126527664 sup_acc: 1.0 unsup_loss: 0.04432293392191352 \n",
      "epoch 2/5: sup_loss: 0.00011831109479289828 sup_acc: 1.0 unsup_loss: 0.04378557689208737 \n",
      "epoch 3/5: sup_loss: 9.257037199313735e-05 sup_acc: 1.0 unsup_loss: 0.04627617193701053 \n",
      "epoch 4/5: sup_loss: 7.581612847854355e-05 sup_acc: 1.0 unsup_loss: 0.046585259713806865 \n",
      "epoch 5/5: sup_loss: 6.51970007023086e-05 sup_acc: 1.0 unsup_loss: 0.049500775196424006 \n",
      "10000/10000 [==============================] - 1s 84us/step\n",
      "[0.9303062978412956, 0.8408]\n",
      "epoch 1/5: sup_loss: 4.657233894756014e-05 sup_acc: 1.0 unsup_loss: 0.05215805953934916 \n",
      "epoch 2/5: sup_loss: 3.964975072581738e-05 sup_acc: 1.0 unsup_loss: 0.054093063631487866 \n",
      "epoch 3/5: sup_loss: 3.2201822176493396e-05 sup_acc: 1.0 unsup_loss: 0.05366287596834867 \n",
      "epoch 4/5: sup_loss: 2.503424611878848e-05 sup_acc: 1.0 unsup_loss: 0.0572894885956571 \n",
      "epoch 5/5: sup_loss: 1.9751563217126086e-05 sup_acc: 1.0 unsup_loss: 0.05564058932417912 \n",
      "10000/10000 [==============================] - 1s 87us/step\n",
      "[0.977445369467698, 0.8398]\n"
     ]
    }
   ],
   "source": [
    "#NOTE: codes in this cell are collected from online resource\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "import numpy\n",
    "\n",
    "from model import VATModel, SemiSupervisedVATModel\n",
    "\n",
    "__author__ = 'Romain Tavenard romain.tavenard[at]univ-rennes2.fr'\n",
    "\n",
    "n_filters = 5\n",
    "kernel_size = 5\n",
    "pool_size = 2\n",
    "n_classes = 10\n",
    "\n",
    "model = Sequential([\n",
    "    Convolution2D(n_filters,\n",
    "                  kernel_size,\n",
    "                  activation='relu',\n",
    "                  input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')[:, :, :, None]\n",
    "X_test = X_test.astype('float32')[:, :, :, None]\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)\n",
    "\n",
    "# # Fully supervised training: do not use VATModel\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "# Semi-supervised training: define a model with two inputs/outputs/losses, one\n",
    "# for supervised data, one for unsupervised data\n",
    "indices_supervised = numpy.random.randint(low=0, high=X_train.shape[0],\n",
    "                                          size=X_train.shape[0] // 300)#we change size here\n",
    "indices_unsupervised = numpy.isin(numpy.arange(X_train.shape[0]),\n",
    "                                  indices_supervised,\n",
    "                                  invert=True)\n",
    "\n",
    "X_train_sup = X_train[indices_supervised]\n",
    "y_train_sup = y_train[indices_supervised]\n",
    "X_train_unsup = X_train[indices_unsupervised]\n",
    "\n",
    "#we print shapes of labeled and unlabeled data\n",
    "print(f'labeled data:{X_train_sup.shape}\\n\\\n",
    "label:{y_train_sup.shape}\\n\\\n",
    "unlabeled data:{X_train_unsup.shape}')\n",
    "\n",
    "model3 = SemiSupervisedVATModel(model=model)\n",
    "model3.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "               metrics=[\"accuracy\"])\n",
    "for i in range(10):\n",
    "    model3.fit([X_train_sup, X_train_unsup],\n",
    "               [y_train_sup, None],\n",
    "               batch_size=128, epochs=5)\n",
    "    print(model3.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the online resource model can only reach 84% accuracy while our model reach 92% accuracy. We only reach a slightly higher accuracy on MNIST dataset.  \n",
    "Then we are curious whether our VAT-based model can outperform online resource model in a more complex dataset-SVHN.  \n",
    "We also try modifying their model and try this new model on SVHN dataset and compare the results with our our model's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a network model which is the same as the network model used in our VAT-based model. We try replace the original network model in the online codes with this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_svhn_model():\n",
    "    network_svhn = Sequential()\n",
    "    network_svhn.add(keras.layers.Conv2D(filters=32,kernel_size = [5,5],\n",
    "                                  padding = 'valid',activation = tf.nn.relu,input_shape = (32,32,1)))\n",
    "    network_svhn.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = 2))\n",
    "    network_svhn.add(keras.layers.Conv2D(filters=64,kernel_size = [5,5],padding = 'valid',activation = tf.nn.relu))\n",
    "    network_svhn.add(keras.layers.MaxPool2D(pool_size=(2,2), strides =2))\n",
    "    #network_svhn.add(keras.layers.Dropout(0.25))\n",
    "    network_svhn.add(keras.layers.Flatten())\n",
    "    #network_svhn.add(keras.layers.Reshape((-1, 8 * 8 * 64)))\n",
    "    network_svhn.add(keras.layers.Dense(units=256,activation = tf.nn.relu))\n",
    "    #network_svhn.add(keras.layers.Dense(units=10,activation=tf.nn.relu))\n",
    "    network_svhn.add(keras.layers.Dense(units=10,activation=tf.nn.softmax))\n",
    "    return network_svhn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to prepare SVNH dataset for the online resource model. We change the SVHN dataset format to the format online resource model need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "svhn_train=loadmat('train_32x32.mat')\n",
    "svhn_test=loadmat('test_32x32.mat')\n",
    "\n",
    "svhn_train_img3072=svhn_train['X'][:,:,:,:].transpose((3,0,1,2))\n",
    "svhn_test_img3072=svhn_test['X'][:,:,:,:].transpose((3,0,1,2))\n",
    "svhn_train_label=svhn_train['y'][:,0]\n",
    "svhn_test_label=svhn_test['y'][:,0]\n",
    "svhn_train_label[svhn_train_label== 10] = 0#change label 10 to label 0\n",
    "svhn_test_label[svhn_test_label == 10] = 0\n",
    "svhn_test_label_onehot=keras.utils.to_categorical(svhn_test_label)\n",
    "svhn_train_label_onehot=keras.utils.to_categorical(svhn_train_label)\n",
    "del svhn_train,svhn_test\n",
    "\n",
    "train_greyscale = rgb2gray(svhn_train_img3072).astype(np.float32)\n",
    "test_greyscale = rgb2gray(svhn_test_img3072).astype(np.float32)\n",
    "\n",
    "del svhn_train_img3072,svhn_test_img3072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use new network model and train the online resource model on SVHN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled data:(3000, 32, 32, 1)\n",
      "label:(3000, 10)\n",
      "unlabeled data:(70257, 32, 32, 1)\n",
      "epoch 1/5: sup_loss: 2.25038704906937 sup_acc: 0.1897810218978102 unsup_loss: 1.1630813343689747e-06 \n",
      "epoch 2/5: sup_loss: 2.236257762804519 sup_acc: 0.1936730155109489 unsup_loss: 4.6689201196617764e-08 \n",
      "epoch 3/5: sup_loss: 2.234832114111768 sup_acc: 0.19626767791970803 unsup_loss: 3.916680197423524e-08 \n",
      "epoch 4/5: sup_loss: 2.2351362970623656 sup_acc: 0.1930314781021898 unsup_loss: 2.844939584502073e-07 \n",
      "epoch 5/5: sup_loss: 2.2409115002973237 sup_acc: 0.19395814324817517 unsup_loss: 2.2394773846224927e-07 \n",
      "26032/26032 [==============================] - 8s 300us/step\n",
      "[2.2245704789211507, 0.1958743085433313]\n",
      "epoch 1/5: sup_loss: 2.2333236454177077 sup_acc: 0.19300296532846714 unsup_loss: 2.473634096910824e-07 \n",
      "epoch 2/5: sup_loss: 2.2306421120671462 sup_acc: 0.19750798357664234 unsup_loss: 1.0439130487856493e-06 \n",
      "epoch 3/5: sup_loss: 2.2283523052278227 sup_acc: 0.19873403284671534 unsup_loss: 2.4386675462072654e-06 \n",
      "epoch 4/5: sup_loss: 2.2233754435594935 sup_acc: 0.19705177919708028 unsup_loss: 4.597642089058261e-06 \n",
      "epoch 5/5: sup_loss: 2.225962144179936 sup_acc: 0.19842039233576642 unsup_loss: 7.054857324495916e-06 \n",
      "26032/26032 [==============================] - 6s 231us/step\n",
      "[2.2225254843699545, 0.1958743085433313]\n",
      "epoch 1/5: sup_loss: 2.2262078906497815 sup_acc: 0.19636747262773724 unsup_loss: 1.0325950913854955e-05 \n",
      "epoch 2/5: sup_loss: 2.2257268511465864 sup_acc: 0.19662408759124086 unsup_loss: 1.3692160007643654e-05 \n",
      "epoch 3/5: sup_loss: 2.2305548861078974 sup_acc: 0.19478501368613138 unsup_loss: 1.8676321025292838e-05 \n",
      "epoch 4/5: sup_loss: 2.234393990387882 sup_acc: 0.1957116788321168 unsup_loss: 2.2635743069285305e-05 \n",
      "epoch 5/5: sup_loss: 2.2466740473343507 sup_acc: 0.19568316605839417 unsup_loss: 2.6275164071087793e-05 \n",
      "26032/26032 [==============================] - 6s 220us/step\n",
      "[2.24526598326033, 0.1958743085433313]\n",
      "epoch 1/5: sup_loss: 2.248906156442461 sup_acc: 0.19307424726277372 unsup_loss: 3.0028635635467487e-05 \n",
      "epoch 2/5: sup_loss: 2.2469903861519196 sup_acc: 0.1965242928832117 unsup_loss: 3.3211664095110755e-05 \n",
      "epoch 3/5: sup_loss: 2.247994364613164 sup_acc: 0.19578296076642335 unsup_loss: 3.723910889755272e-05 \n",
      "epoch 4/5: sup_loss: 2.2461802911584394 sup_acc: 0.19775034215328466 unsup_loss: 4.250059739710873e-05 \n",
      "epoch 5/5: sup_loss: 2.2548238974418084 sup_acc: 0.19606808850364962 unsup_loss: 4.464389712002297e-05 \n",
      "26032/26032 [==============================] - 6s 225us/step\n",
      "[2.249749969395671, 0.1958743085433313]\n",
      "epoch 1/5: sup_loss: 2.2571463367364704 sup_acc: 0.19598255018248176 unsup_loss: 4.6161956292398425e-05 \n",
      "epoch 2/5: sup_loss: 2.256144051134151 sup_acc: 0.19651003649635038 unsup_loss: 4.299916387079424e-05 \n",
      "epoch 3/5: sup_loss: 2.2630767417650155 sup_acc: 0.19542655109489052 unsup_loss: 5.270700723592007e-05 \n",
      "epoch 4/5: sup_loss: 2.2580467057924203 sup_acc: 0.19542655109489052 unsup_loss: 2.5989315775294684e-05 \n",
      "epoch 5/5: sup_loss: 2.230773811357735 sup_acc: 0.19425752737226276 unsup_loss: 1.398288216763089e-12 \n",
      "26032/26032 [==============================] - 6s 226us/step\n",
      "[2.224874629810242, 0.1958743085433313]\n",
      "epoch 1/5: sup_loss: 2.228190156230091 sup_acc: 0.19512716697080293 unsup_loss: -3.534133118960389e-12 \n",
      "epoch 2/5: sup_loss: 2.2275454267098085 sup_acc: 0.196538549270073 unsup_loss: -2.7849968884175147e-12 \n",
      "epoch 3/5: sup_loss: 2.229246438419732 sup_acc: 0.1952127052919708 unsup_loss: 9.388596955833787e-14 \n",
      "epoch 4/5: sup_loss: 2.227721248229925 sup_acc: 0.19420050182481752 unsup_loss: 1.69496290482337e-12 \n",
      "epoch 5/5: sup_loss: 2.229071222082542 sup_acc: 0.19442860401459855 unsup_loss: -1.93414141150727e-12 \n",
      "26032/26032 [==============================] - 6s 220us/step\n",
      "[2.2252237543271374, 0.1958743085433313]\n",
      "epoch 1/5: sup_loss: 2.227757438690993 sup_acc: 0.19608234489051096 unsup_loss: 1.5332179970073412e-13 \n",
      "epoch 2/5: sup_loss: 2.2274528393780226 sup_acc: 0.19638172901459855 unsup_loss: -1.1962110134550297e-12 \n",
      "epoch 3/5: sup_loss: 2.2297272673488533 sup_acc: 0.19514142335766424 unsup_loss: -6.602050617472417e-13 \n",
      "epoch 4/5: sup_loss: 2.22649978895257 sup_acc: 0.19752223996350365 unsup_loss: 2.407930910625037e-13 \n",
      "epoch 5/5: sup_loss: 2.2280839133436663 sup_acc: 0.1953125 unsup_loss: 4.382947573410103e-12 \n",
      "26032/26032 [==============================] - 6s 222us/step\n",
      "[2.2255786033810905, 0.1958743085433313]\n",
      "epoch 1/5: sup_loss: 2.231835034206836 sup_acc: 0.19348768248175183 unsup_loss: 0.0 \n",
      "epoch 2/5: sup_loss: 2.228218710770572 sup_acc: 0.19464244981751824 unsup_loss: -1.9594569276140326e-12 \n",
      "epoch 3/5: sup_loss: 2.2294995654238403 sup_acc: 0.19430029653284672 unsup_loss: 1.562293481930531e-13 \n",
      "epoch 4/5: sup_loss: 2.227117401840043 sup_acc: 0.19689495894160583 unsup_loss: 1.1344574914510072e-12 \n",
      "epoch 5/5: sup_loss: 2.2273837523738833 sup_acc: 0.19549783302919707 unsup_loss: 0.0 \n",
      "26032/26032 [==============================] - 6s 217us/step\n",
      "[2.224849454156445, 0.1958743085433313]\n",
      "epoch 1/5: sup_loss: 2.2281068354627513 sup_acc: 0.19729413777372262 unsup_loss: -1.7779555200366222e-12 \n",
      "epoch 2/5: sup_loss: 2.2271661340755267 sup_acc: 0.19559762773722628 unsup_loss: -3.916401367848524e-13 \n",
      "epoch 3/5: sup_loss: 2.227688825913589 sup_acc: 0.19746521441605838 unsup_loss: 5.879289350578166e-14 \n",
      "epoch 4/5: sup_loss: 2.2278556832431877 sup_acc: 0.19509865419708028 unsup_loss: 0.0 \n",
      "epoch 5/5: sup_loss: 2.229457038597469 sup_acc: 0.1964387545620438 unsup_loss: 4.951321185980891e-13 \n",
      "26032/26032 [==============================] - 6s 220us/step\n",
      "[2.225924213458661, 0.1958743085433313]\n",
      "epoch 1/5: sup_loss: 2.2300847568651188 sup_acc: 0.19290317062043796 unsup_loss: -1.8950691583695562e-13 \n",
      "epoch 2/5: sup_loss: 2.2330427261164587 sup_acc: 0.192133325729927 unsup_loss: 0.0 \n",
      "epoch 3/5: sup_loss: 2.2313605677472412 sup_acc: 0.19583998631386862 unsup_loss: 0.0 \n",
      "epoch 4/5: sup_loss: 2.230701790238819 sup_acc: 0.19321681113138686 unsup_loss: 0.0 \n",
      "epoch 5/5: sup_loss: 2.2281709417809537 sup_acc: 0.19518419251824817 unsup_loss: 0.0 \n",
      "26032/26032 [==============================] - 6s 219us/step\n",
      "[2.2274679973510856, 0.1958743085433313]\n"
     ]
    }
   ],
   "source": [
    "#NOTE: codes in this cell are collected from online resource, and are modified by us\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "import numpy\n",
    "\n",
    "from model import VATModel, SemiSupervisedVATModel\n",
    "\n",
    "__author__ = 'Romain Tavenard romain.tavenard[at]univ-rennes2.fr'\n",
    "\n",
    "model=net_svhn_model()\n",
    "\n",
    "X_train=train_greyscale.reshape(-1,32,32)\n",
    "X_test=test_greyscale.reshape(-1,32,32)\n",
    "X_train = X_train.astype('float32')[:, :, :, None]\n",
    "X_test = X_test.astype('float32')[:, :, :, None]\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train=to_categorical(svhn_train_label)\n",
    "y_test=to_categorical(svhn_test_label)\n",
    "\n",
    "# # Fully supervised training: do not use VATModel\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Semi-supervised training: define a model with two inputs/outputs/losses, one\n",
    "# for supervised data, one for unsupervised data\n",
    "indices_supervised = numpy.random.randint(low=0, high=X_train.shape[0],\n",
    "                                          size=X_train.shape[0] // 24)\n",
    "indices_unsupervised = numpy.isin(numpy.arange(X_train.shape[0]),\n",
    "                                  indices_supervised,\n",
    "                                  invert=True)\n",
    "X_train=X_train[permutation]\n",
    "y_train=y_train[permutation]\n",
    "#X_train_sup = X_train[indices_supervised]\n",
    "#y_train_sup = y_train[indices_supervised]\n",
    "#X_train_unsup = X_train[indices_unsupervised]\n",
    "X_train_sup=X_train[0:3000]\n",
    "y_train_sup = y_train[0:3000]\n",
    "X_train_unsup = X_train[3000:]\n",
    "\n",
    "#we print shapes of labeled and unlabeled data\n",
    "print(f'labeled data:{X_train_sup.shape}\\n\\\n",
    "label:{y_train_sup.shape}\\n\\\n",
    "unlabeled data:{X_train_unsup.shape}')\n",
    "\n",
    "model3 = SemiSupervisedVATModel(model=model,input_shape=(32, 32, 1))\n",
    "model3.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "               metrics=[\"accuracy\"])\n",
    "for i in range(10):\n",
    "    model3.fit([X_train_sup, X_train_unsup],\n",
    "               [y_train_sup, None],\n",
    "               batch_size=128, epochs=5)\n",
    "    print(model3.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that something strange happened. The model cannot learn. We guess the reason could be something related to the activation function ReLU. As a result, we try another activation function LeakyReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_svhn_model_with_lrelu():\n",
    "    network_svhn = Sequential()\n",
    "    lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\n",
    "    network_svhn.add(keras.layers.Conv2D(filters=32,kernel_size = [5,5],\n",
    "                                  padding = 'valid',activation = lrelu,input_shape = (32,32,1)))\n",
    "    network_svhn.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = 2))\n",
    "    network_svhn.add(keras.layers.Conv2D(filters=64,kernel_size = [5,5],padding = 'valid',activation = lrelu))\n",
    "    network_svhn.add(keras.layers.MaxPool2D(pool_size=(2,2), strides =2))\n",
    "    network_svhn.add(keras.layers.Flatten())\n",
    "    #network_svhn.add(keras.layers.Reshape((-1, 8 * 8 * 64)))\n",
    "    network_svhn.add(keras.layers.Dense(units=256,activation = lrelu))\n",
    "    #network_svhn.add(keras.layers.Dense(units=10,activation=tf.nn.relu))\n",
    "    network_svhn.add(keras.layers.Dense(units=10,activation=tf.nn.softmax))\n",
    "    return network_svhn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled data:(3052, 32, 32, 1)\n",
      "label:(3052, 10)\n",
      "unlabeled data:(70270, 32, 32, 1)\n",
      "epoch 1/5: sup_loss: 2.2386076193656366 sup_acc: 0.1802007299270073 unsup_loss: 1.7707612137243386e-06 \n",
      "epoch 2/5: sup_loss: 2.2400430510513973 sup_acc: 0.18464872262773724 unsup_loss: 0.0003835279454127832 \n",
      "epoch 3/5: sup_loss: 2.2524673912646995 sup_acc: 0.178461450729927 unsup_loss: 0.0006329534772756591 \n",
      "epoch 4/5: sup_loss: 2.2751057065316362 sup_acc: 0.1847200045620438 unsup_loss: 0.001956706206527977 \n",
      "epoch 5/5: sup_loss: 2.500107346224959 sup_acc: 0.16312157846715328 unsup_loss: 0.022342054032186136 \n",
      "26032/26032 [==============================] - 8s 317us/step\n",
      "[2.2537620877955833, 0.18200676090964965]\n",
      "epoch 1/5: sup_loss: 2.2363139760755275 sup_acc: 0.18851220346715328 unsup_loss: 0.0016914591505268698 \n",
      "epoch 2/5: sup_loss: 2.2134174025841875 sup_acc: 0.19742244525547445 unsup_loss: 0.002013315018083744 \n",
      "epoch 3/5: sup_loss: 2.2280981923541883 sup_acc: 0.19343065693430658 unsup_loss: 0.009055983769302903 \n",
      "epoch 4/5: sup_loss: 2.2360443507667878 sup_acc: 0.19097855839416059 unsup_loss: 0.011100619205615286 \n",
      "epoch 5/5: sup_loss: 2.1959552564760196 sup_acc: 0.21199247262773724 unsup_loss: 0.015418373182990391 \n",
      "26032/26032 [==============================] - 6s 244us/step\n",
      "[2.461889742925699, 0.2042102028272895]\n",
      "epoch 1/5: sup_loss: 2.398257518554256 sup_acc: 0.19978900547445255 unsup_loss: 0.055625025900148777 \n",
      "epoch 2/5: sup_loss: 2.335894772171104 sup_acc: 0.1678832116788321 unsup_loss: 0.011176421542407224 \n",
      "epoch 3/5: sup_loss: 2.20942794362994 sup_acc: 0.19837762317518248 unsup_loss: 0.011113568225894191 \n",
      "epoch 4/5: sup_loss: 2.386700331729694 sup_acc: 0.17572422445255476 unsup_loss: 0.03872418872175915 \n",
      "epoch 5/5: sup_loss: 2.474189662150223 sup_acc: 0.18787066605839417 unsup_loss: 0.057395111789300624 \n",
      "26032/26032 [==============================] - 6s 234us/step\n",
      "[2.242525099096081, 0.15972649047326368]\n",
      "epoch 1/5: sup_loss: 2.2343074527100053 sup_acc: 0.19164860857664234 unsup_loss: 0.0040298302816804904 \n",
      "epoch 2/5: sup_loss: 2.337449138181923 sup_acc: 0.19082173813868614 unsup_loss: 0.019957985087845093 \n",
      "epoch 3/5: sup_loss: 2.2351937246148603 sup_acc: 0.18836963959854014 unsup_loss: 0.0037154114177836324 \n",
      "epoch 4/5: sup_loss: 2.2368409155059035 sup_acc: 0.20027372262773724 unsup_loss: 0.019658974099081745 \n",
      "epoch 5/5: sup_loss: 2.37357063602357 sup_acc: 0.18899692062043796 unsup_loss: 0.05225126037234355 \n",
      "26032/26032 [==============================] - 6s 232us/step\n",
      "[2.1429677764257185, 0.2312922556853104]\n",
      "epoch 1/5: sup_loss: 2.2493594095219662 sup_acc: 0.20335310218978103 unsup_loss: 0.04906887869371006 \n",
      "epoch 2/5: sup_loss: 2.153894076599692 sup_acc: 0.2273751140510949 unsup_loss: 0.02610510548433955 \n",
      "epoch 3/5: sup_loss: 2.0491049372366743 sup_acc: 0.3147525091240876 unsup_loss: 0.2729384860237992 \n",
      "epoch 4/5: sup_loss: 1.0443060899520442 sup_acc: 0.681597855839416 unsup_loss: 0.6019264931661369 \n",
      "epoch 5/5: sup_loss: 0.6400560376091595 sup_acc: 0.8164917883211679 unsup_loss: 0.6852131094375666 \n",
      "26032/26032 [==============================] - 6s 242us/step\n",
      "[0.8770035286219339, 0.7318684695759066]\n",
      "epoch 1/5: sup_loss: 0.7231332542252367 sup_acc: 0.7828609717153284 unsup_loss: 0.860264407022156 \n",
      "epoch 2/5: sup_loss: 1.2414989289249816 sup_acc: 0.6664575729927007 unsup_loss: 1.2783481050146757 \n",
      "epoch 3/5: sup_loss: 0.5650760508040442 sup_acc: 0.8380474452554745 unsup_loss: 0.7521825071668973 \n",
      "epoch 4/5: sup_loss: 0.5585270340751557 sup_acc: 0.8440066149635036 unsup_loss: 0.7778533443699788 \n",
      "epoch 5/5: sup_loss: 0.5438053686253346 sup_acc: 0.841754105839416 unsup_loss: 0.9002202486034727 \n",
      "26032/26032 [==============================] - 6s 232us/step\n",
      "[0.9020098465017988, 0.7329824830977258]\n",
      "epoch 1/5: sup_loss: 0.4823852719834251 sup_acc: 0.8569799270072993 unsup_loss: 1.0005224984492698 \n",
      "epoch 2/5: sup_loss: 0.5460050168046116 sup_acc: 0.8279681797445255 unsup_loss: 1.2417550123956083 \n",
      "epoch 3/5: sup_loss: 0.7189093026040244 sup_acc: 0.7736798585766423 unsup_loss: 1.5230522449434238 \n",
      "epoch 4/5: sup_loss: 0.9716002574040942 sup_acc: 0.7139170848540146 unsup_loss: 1.8148986853822304 \n",
      "epoch 5/5: sup_loss: 2.0727848902779775 sup_acc: 0.6138657618613139 unsup_loss: 3.052304391008224 \n",
      "26032/26032 [==============================] - 6s 242us/step\n",
      "[2.991077871284532, 0.413260602335587]\n",
      "epoch 1/5: sup_loss: 3.200002706398929 sup_acc: 0.4862996122262774 unsup_loss: 4.1510168553268825 \n",
      "epoch 2/5: sup_loss: 1.330258983003832 sup_acc: 0.6888401003649635 unsup_loss: 2.5110570821013765 \n",
      "epoch 3/5: sup_loss: 1.1785025101182234 sup_acc: 0.7086422217153284 unsup_loss: 2.7692947326785458 \n",
      "epoch 4/5: sup_loss: 1.3394792774406663 sup_acc: 0.6779482208029197 unsup_loss: 3.161310207234682 \n",
      "epoch 5/5: sup_loss: 1.8562299323125477 sup_acc: 0.6403683850364964 unsup_loss: 3.819013430254303 \n",
      "26032/26032 [==============================] - 6s 233us/step\n",
      "[5.902464323858579, 0.3945912722802704]\n",
      "epoch 1/5: sup_loss: 2.928422680736458 sup_acc: 0.5798642791970803 unsup_loss: 4.8420904970517125 \n",
      "epoch 2/5: sup_loss: 2.0755755523913098 sup_acc: 0.603558394160584 unsup_loss: 5.24532590773854 \n",
      "epoch 3/5: sup_loss: 1.7903199132776608 sup_acc: 0.6371606979927007 unsup_loss: 5.221321767699109 \n",
      "epoch 4/5: sup_loss: 1.9917101569636895 sup_acc: 0.6107151003649635 unsup_loss: 5.09033248894406 \n",
      "epoch 5/5: sup_loss: 1.695192895666526 sup_acc: 0.6517592381386861 unsup_loss: 4.9558625660673545 \n",
      "26032/26032 [==============================] - 6s 233us/step\n",
      "[2.0235275460960094, 0.6258451137062078]\n",
      "epoch 1/5: sup_loss: 1.3485984709249796 sup_acc: 0.7010150547445255 unsup_loss: 4.472348730929577 \n",
      "epoch 2/5: sup_loss: 1.7076176175453368 sup_acc: 0.6680400319343066 unsup_loss: 4.988209882791895 \n",
      "epoch 3/5: sup_loss: 1.080145423375342 sup_acc: 0.7227132755474452 unsup_loss: 4.252578200650041 \n",
      "epoch 4/5: sup_loss: 1.2539373510075311 sup_acc: 0.7091697080291971 unsup_loss: 4.233817405944323 \n",
      "epoch 5/5: sup_loss: 1.0801412263055787 sup_acc: 0.713574931569343 unsup_loss: 4.646877393235255 \n",
      "26032/26032 [==============================] - 6s 230us/step\n",
      "[1.2874351653425549, 0.6375614628149969]\n"
     ]
    }
   ],
   "source": [
    "#NOTE: codes in this cell are collected from online resource, and are modified by us\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "import numpy\n",
    "\n",
    "from model import VATModel, SemiSupervisedVATModel\n",
    "\n",
    "__author__ = 'Romain Tavenard romain.tavenard[at]univ-rennes2.fr'\n",
    "\n",
    "model=net_svhn_model_with_lrelu()\n",
    "\n",
    "X_train=train_greyscale.reshape(-1,32,32)\n",
    "X_test=test_greyscale.reshape(-1,32,32)\n",
    "X_train = X_train.astype('float32')[:, :, :, None]\n",
    "X_test = X_test.astype('float32')[:, :, :, None]\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train=to_categorical(svhn_train_label)\n",
    "y_test=to_categorical(svhn_test_label)\n",
    "\n",
    "# # Fully supervised training: do not use VATModel\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Semi-supervised training: define a model with two inputs/outputs/losses, one\n",
    "# for supervised data, one for unsupervised data\n",
    "indices_supervised = numpy.random.randint(low=0, high=X_train.shape[0],\n",
    "                                          size=X_train.shape[0] // 24)\n",
    "indices_unsupervised = numpy.isin(numpy.arange(X_train.shape[0]),\n",
    "                                  indices_supervised,\n",
    "                                  invert=True)\n",
    "#X_train=X_train[permutation]\n",
    "#y_train=y_train[permutation]\n",
    "X_train_sup = X_train[indices_supervised]\n",
    "y_train_sup = y_train[indices_supervised]\n",
    "X_train_unsup = X_train[indices_unsupervised]\n",
    "#X_train_sup=X_train[0:3000]\n",
    "#y_train_sup = y_train[0:3000]\n",
    "#X_train_unsup = X_train[3000:]\n",
    "\n",
    "#we print shapes of labeled and unlabeled data\n",
    "print(f'labeled data:{X_train_sup.shape}\\n\\\n",
    "label:{y_train_sup.shape}\\n\\\n",
    "unlabeled data:{X_train_unsup.shape}')\n",
    "\n",
    "model3 = SemiSupervisedVATModel(model=model,input_shape=(32, 32, 1))\n",
    "model3.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "               metrics=[\"accuracy\"])\n",
    "for i in range(10):\n",
    "    model3.fit([X_train_sup, X_train_unsup],\n",
    "               [y_train_sup, None],\n",
    "               batch_size=128, epochs=5)\n",
    "    print(model3.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that introducing LeakyReLU as activation function can solve that problem to some extent.  \n",
    "The following codes are used to free some memory to avoid MemoryError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_greyscale.reshape(-1,32,32)\n",
    "X_test=test_greyscale.reshape(-1,32,32)\n",
    "X_train = X_train.astype('float32')[:, :, :, None]\n",
    "X_test = X_test.astype('float32')[:, :, :, None]\n",
    "X_train /= 255.\n",
    "X_test /= 255.\n",
    "y_train=to_categorical(svhn_train_label)\n",
    "y_test=to_categorical(svhn_test_label)\n",
    "\n",
    "indices_supervised = numpy.random.randint(low=0, high=X_train.shape[0],\n",
    "                                          size=X_train.shape[0] // 24)\n",
    "indices_unsupervised = numpy.isin(numpy.arange(X_train.shape[0]),\n",
    "                                  indices_supervised,\n",
    "                                  invert=True)\n",
    "X_train=X_train[permutation]\n",
    "X_train_sup = X_train[indices_supervised]\n",
    "y_train_sup = y_train[indices_supervised]\n",
    "X_train_unsup = X_train[indices_unsupervised]\n",
    "X_train_sup=X_train[0:3000]\n",
    "y_train_sup = y_train[0:3000]\n",
    "X_train_unsup = X_train[3000:]\n",
    "del X_train,X_train_sup,y_train_sup,X_train_unsup,indices_unsupervised,indices_supervised,y_test\n",
    "del y_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Different Activation Functions -- Leaky ReLU and ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some research online, we found that there is another activation function called LeakyReLU, we want to explore whether the activation can improve our VAT-based model performance. In the following model, we replace the original activation function ReLU with LeakyReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky ReLU has a small slope for negative values, instead of simply set them all to zeros. For example, leaky ReLU may have $y = 0.01x$ when $x < 0$.  \n",
    "Leaky ReLU has two benefits:\n",
    "1. It fixes the “dying ReLU” problem, as it doesn’t have zero-slope parts.\n",
    "2. It speeds up training. There is evidence that having the “mean activation” be close to 0 makes training faster. (It helps keep off-diagonal entries of the Fisher information matrix small, but you can safely ignore this.) Unlike ReLU, leaky ReLU is more “balanced,” and may therefore learn faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vat_svhn_2(epsilon=1,alpha=0.3):\n",
    "    network_svhn = Sequential()\n",
    "    input_shape_svhn=(1024,)\n",
    "    lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.1)\n",
    "    network_svhn.add(keras.layers.Reshape((32,32,1),input_shape = input_shape_svhn))\n",
    "    network_svhn.add(keras.layers.Conv2D(filters=32,kernel_size = [5,5],padding = 'same',activation = lrelu))\n",
    "    network_svhn.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = 2))\n",
    "    network_svhn.add(keras.layers.Conv2D(filters=64,kernel_size = [5,5],padding = 'same',activation = lrelu))\n",
    "    network_svhn.add(keras.layers.MaxPool2D(pool_size=(2,2), strides =2))\n",
    "    network_svhn.add(keras.layers.Dropout(0.25))\n",
    "    network_svhn.add(keras.layers.Reshape((-1, 8 * 8 * 64)))\n",
    "    #network_svhn.add(keras.layers.Dropout(0.25))\n",
    "    #network_svhn.add(keras.layers.Flatten())\n",
    "    network_svhn.add(keras.layers.Dense(units=256,activation = lrelu))\n",
    "    #network_svhn.add(keras.layers.Dropout(0.5))\n",
    "    network_svhn.add(keras.layers.Dense(units=10))\n",
    "\n",
    "    model_input_n_unlabeled = Input((1024,))\n",
    "    model_input_nlabels=Input((10,))\n",
    "    model_input_n_labeled=Input((1024,))\n",
    "\n",
    "    p_logit_svhn = network_svhn( model_input_n_unlabeled )\n",
    "    p_svhn = Activation('softmax')( p_logit_svhn )\n",
    "    logit_svhn = network_svhn(model_input_n_labeled)\n",
    "    _svhn = Activation('softmax')( logit_svhn )\n",
    "    nll_loss_svhn = ce_loss(logit_svhn, model_input_nlabels)\n",
    "    ul_logit_svhn = network_svhn(model_input_n_unlabeled)\n",
    "    r_svhn = tf.random_normal(shape=tf.shape( model_input_n_unlabeled ))\n",
    "    r_svhn = unit_norm( r_svhn )\n",
    "    p_logit_r_svhn = network_svhn( model_input_n_unlabeled + epsilon*r_svhn  )\n",
    "    kld_svhn = tf.reduce_mean(kl_divergence( p_logit_svhn , p_logit_r_svhn ))\n",
    "    grad_kld_svhn = tf.gradients( kld_svhn , [r_svhn])[0]\n",
    "    r_vadv_svhn = tf.stop_gradient(grad_kld_svhn)\n",
    "    r_vadv_svhn = unit_norm( r_vadv_svhn )*alpha\n",
    "    p_logit_no_gradient_svhn = tf.stop_gradient(p_logit_svhn)\n",
    "    logit_no_gradient_svhn=tf.stop_gradient(logit_svhn)\n",
    "    p_logit_r_adv_svhn = network_svhn( model_input_n_unlabeled + r_vadv_svhn )\n",
    "    p_logit_r_adv_svhn_2 = network_svhn( model_input_n_labeled+ r_vadv_svhn )\n",
    "    vat_loss_svhn =  tf.reduce_mean(kl_divergence( p_logit_no_gradient_svhn, p_logit_r_adv_svhn ))\n",
    "    vat_loss_svhn_2=tf.reduce_mean(kl_divergence( _svhn, model_input_nlabels))\n",
    "    ent_loss_svhn = entropy_y_x(ul_logit_svhn)\n",
    "    additional_loss_svhn = vat_loss_svhn + ent_loss_svhn+vat_loss_svhn_2\n",
    "    loss_f_svhn = nll_loss_svhn + additional_loss_svhn\n",
    "    model_vat_svhn = Model([model_input_n_unlabeled,model_input_n_labeled,model_input_nlabels], p_svhn )\n",
    "    model_vat_svhn.add_loss(loss_f_svhn)\n",
    "    model_vat_svhn.compile( 'sgd',None,metrics=['accuracy'])\n",
    "    model_vat_svhn.metrics_names.append('total_loss_svhn')\n",
    "    model_vat_svhn.metrics_tensors.append(loss_f_svhn )\n",
    "    return model_vat_svhn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As what we did before, we prepare the dataset for new model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn_train=loadmat('train_32x32.mat')\n",
    "svhn_test=loadmat('test_32x32.mat')\n",
    "svhn_train_img3072=svhn_train['X'][:,:,:,:].transpose((3,0,1,2))\n",
    "svhn_test_img3072=svhn_test['X'][:,:,:,:].transpose((3,0,1,2))\n",
    "svhn_train_label=svhn_train['y'][:,0]\n",
    "svhn_test_label=svhn_test['y'][:,0]\n",
    "svhn_train_label[svhn_train_label== 10] = 0#change label 10 to label 0\n",
    "svhn_test_label[svhn_test_label == 10] = 0\n",
    "svhn_test_label_onehot=keras.utils.to_categorical(svhn_test_label)\n",
    "svhn_train_label_onehot=keras.utils.to_categorical(svhn_train_label)\n",
    "del svhn_train,svhn_test\n",
    "train_greyscale = rgb2gray(svhn_train_img3072).astype(np.float32).reshape(-1,1024)\n",
    "test_greyscale = rgb2gray(svhn_test_img3072).astype(np.float32).reshape(-1,1024)\n",
    "\n",
    "train_mean = np.mean(train_greyscale, axis=0)\n",
    "\n",
    "# Calculate the std on the training data\n",
    "train_std = np.std(train_greyscale, axis=0)\n",
    "\n",
    "# Subtract it equally from all splits\n",
    "train_greyscale = (train_greyscale - train_mean) / train_std\n",
    "test_greyscale = (test_greyscale - train_mean)  / train_std\n",
    "\n",
    "permutation = np.random.permutation(train_greyscale.shape[0])\n",
    "train_greyscale = train_greyscale[permutation, :]\n",
    "svhn_train_label = svhn_train_label[permutation]\n",
    "svhn_train_label_onehot = svhn_train_label_onehot[permutation,:]\n",
    "train_greyscale_3000label=train_greyscale[0:3000]\n",
    "svhn_train_label_onehot_3000label=svhn_train_label_onehot[0:3000]\n",
    "\n",
    "svhn_test_label_onehot_useless=keras.utils.to_categorical([9]*len(svhn_test_label_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same hyperparameter set to implement our VAT-based model with LeakyReLU activation function, and train this model use the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 13s 4ms/step - loss: 2.4379 - total_loss_svhn: 2.4379\n",
      "epoch:1, accruracy in testing dataset(3000 labeled): 0.19591272280270436\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.4073 - total_loss_svhn: 2.4073\n",
      "epoch:2, accruracy in testing dataset(3000 labeled): 0.20467117393976644\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.3752 - total_loss_svhn: 2.3752\n",
      "epoch:3, accruracy in testing dataset(3000 labeled): 0.25084511370620777\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.3142 - total_loss_svhn: 2.3142\n",
      "epoch:4, accruracy in testing dataset(3000 labeled): 0.2988245236631838\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.2132 - total_loss_svhn: 2.2132\n",
      "epoch:5, accruracy in testing dataset(3000 labeled): 0.3319376152427781\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 2.0263 - total_loss_svhn: 2.0263\n",
      "epoch:6, accruracy in testing dataset(3000 labeled): 0.4414950829748002\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.7924 - total_loss_svhn: 1.7924\n",
      "epoch:7, accruracy in testing dataset(3000 labeled): 0.5983020897357099\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.5606 - total_loss_svhn: 1.5606\n",
      "epoch:8, accruracy in testing dataset(3000 labeled): 0.6235786724031961\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.3903 - total_loss_svhn: 1.3903\n",
      "epoch:9, accruracy in testing dataset(3000 labeled): 0.6781653349723418\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.2623 - total_loss_svhn: 1.2623\n",
      "epoch:10, accruracy in testing dataset(3000 labeled): 0.7034035033804549\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.1521 - total_loss_svhn: 1.1521\n",
      "epoch:11, accruracy in testing dataset(3000 labeled): 0.7386293792255685\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.0887 - total_loss_svhn: 1.0887\n",
      "epoch:12, accruracy in testing dataset(3000 labeled): 0.7469652735095267\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 1.0292 - total_loss_svhn: 1.0292\n",
      "epoch:13, accruracy in testing dataset(3000 labeled): 0.7475799016594961\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.9964 - total_loss_svhn: 0.9964\n",
      "epoch:14, accruracy in testing dataset(3000 labeled): 0.755339582052858\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.9398 - total_loss_svhn: 0.9398\n",
      "epoch:15, accruracy in testing dataset(3000 labeled): 0.7641364474492932\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.9133 - total_loss_svhn: 0.9133\n",
      "epoch:16, accruracy in testing dataset(3000 labeled): 0.7684772587584512\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.8726 - total_loss_svhn: 0.8726\n",
      "epoch:17, accruracy in testing dataset(3000 labeled): 0.7680162876459742\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.8453 - total_loss_svhn: 0.8453\n",
      "epoch:18, accruracy in testing dataset(3000 labeled): 0.7826137062077443\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.8275 - total_loss_svhn: 0.8275\n",
      "epoch:19, accruracy in testing dataset(3000 labeled): 0.7763521819299324\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.8041 - total_loss_svhn: 0.8041\n",
      "epoch:20, accruracy in testing dataset(3000 labeled): 0.781499692685925\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7823 - total_loss_svhn: 0.7823\n",
      "epoch:21, accruracy in testing dataset(3000 labeled): 0.7854563614013522\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7620 - total_loss_svhn: 0.7620\n",
      "epoch:22, accruracy in testing dataset(3000 labeled): 0.7780424093423479\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7407 - total_loss_svhn: 0.7407\n",
      "epoch:23, accruracy in testing dataset(3000 labeled): 0.7938690842040566\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7197 - total_loss_svhn: 0.7197\n",
      "epoch:1, accruracy in testing dataset(3000 labeled): 0.7871465888137676\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7091 - total_loss_svhn: 0.7091\n",
      "epoch:2, accruracy in testing dataset(3000 labeled): 0.7744698832206515\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.7026 - total_loss_svhn: 0.7026\n",
      "epoch:3, accruracy in testing dataset(3000 labeled): 0.7882606023355869\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6836 - total_loss_svhn: 0.6836\n",
      "epoch:4, accruracy in testing dataset(3000 labeled): 0.796519668100799\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6762 - total_loss_svhn: 0.6762\n",
      "epoch:5, accruracy in testing dataset(3000 labeled): 0.7860325752919484\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6570 - total_loss_svhn: 0.6570\n",
      "epoch:6, accruracy in testing dataset(3000 labeled): 0.7937922556853104\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6537 - total_loss_svhn: 0.6537\n",
      "epoch:7, accruracy in testing dataset(3000 labeled): 0.7984019668100799\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6286 - total_loss_svhn: 0.6286\n",
      "epoch:8, accruracy in testing dataset(3000 labeled): 0.7963660110633067\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6295 - total_loss_svhn: 0.6295\n",
      "epoch:9, accruracy in testing dataset(3000 labeled): 0.7958666256914567\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6107 - total_loss_svhn: 0.6107\n",
      "epoch:10, accruracy in testing dataset(3000 labeled): 0.793638598647818\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6112 - total_loss_svhn: 0.6112\n",
      "epoch:11, accruracy in testing dataset(3000 labeled): 0.7955593116164721\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6003 - total_loss_svhn: 0.6003\n",
      "epoch:12, accruracy in testing dataset(3000 labeled): 0.7951751690227412\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.6022 - total_loss_svhn: 0.6022\n",
      "epoch:13, accruracy in testing dataset(3000 labeled): 0.7958666256914567\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5839 - total_loss_svhn: 0.5839\n",
      "epoch:14, accruracy in testing dataset(3000 labeled): 0.7992086662569146\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5792 - total_loss_svhn: 0.5792\n",
      "epoch:15, accruracy in testing dataset(3000 labeled): 0.7934465273509527\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5789 - total_loss_svhn: 0.5789\n",
      "epoch:16, accruracy in testing dataset(3000 labeled): 0.796020282728949\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5607 - total_loss_svhn: 0.5607\n",
      "epoch:17, accruracy in testing dataset(3000 labeled): 0.7984019668100799\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5667 - total_loss_svhn: 0.5667\n",
      "epoch:18, accruracy in testing dataset(3000 labeled): 0.7983251382913338\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5603 - total_loss_svhn: 0.5603\n",
      "epoch:19, accruracy in testing dataset(3000 labeled): 0.801820835894284\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5512 - total_loss_svhn: 0.5512\n",
      "epoch:20, accruracy in testing dataset(3000 labeled): 0.7991702519975414\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5482 - total_loss_svhn: 0.5482\n",
      "epoch:21, accruracy in testing dataset(3000 labeled): 0.8051244622003688\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5359 - total_loss_svhn: 0.5359\n",
      "epoch:22, accruracy in testing dataset(3000 labeled): 0.8035878918254457\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5412 - total_loss_svhn: 0.5412\n",
      "epoch:23, accruracy in testing dataset(3000 labeled): 0.7969422249539029\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5313 - total_loss_svhn: 0.5313\n",
      "epoch:1, accruracy in testing dataset(3000 labeled): 0.7967885679164106\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5243 - total_loss_svhn: 0.5243\n",
      "epoch:2, accruracy in testing dataset(3000 labeled): 0.8050476336816226\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5316 - total_loss_svhn: 0.5316\n",
      "epoch:3, accruracy in testing dataset(3000 labeled): 0.8024354640442533\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5181 - total_loss_svhn: 0.5181\n",
      "epoch:4, accruracy in testing dataset(3000 labeled): 0.7977873386601106\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5118 - total_loss_svhn: 0.5118\n",
      "epoch:5, accruracy in testing dataset(3000 labeled): 0.8073524892440074\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5031 - total_loss_svhn: 0.5031\n",
      "epoch:6, accruracy in testing dataset(3000 labeled): 0.8035878918254457\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5064 - total_loss_svhn: 0.5064\n",
      "epoch:7, accruracy in testing dataset(3000 labeled): 0.8020513214505224\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5064 - total_loss_svhn: 0.5064\n",
      "epoch:8, accruracy in testing dataset(3000 labeled): 0.801321450522434\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5101 - total_loss_svhn: 0.5101\n",
      "epoch:9, accruracy in testing dataset(3000 labeled): 0.7985940381069453\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4985 - total_loss_svhn: 0.4985\n",
      "epoch:10, accruracy in testing dataset(3000 labeled): 0.8039336201598033\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.5017 - total_loss_svhn: 0.5017\n",
      "epoch:11, accruracy in testing dataset(3000 labeled): 0.8040488629379225\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4930 - total_loss_svhn: 0.4930\n",
      "epoch:12, accruracy in testing dataset(3000 labeled): 0.803280577750461\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4944 - total_loss_svhn: 0.4944\n",
      "epoch:13, accruracy in testing dataset(3000 labeled): 0.8005147510755992\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4912 - total_loss_svhn: 0.4912\n",
      "epoch:14, accruracy in testing dataset(3000 labeled): 0.8057390903503381\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4899 - total_loss_svhn: 0.4899\n",
      "epoch:15, accruracy in testing dataset(3000 labeled): 0.8045482483097726\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4807 - total_loss_svhn: 0.4807\n",
      "epoch:16, accruracy in testing dataset(3000 labeled): 0.8052397049784881\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4829 - total_loss_svhn: 0.4829\n",
      "epoch:17, accruracy in testing dataset(3000 labeled): 0.8032037492317148\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4839 - total_loss_svhn: 0.4839\n",
      "epoch:18, accruracy in testing dataset(3000 labeled): 0.794560540872772\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4803 - total_loss_svhn: 0.4803\n",
      "epoch:19, accruracy in testing dataset(3000 labeled): 0.8026275353411186\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4813 - total_loss_svhn: 0.4813\n",
      "epoch:20, accruracy in testing dataset(3000 labeled): 0.8033958205285802\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4782 - total_loss_svhn: 0.4782\n",
      "epoch:21, accruracy in testing dataset(3000 labeled): 0.8042025199754149\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4778 - total_loss_svhn: 0.4778\n",
      "epoch:22, accruracy in testing dataset(3000 labeled): 0.8047787338660111\n",
      "Epoch 1/1\n",
      "3000/3000 [==============================] - 11s 4ms/step - loss: 0.4707 - total_loss_svhn: 0.4707\n",
      "epoch:23, accruracy in testing dataset(3000 labeled): 0.8048171481253842\n"
     ]
    }
   ],
   "source": [
    "model_vat_svhn_3000_2_acc_trace=[]\n",
    "model_vat_svhn_3000_2_loss_trace=[]\n",
    "model_vat_svhn_3000_2=vat_svhn_2(epsilon=10,alpha=0.3)\n",
    "for j in range(3):\n",
    "    for i in range(3000,70256,3000):\n",
    "        model_vat_svhn_3000_2.fit([train_greyscale[i:i+3000],train_greyscale_3000label,svhn_train_label_onehot_3000label], None,epochs=1 )\n",
    "        y_pred_svhn = model_vat_svhn_3000_2.predict( [test_greyscale,test_greyscale,svhn_test_label_onehot_useless] ).argmax(-1)\n",
    "        acc=accuracy_score(svhn_test_label , y_pred_svhn)\n",
    "        print(f\"epoch:{i//3000}, accruracy in testing dataset(3000 labeled): {acc}\")\n",
    "        model_vat_svhn_3000_2_acc_trace.append(acc)\n",
    "        model_vat_svhn_3000_2_loss_trace.append(model_vat_svhn_3000_2.history.history['total_loss_svhn'][-1])\n",
    "#del model_vat_svhn_3000_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results do not show obvious improvement compared to ReLU. Our original thought is that LeakyReLU can solve the \"dying ReLU\" problem, so using LeakyReLU can achieve a better result or at least a faster training speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 5. RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Compare the accuracy of CNN and VAT in MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feba2be9d68>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_trace=model_cnn_mnist_full_acc_trace[:]\n",
    "while len(temp_trace)<len(model_vat_mnist_full_acc_trace):\n",
    "    temp_trace.append(temp_trace[-1])\n",
    "plt.plot(range(len(temp_trace)),temp_trace,\"x-\",label=\"model_cnn_mnist_full_acc\")\n",
    "plt.plot(range(len(model_vat_mnist_full_acc_trace)),model_vat_mnist_full_acc_trace,\"+-\",label=\"model_vat_mnist_full_acc\")\n",
    "plt.legend()\n",
    "del temp_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figures we can see that if all data (60000 images) are labeled, CNN and VAT (or it should be called AT at this time) can reach similar accuracy. However, CNN has shorter training time. As a result, if all data are labeled and there is low noise in the dataset, there is no need to use VAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febf8b50160>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_trace=model_cnn_mnist_200_acc_trace[:]\n",
    "while len(temp_trace)<len(model_vat_mnist_200_acc_trace):\n",
    "    temp_trace.append(temp_trace[-1])\n",
    "plt.plot(range(len(temp_trace)),temp_trace,\"-\",label=\"model_cnn_mnist_200_acc\")\n",
    "plt.plot(range(len(model_vat_mnist_200_acc_trace)),model_vat_mnist_200_acc_trace,\"-\",label=\"model_vat_mnist_200_acc\")\n",
    "plt.legend()\n",
    "del temp_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figures we can see that if only 200 images are labeled, VAT reaches higher accuracy. The reason is that VAT use LDS to improve model robustness. Even if there are plenty of data are unlabeled, VAT can still use these data for training while CNN cannot use unlabeled data for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Compare the accuracy of CNN and VAT in SVHN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7febb3a25630>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febb3afafd0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(model_vat_svhn_3000_acc_trace)),model_vat_svhn_3000_acc_trace,\"-\",label=\"model_vat_svhn_3000_acc_trace\")\n",
    "plt.plot(range(len(model_vat_svhn_2000_acc_trace)),model_vat_svhn_2000_acc_trace,\"-\",label=\"model_vat_svhn_2000_acc_trace\")\n",
    "plt.plot(range(len(model_vat_svhn_1000_acc_trace)),model_vat_svhn_1000_acc_trace,\"-\",label=\"model_vat_svhn_1000_acc_trace\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x400267f10>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(cnn_model_1000label_acc)),cnn_model_1000label_acc,\"-\",label=\"cnn_model_1000label_acc\")\n",
    "plt.plot(range(len(cnn_model_2000label_acc)),cnn_model_2000label_acc,\"-\",label=\"cnn_model_2000label_acc\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure we can see that when the number of labeled data decreases, the accuracy of CNN model falls faster than VAT-based model, which means CNN model has lower robustness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Analysis of the indistinguishable images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train our VAT-based model, we can analyze which images cannot be distinguished. We plot 12 images which are wrongly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febb3a58630>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    plt.figure(0,figsize=(20,20))\n",
    "    while True:\n",
    "        rffwp=randint_for_finding_wrong_prediction=random.randint(0,len(test_greyscale)-1)\n",
    "        pred=model_vat_svhn_3000.predict( [test_greyscale[rffwp].reshape(-1,1024),test_greyscale[rffwp].reshape(-1,1024),svhn_test_label_onehot_useless ]).argmax(-1)\n",
    "        if svhn_test_label[rffwp]!=pred:\n",
    "            plt.subplot(3,4,i+1)\n",
    "            plt.imshow(test_greyscale[rffwp].reshape(32,32),cmap ='gray')\n",
    "            plt.title(f'prediction:{pred[0][0]},true:{svhn_test_label[rffwp]}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some images in the previous figure are really unclear, even human cannot distingguish them. However, there are still some images that fairly clear but are also wrongly classified. We think the reason may be the neighbor number (as noise) influence the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above experiments of using CNN and VAT models on MNIST and SVHN dataset, we can draw a firm conclusion that VAT performs well in the situation of inadequate labeled data.\n",
    "\n",
    "With 3000 labeled data samples, CNN can only reach 70~75% accuracy, while VAT could boost the accuracy to more than 82% if we add unlabeled data samples as input.\n",
    "\n",
    "With 2000 labeled data samples, CNN can only reach 63% accuracy at most, while VAT could boost the accuracy to more than 78.5%.\n",
    "\n",
    "With 1000 labeled data samples, the prediction using CNN model only reaches 54% accuracy in the best situation, while VAT could reach more than 69%.\n",
    "\n",
    "We find that if choosing proper hyperparameters, VAT can perform better than CNN. We think the reasons are that VAT-based model can utilize unlabeled data for robustness improvement. If there are few data are labeled, CNN will easily reach the overfitting state.  \n",
    "\n",
    "VAT has shown good results in semi-supervised learning, it performs better when there are very few training samples. In the real world application, VAT is especially good for the situation that data analysts have a few labeled data (or no labeled data) and great amount of unlabeled data samples. It can still make considerably accurate prediction. \n",
    "\n",
    "The most interesting part we learned from this project is how to improve robustness of a  model. For CNN, we can use dropout layers, which are used to control not training part of the neurons. For semi-supervised learning, we can use LDS, which utilize unlabeled data to decrease the influence of negative gradient perturbation.\n",
    "\n",
    "LeakyReLU can be used to solve \"dying ReLU\" to some extent. However, LeakyReLU does not always help.  \n",
    "\n",
    "The most difficult part is trying to improve the performance of our VAT-based model. Only after we compare our VAT-based model with the model from online resource, we find that our VAT-based model is fairly good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. WORK DIVISION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Zheyi Qin -- Implimentation of VAT, leaky ReLU   \n",
    "* Zihui Li -- Implimentation of CNN   \n",
    "* Both -- Collect reference materials and online resources; determine the topic; make plan; set methodology; write proposal and final report, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng Reading Digits in Natural Images with Unsupervised Feature Learning NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011.\n",
    "* [2] T. Miyato, S. Maeda, M. Koyama and S. Ishii, \"Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 8, pp. 1979-1993, 1 Aug. 2019.\n",
    "* [3] W. Pratt, Digital Image Processing, Wiley-Interscience, 2007.\n",
    "* [4] Goodfellow, Ian J., et al. \"Multi-digit number recognition from street view imagery using deep convolutional neural networks.\" arXiv preprint arXiv:1312.6082 (2013).\n",
    "* [5] An Introduction to Virtual Adversarial Training. \n",
    "https://divamgupta.com/unsupervised-learning/semi-supervised-learning/2019/05/31/introduction-to-virtual-adversarial-training.html\n",
    "* [6] A Practical Guide to ReLU.\n",
    "https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Project_Report_Zheyi_Zihui.ipynb is 4131\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from nbformat import current\n",
    "import glob\n",
    "nbfile = glob.glob('Project_Report_Zheyi_Zihui.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
